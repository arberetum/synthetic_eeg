{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALDYZNVNCyb1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 4000\n",
    "NUM_CHANNELS = 16\n",
    "X_pre = np.load(\"preictal_gan_train.npy\")\n",
    "X_pre = X_pre.reshape(-1, WINDOW_SIZE, NUM_CHANNELS)\n",
    "y_pre = np.ones((X_pre.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9499, 4000, 16)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pre_normalized = (X_pre - X_pre.min()) / (X_pre.max() - X_pre.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_pre_train, X_pre_val, y_pre_train, y_pre_val = train_test_split(X_pre_normalized, y_pre, test_size = 0.1, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Denoising Diffusion Probabilistic Model (DDPM)\n",
    "class DDPM(nn.Module):\n",
    "    def __init__(self, input_channels, num_filters):\n",
    "        super(DDPM, self).__init__()\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(num_filters, num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv1d(num_filters, num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(num_filters, input_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Sigmoid()  # Output range between 0 and 1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #encoded = self.encoder(x)\n",
    "        encoded = self.encoder(x.permute(0, 2, 1))  # Permute to match Conv1d input format\n",
    "        decoded = self.decoder(encoded)\n",
    "        #return decoded\n",
    "        return decoded.permute(0, 2, 1)  # Permute back to original shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_pre_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_pre_train, dtype=torch.float32)\n",
    "\n",
    "# Convert validation data to PyTorch tensors\n",
    "X_val_tensor = torch.tensor(X_pre_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_pre_val, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training set\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.0048351529985666275, Validation Loss: 0.0004256423078516514\n",
      "Epoch [1/10], Train Loss: 0.004500354174524546, Validation Loss: 0.00039376823968711864\n",
      "Epoch [1/10], Train Loss: 0.004158806521445513, Validation Loss: 0.00037502437233709644\n",
      "Epoch [1/10], Train Loss: 0.004089015536010265, Validation Loss: 0.0003688362452246546\n",
      "Epoch [1/10], Train Loss: 0.003691932652145624, Validation Loss: 0.00038020749857156156\n",
      "Epoch [1/10], Train Loss: 0.003551373491063714, Validation Loss: 0.0004100215735262213\n",
      "Epoch [1/10], Train Loss: 0.003291652537882328, Validation Loss: 0.00046393126415620955\n",
      "Epoch [1/10], Train Loss: 0.0029123034328222275, Validation Loss: 0.0005592500234312308\n",
      "Epoch [1/10], Train Loss: 0.0026990713085979223, Validation Loss: 0.0007185484140821803\n",
      "Epoch [1/10], Train Loss: 0.0021985876373946667, Validation Loss: 0.0009758809230196438\n",
      "Epoch [1/10], Train Loss: 0.0018890135688707232, Validation Loss: 0.0013596750570836814\n",
      "Epoch [1/10], Train Loss: 0.0015550259267911315, Validation Loss: 0.0018952221134207956\n",
      "Epoch [1/10], Train Loss: 0.0013060109922662377, Validation Loss: 0.002556953318964909\n",
      "Epoch [1/10], Train Loss: 0.0013008232926949859, Validation Loss: 0.003220639533965903\n",
      "Epoch [1/10], Train Loss: 0.0012058538850396872, Validation Loss: 0.003709995318908526\n",
      "Epoch [2/10], Train Loss: 0.001161872292868793, Validation Loss: 0.003923684070982477\n",
      "Epoch [2/10], Train Loss: 0.0010153878247365355, Validation Loss: 0.0038774992464532874\n",
      "Epoch [2/10], Train Loss: 0.0009804596193134785, Validation Loss: 0.003649807777725348\n",
      "Epoch [2/10], Train Loss: 0.0009269732399843633, Validation Loss: 0.0033441198375389106\n",
      "Epoch [2/10], Train Loss: 0.0007641419069841504, Validation Loss: 0.0030337632027212062\n",
      "Epoch [2/10], Train Loss: 0.0006061465246602893, Validation Loss: 0.002739850723208124\n",
      "Epoch [2/10], Train Loss: 0.0005446590948849916, Validation Loss: 0.002494653217306658\n",
      "Epoch [2/10], Train Loss: 0.0005522713763639331, Validation Loss: 0.002315099550677197\n",
      "Epoch [2/10], Train Loss: 0.00036029383772984147, Validation Loss: 0.0021976228654697664\n",
      "Epoch [2/10], Train Loss: 0.00046204059617593884, Validation Loss: 0.002139068202765173\n",
      "Epoch [2/10], Train Loss: 0.0005302189965732396, Validation Loss: 0.0021336537965831637\n",
      "Epoch [2/10], Train Loss: 0.0006605377420783043, Validation Loss: 0.002174500487510021\n",
      "Epoch [2/10], Train Loss: 0.00048538914415985346, Validation Loss: 0.0022536869642144743\n",
      "Epoch [2/10], Train Loss: 0.0005176697741262615, Validation Loss: 0.0023616946306267454\n",
      "Epoch [2/10], Train Loss: 0.0004650703922379762, Validation Loss: 0.002487105143279964\n",
      "Epoch [3/10], Train Loss: 0.0004961343947798014, Validation Loss: 0.002622411827001126\n",
      "Epoch [3/10], Train Loss: 0.0006357579841278493, Validation Loss: 0.002754633723828597\n",
      "Epoch [3/10], Train Loss: 0.0005651823594234884, Validation Loss: 0.002843288516923159\n",
      "Epoch [3/10], Train Loss: 0.0004988119471818209, Validation Loss: 0.0028865393016393446\n",
      "Epoch [3/10], Train Loss: 0.0007249294430948794, Validation Loss: 0.002894687309798573\n",
      "Epoch [3/10], Train Loss: 0.00045939002302475274, Validation Loss: 0.0028658714262815585\n",
      "Epoch [3/10], Train Loss: 0.0005969682824797928, Validation Loss: 0.002816371500351224\n",
      "Epoch [3/10], Train Loss: 0.0005100860144011676, Validation Loss: 0.002748748620173761\n",
      "Epoch [3/10], Train Loss: 0.0004718360141851008, Validation Loss: 0.002675035849072728\n",
      "Epoch [3/10], Train Loss: 0.0004666790773626417, Validation Loss: 0.0026103782295366796\n",
      "Epoch [3/10], Train Loss: 0.0004380536265671253, Validation Loss: 0.002566003295428613\n",
      "Epoch [3/10], Train Loss: 0.0004444032674655318, Validation Loss: 0.0025397324328068176\n",
      "Epoch [3/10], Train Loss: 0.00037117983447387815, Validation Loss: 0.002537545528920258\n",
      "Epoch [3/10], Train Loss: 0.0003863663587253541, Validation Loss: 0.0025561991492051538\n",
      "Epoch [3/10], Train Loss: 0.0003016411792486906, Validation Loss: 0.0025892695490302158\n",
      "Epoch [4/10], Train Loss: 0.0004029999254271388, Validation Loss: 0.0026281661913581506\n",
      "Epoch [4/10], Train Loss: 0.0004772944375872612, Validation Loss: 0.002659919556277264\n",
      "Epoch [4/10], Train Loss: 0.0005042706616222858, Validation Loss: 0.0026813771305684033\n",
      "Epoch [4/10], Train Loss: 0.00032452010782435536, Validation Loss: 0.002694152984298578\n",
      "Epoch [4/10], Train Loss: 0.0005190509837120771, Validation Loss: 0.0027001257946699104\n",
      "Epoch [4/10], Train Loss: 0.00044740730663761497, Validation Loss: 0.0026975144150846895\n",
      "Epoch [4/10], Train Loss: 0.000438391201896593, Validation Loss: 0.0026888822091884223\n",
      "Epoch [4/10], Train Loss: 0.0006001961883157492, Validation Loss: 0.0026767222981649535\n",
      "Epoch [4/10], Train Loss: 0.00046384616871364415, Validation Loss: 0.0026657970410873407\n",
      "Epoch [4/10], Train Loss: 0.0004806341021321714, Validation Loss: 0.0026562930644341127\n",
      "Epoch [4/10], Train Loss: 0.00043088090023957193, Validation Loss: 0.002649085402504361\n",
      "Epoch [4/10], Train Loss: 0.0003409278579056263, Validation Loss: 0.002643457768453645\n",
      "Epoch [4/10], Train Loss: 0.00040995131712406874, Validation Loss: 0.0026385710621643966\n",
      "Epoch [4/10], Train Loss: 0.0004800263268407434, Validation Loss: 0.0026355044380174716\n",
      "Epoch [4/10], Train Loss: 0.0005333886365406215, Validation Loss: 0.002636505466956301\n",
      "Epoch [5/10], Train Loss: 0.0005276598385535181, Validation Loss: 0.0026390919414888913\n",
      "Epoch [5/10], Train Loss: 0.0005150308134034276, Validation Loss: 0.0026424000794928867\n",
      "Epoch [5/10], Train Loss: 0.00043189991265535355, Validation Loss: 0.0026473051089668225\n",
      "Epoch [5/10], Train Loss: 0.00037698078085668385, Validation Loss: 0.0026521868377673524\n",
      "Epoch [5/10], Train Loss: 0.00048776293988339603, Validation Loss: 0.002655479726342469\n",
      "Epoch [5/10], Train Loss: 0.0005670498358085752, Validation Loss: 0.0026576082660433125\n",
      "Epoch [5/10], Train Loss: 0.0004873193392995745, Validation Loss: 0.002664967757031447\n",
      "Epoch [5/10], Train Loss: 0.0003310005704406649, Validation Loss: 0.0026690935688641142\n",
      "Epoch [5/10], Train Loss: 0.000566584465559572, Validation Loss: 0.0026672802055489366\n",
      "Epoch [5/10], Train Loss: 0.00027242195210419595, Validation Loss: 0.0026628447133077294\n",
      "Epoch [5/10], Train Loss: 0.0004399738390929997, Validation Loss: 0.002657468037429352\n",
      "Epoch [5/10], Train Loss: 0.00039639920578338206, Validation Loss: 0.0026544582337422783\n",
      "Epoch [5/10], Train Loss: 0.00035677224514074624, Validation Loss: 0.00265497321990796\n",
      "Epoch [5/10], Train Loss: 0.0004455395392142236, Validation Loss: 0.002657832675652594\n",
      "Epoch [5/10], Train Loss: 0.0006117109442129731, Validation Loss: 0.0026605394160171517\n",
      "Epoch [6/10], Train Loss: 0.0004840768815483898, Validation Loss: 0.0026617775800317025\n",
      "Epoch [6/10], Train Loss: 0.0004115269985049963, Validation Loss: 0.002664145765074787\n",
      "Epoch [6/10], Train Loss: 0.0004642110725399107, Validation Loss: 0.002666634585739685\n",
      "Epoch [6/10], Train Loss: 0.0005846723797731102, Validation Loss: 0.0026685976572021716\n",
      "Epoch [6/10], Train Loss: 0.0004529471625573933, Validation Loss: 0.0026686344777053896\n",
      "Epoch [6/10], Train Loss: 0.0002947382163256407, Validation Loss: 0.002668209555911041\n",
      "Epoch [6/10], Train Loss: 0.00036373420152813196, Validation Loss: 0.0026653962534609714\n",
      "Epoch [6/10], Train Loss: 0.000418798386817798, Validation Loss: 0.0026626975056441393\n",
      "Epoch [6/10], Train Loss: 0.00038856748142279685, Validation Loss: 0.002656832801084183\n",
      "Epoch [6/10], Train Loss: 0.00041833825525827706, Validation Loss: 0.002648390201507371\n",
      "Epoch [6/10], Train Loss: 0.0004998798831366003, Validation Loss: 0.0026406813256379938\n",
      "Epoch [6/10], Train Loss: 0.0003568608080968261, Validation Loss: 0.0026371644500705624\n",
      "Epoch [6/10], Train Loss: 0.0005191820673644543, Validation Loss: 0.0026366254882182645\n",
      "Epoch [6/10], Train Loss: 0.0006042014574632049, Validation Loss: 0.0026398814741146414\n",
      "Epoch [6/10], Train Loss: 0.0004704510502051562, Validation Loss: 0.0026445327005779544\n",
      "Epoch [7/10], Train Loss: 0.00036547461058944464, Validation Loss: 0.0026512844135843905\n",
      "Epoch [7/10], Train Loss: 0.000514367304276675, Validation Loss: 0.0026577104845655567\n",
      "Epoch [7/10], Train Loss: 0.0004238446708768606, Validation Loss: 0.002660812285705274\n",
      "Epoch [7/10], Train Loss: 0.0004183861310593784, Validation Loss: 0.00266400194226005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 0.000520621775649488, Validation Loss: 0.002666254760697484\n",
      "Epoch [7/10], Train Loss: 0.0005852887406945229, Validation Loss: 0.0026668770211365294\n",
      "Epoch [7/10], Train Loss: 0.00043646738049574196, Validation Loss: 0.00266541719452298\n",
      "Epoch [7/10], Train Loss: 0.0004755970439873636, Validation Loss: 0.0026585784869878983\n",
      "Epoch [7/10], Train Loss: 0.0005724841030314565, Validation Loss: 0.0026543452208783445\n",
      "Epoch [7/10], Train Loss: 0.0003762416890822351, Validation Loss: 0.0026518823715912946\n",
      "Epoch [7/10], Train Loss: 0.0003309249004814774, Validation Loss: 0.002650556166288482\n",
      "Epoch [7/10], Train Loss: 0.0004048096598125994, Validation Loss: 0.002647272966598638\n",
      "Epoch [7/10], Train Loss: 0.00046113476855680346, Validation Loss: 0.002643666568627002\n",
      "Epoch [7/10], Train Loss: 0.00038658236735500395, Validation Loss: 0.0026464815791539787\n",
      "Epoch [7/10], Train Loss: 0.00037439557490870357, Validation Loss: 0.002653070066139853\n",
      "Epoch [8/10], Train Loss: 0.0004141093522775918, Validation Loss: 0.002658820611291698\n",
      "Epoch [8/10], Train Loss: 0.00038369130925275385, Validation Loss: 0.00265960960977656\n",
      "Epoch [8/10], Train Loss: 0.0005048527382314205, Validation Loss: 0.0026528306144970554\n",
      "Epoch [8/10], Train Loss: 0.0004945862456224859, Validation Loss: 0.0026545328060712885\n",
      "Epoch [8/10], Train Loss: 0.00042527896584942937, Validation Loss: 0.002659636195514257\n",
      "Epoch [8/10], Train Loss: 0.0004587900184560567, Validation Loss: 0.0026532005941291817\n",
      "Epoch [8/10], Train Loss: 0.00045588327338919044, Validation Loss: 0.0026458392678877386\n",
      "Epoch [8/10], Train Loss: 0.00043237381032668054, Validation Loss: 0.0026381928630235816\n",
      "Epoch [8/10], Train Loss: 0.0003906823112629354, Validation Loss: 0.0026360867905015706\n",
      "Epoch [8/10], Train Loss: 0.0003360555856488645, Validation Loss: 0.002642181803699301\n",
      "Epoch [8/10], Train Loss: 0.0004585080314427614, Validation Loss: 0.002657677941102566\n",
      "Epoch [8/10], Train Loss: 0.0004035911406390369, Validation Loss: 0.00266248027074049\n",
      "Epoch [8/10], Train Loss: 0.0004293585370760411, Validation Loss: 0.0026511842396673784\n",
      "Epoch [8/10], Train Loss: 0.0004212202038615942, Validation Loss: 0.002639425337361563\n",
      "Epoch [8/10], Train Loss: 0.0005480860127136111, Validation Loss: 0.0026327147003764114\n",
      "Epoch [9/10], Train Loss: 0.00039547760388813913, Validation Loss: 0.0026481248860975273\n",
      "Epoch [9/10], Train Loss: 0.0004264815943315625, Validation Loss: 0.002682185187904524\n",
      "Epoch [9/10], Train Loss: 0.00039733966696076095, Validation Loss: 0.002703355750491639\n",
      "Epoch [9/10], Train Loss: 0.00038312625838443637, Validation Loss: 0.002675430733757485\n",
      "Epoch [9/10], Train Loss: 0.0004613128839991987, Validation Loss: 0.0026310207283108673\n",
      "Epoch [9/10], Train Loss: 0.0003661690861918032, Validation Loss: 0.002610595857708895\n",
      "Epoch [9/10], Train Loss: 0.00041329581290483475, Validation Loss: 0.0026206475968214393\n",
      "Epoch [9/10], Train Loss: 0.0004865047230850905, Validation Loss: 0.0026438394111401144\n",
      "Epoch [9/10], Train Loss: 0.00038528480217792094, Validation Loss: 0.002650899566052591\n",
      "Epoch [9/10], Train Loss: 0.0004329308576416224, Validation Loss: 0.0026341039773483737\n",
      "Epoch [9/10], Train Loss: 0.00041606894228607416, Validation Loss: 0.0026385767166228857\n",
      "Epoch [9/10], Train Loss: 0.0003832844377029687, Validation Loss: 0.0027151586988768657\n",
      "Epoch [9/10], Train Loss: 0.0005587629275396466, Validation Loss: 0.0026752984448641286\n",
      "Epoch [9/10], Train Loss: 0.0003218332421965897, Validation Loss: 0.002662273634522527\n",
      "Epoch [9/10], Train Loss: 0.0004793814441654831, Validation Loss: 0.0026461347475835754\n",
      "Epoch [10/10], Train Loss: 0.0003475150151643902, Validation Loss: 0.0026197240457032657\n",
      "Epoch [10/10], Train Loss: 0.00040145235834643245, Validation Loss: 0.0026241607940560127\n",
      "Epoch [10/10], Train Loss: 0.0004365291097201407, Validation Loss: 0.0026518793271838865\n",
      "Epoch [10/10], Train Loss: 0.00040252041071653366, Validation Loss: 0.0027062642337175227\n",
      "Epoch [10/10], Train Loss: 0.0003737022925633937, Validation Loss: 0.002739033374168417\n",
      "Epoch [10/10], Train Loss: 0.00038344060885719955, Validation Loss: 0.002680780111710314\n",
      "Epoch [10/10], Train Loss: 0.0004061122890561819, Validation Loss: 0.0025959035984295257\n",
      "Epoch [10/10], Train Loss: 0.00035097135696560144, Validation Loss: 0.0025931840132483665\n",
      "Epoch [10/10], Train Loss: 0.0003048365470021963, Validation Loss: 0.002645245583007942\n",
      "Epoch [10/10], Train Loss: 0.00038480976945720613, Validation Loss: 0.0026962934277106484\n",
      "Epoch [10/10], Train Loss: 0.0005174491670913994, Validation Loss: 0.00270205458720066\n",
      "Epoch [10/10], Train Loss: 0.0003789395559579134, Validation Loss: 0.0026596111730680244\n",
      "Epoch [10/10], Train Loss: 0.00039331059087999165, Validation Loss: 0.002605509797518118\n",
      "Epoch [10/10], Train Loss: 0.00041325861820951104, Validation Loss: 0.0026073072833486463\n",
      "Epoch [10/10], Train Loss: 0.0002661683247424662, Validation Loss: 0.0026533247319934497\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generated_samples_interictal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m         noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m32\u001b[39m)  \u001b[38;5;66;03m# Generate noise with 32 channels\u001b[39;00m\n\u001b[1;32m     69\u001b[0m         generated \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecoder(noise\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Adjust input shape for the decoder\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m         generated_samples_interictal\u001b[38;5;241m.\u001b[39mappend(generated)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Convert generated samples to a NumPy array\u001b[39;00m\n\u001b[1;32m     73\u001b[0m generated_samples_interictal \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(generated_samples_interictal)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generated_samples_interictal' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hyperparameters\n",
    "input_channels = 16  # NUM_CHANNELS\n",
    "num_filters = 32\n",
    "batch_size = 8  # Reducing batch size for demonstration purposes\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the model\n",
    "model = DDPM(input_channels, num_filters)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for reconstruction\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()  # Set model to training mode\n",
    "    for i, (batch_X, _) in enumerate(dataloader):\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed = model(batch_X)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(reconstructed, batch_X)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print progress\n",
    "       # print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], Loss: {loss.item()}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X_val, _ in val_dataloader:\n",
    "                reconstructed_val = model(batch_X_val)\n",
    "                val_loss += criterion(reconstructed_val, batch_X_val).item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "\n",
    "        # Print training and validation loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()}, Validation Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Train Loss: 0.004151613917201757, Validation Loss: 0.0004970612835448172\n",
      "Epoch [1/25], Train Loss: 0.0038952850736677647, Validation Loss: 0.0004967649525007969\n",
      "Epoch [1/25], Train Loss: 0.0035872994922101498, Validation Loss: 0.0004956763535601032\n",
      "Epoch [1/25], Train Loss: 0.003310900880023837, Validation Loss: 0.000507585605031208\n",
      "Epoch [1/25], Train Loss: 0.0030296563636511564, Validation Loss: 0.0005395082647309584\n",
      "Epoch [1/25], Train Loss: 0.0028010548558086157, Validation Loss: 0.0006018070779944293\n",
      "Epoch [1/25], Train Loss: 0.002521986374631524, Validation Loss: 0.000709228341843338\n",
      "Epoch [1/25], Train Loss: 0.0023307306692004204, Validation Loss: 0.0008813487096814861\n",
      "Epoch [1/25], Train Loss: 0.0019895194564014673, Validation Loss: 0.001147441889801133\n",
      "Epoch [1/25], Train Loss: 0.0016288339393213391, Validation Loss: 0.0015336085578520633\n",
      "Epoch [1/25], Train Loss: 0.0015874417731538415, Validation Loss: 0.0020401148495319136\n",
      "Epoch [1/25], Train Loss: 0.0013582060346379876, Validation Loss: 0.002612673264669896\n",
      "Epoch [1/25], Train Loss: 0.0011158367851749063, Validation Loss: 0.003153440209903887\n",
      "Epoch [1/25], Train Loss: 0.001119008520618081, Validation Loss: 0.0035516618762728796\n",
      "Epoch [1/25], Train Loss: 0.0009716751519590616, Validation Loss: 0.003744103562305955\n",
      "Epoch [2/25], Train Loss: 0.0009679930517449975, Validation Loss: 0.0037362961227684712\n",
      "Epoch [2/25], Train Loss: 0.0009105519275180995, Validation Loss: 0.0035759496733452343\n",
      "Epoch [2/25], Train Loss: 0.0008794873720034957, Validation Loss: 0.0033286373217065795\n",
      "Epoch [2/25], Train Loss: 0.0005723651847802103, Validation Loss: 0.003055640748709816\n",
      "Epoch [2/25], Train Loss: 0.0006363235879689455, Validation Loss: 0.0028089230856662537\n",
      "Epoch [2/25], Train Loss: 0.0005385878030210733, Validation Loss: 0.0026031904814749203\n",
      "Epoch [2/25], Train Loss: 0.0007217699312604964, Validation Loss: 0.002456081518512063\n",
      "Epoch [2/25], Train Loss: 0.00048447828157804906, Validation Loss: 0.002360338411693062\n",
      "Epoch [2/25], Train Loss: 0.0005176807171665132, Validation Loss: 0.0023132619044954786\n",
      "Epoch [2/25], Train Loss: 0.0004310054355300963, Validation Loss: 0.0023110499292486857\n",
      "Epoch [2/25], Train Loss: 0.0004529007710516453, Validation Loss: 0.002348791653321696\n",
      "Epoch [2/25], Train Loss: 0.0004652582574635744, Validation Loss: 0.0024191910154460106\n",
      "Epoch [2/25], Train Loss: 0.0005354617023840547, Validation Loss: 0.0025144660407129455\n",
      "Epoch [2/25], Train Loss: 0.00043767274473793805, Validation Loss: 0.0026245041899070017\n",
      "Epoch [2/25], Train Loss: 0.0006521904724650085, Validation Loss: 0.0027376181297317274\n",
      "Epoch [3/25], Train Loss: 0.000517186475917697, Validation Loss: 0.0028395078598889485\n",
      "Epoch [3/25], Train Loss: 0.0005234808777458966, Validation Loss: 0.0029176292283570066\n",
      "Epoch [3/25], Train Loss: 0.00044853618601337075, Validation Loss: 0.0029565740216272967\n",
      "Epoch [3/25], Train Loss: 0.0005361225921660662, Validation Loss: 0.0029597683465343062\n",
      "Epoch [3/25], Train Loss: 0.0005673173582181334, Validation Loss: 0.002928257158746113\n",
      "Epoch [3/25], Train Loss: 0.00040676488424651325, Validation Loss: 0.002868729686568014\n",
      "Epoch [3/25], Train Loss: 0.0005306528764776886, Validation Loss: 0.002792473800261231\n",
      "Epoch [3/25], Train Loss: 0.0004732882371172309, Validation Loss: 0.0027119684025269596\n",
      "Epoch [3/25], Train Loss: 0.0004701366415247321, Validation Loss: 0.002652872046612266\n",
      "Epoch [3/25], Train Loss: 0.0005317265749908984, Validation Loss: 0.0026098659492935204\n",
      "Epoch [3/25], Train Loss: 0.00048263644566759467, Validation Loss: 0.002587859231211683\n",
      "Epoch [3/25], Train Loss: 0.0003823973529506475, Validation Loss: 0.002587149539931106\n",
      "Epoch [3/25], Train Loss: 0.00039233913412317634, Validation Loss: 0.0026028189483341298\n",
      "Epoch [3/25], Train Loss: 0.0004933056188747287, Validation Loss: 0.0026269543372063327\n",
      "Epoch [3/25], Train Loss: 0.0005645297933369875, Validation Loss: 0.0026504907682694556\n",
      "Epoch [4/25], Train Loss: 0.0004786881909240037, Validation Loss: 0.0026681926473192557\n",
      "Epoch [4/25], Train Loss: 0.0005713598220609128, Validation Loss: 0.002681320161409989\n",
      "Epoch [4/25], Train Loss: 0.0005266773514449596, Validation Loss: 0.002699601456147282\n",
      "Epoch [4/25], Train Loss: 0.00036192566039972007, Validation Loss: 0.002727902907205104\n",
      "Epoch [4/25], Train Loss: 0.00035415569436736405, Validation Loss: 0.0027495317352826103\n",
      "Epoch [4/25], Train Loss: 0.0004695295647252351, Validation Loss: 0.002748053442655491\n",
      "Epoch [4/25], Train Loss: 0.000501469534356147, Validation Loss: 0.0027418463440032816\n",
      "Epoch [4/25], Train Loss: 0.0005080521223135293, Validation Loss: 0.0027288397296699658\n",
      "Epoch [4/25], Train Loss: 0.0003282088437117636, Validation Loss: 0.002716268117328276\n",
      "Epoch [4/25], Train Loss: 0.0005374188767746091, Validation Loss: 0.002707739796943399\n",
      "Epoch [4/25], Train Loss: 0.0004019819025415927, Validation Loss: 0.0027147540411747552\n",
      "Epoch [4/25], Train Loss: 0.0004271751386113465, Validation Loss: 0.0027425489368840678\n",
      "Epoch [4/25], Train Loss: 0.00044731635716743767, Validation Loss: 0.002767931789300647\n",
      "Epoch [4/25], Train Loss: 0.0005652767140418291, Validation Loss: 0.0027839139733901795\n",
      "Epoch [4/25], Train Loss: 0.00037412758683785796, Validation Loss: 0.002778430645424528\n",
      "Epoch [5/25], Train Loss: 0.0005511476774699986, Validation Loss: 0.002763701610642822\n",
      "Epoch [5/25], Train Loss: 0.0004256897373124957, Validation Loss: 0.0027405220659120996\n",
      "Epoch [5/25], Train Loss: 0.0005005611455999315, Validation Loss: 0.0027210267491544747\n",
      "Epoch [5/25], Train Loss: 0.00043310856563039124, Validation Loss: 0.002712922272499369\n",
      "Epoch [5/25], Train Loss: 0.000374861090676859, Validation Loss: 0.0027154349827622415\n",
      "Epoch [5/25], Train Loss: 0.0004279649874661118, Validation Loss: 0.002724440553139488\n",
      "Epoch [5/25], Train Loss: 0.0006012246012687683, Validation Loss: 0.002734638228151603\n",
      "Epoch [5/25], Train Loss: 0.00043293944327160716, Validation Loss: 0.0027413651769777306\n",
      "Epoch [5/25], Train Loss: 0.000545480870641768, Validation Loss: 0.0027448484134085537\n",
      "Epoch [5/25], Train Loss: 0.0003634933673311025, Validation Loss: 0.002752559570626927\n",
      "Epoch [5/25], Train Loss: 0.0004358430451247841, Validation Loss: 0.0027495763331119505\n",
      "Epoch [5/25], Train Loss: 0.000376226962544024, Validation Loss: 0.0027479845756555304\n",
      "Epoch [5/25], Train Loss: 0.00045020191464573145, Validation Loss: 0.0027431668478901648\n",
      "Epoch [5/25], Train Loss: 0.00040726992301642895, Validation Loss: 0.002735515753151614\n",
      "Epoch [5/25], Train Loss: 0.0005038348608650267, Validation Loss: 0.0027348734144935338\n",
      "Epoch [6/25], Train Loss: 0.000543992209713906, Validation Loss: 0.0027372478507459164\n",
      "Epoch [6/25], Train Loss: 0.00047622452257201076, Validation Loss: 0.002741022047218906\n",
      "Epoch [6/25], Train Loss: 0.0003526151122059673, Validation Loss: 0.0027425909461844867\n",
      "Epoch [6/25], Train Loss: 0.00044338832958601415, Validation Loss: 0.0027385899041849774\n",
      "Epoch [6/25], Train Loss: 0.0005479974206537008, Validation Loss: 0.0027288562234710247\n",
      "Epoch [6/25], Train Loss: 0.0004169797757640481, Validation Loss: 0.0027189125921264165\n",
      "Epoch [6/25], Train Loss: 0.0004043208318762481, Validation Loss: 0.002709424581003039\n",
      "Epoch [6/25], Train Loss: 0.0003993874997831881, Validation Loss: 0.0027061912794636577\n",
      "Epoch [6/25], Train Loss: 0.00047429808182641864, Validation Loss: 0.0027033931677456664\n",
      "Epoch [6/25], Train Loss: 0.00039548787754029036, Validation Loss: 0.002709993953667894\n",
      "Epoch [6/25], Train Loss: 0.00048421084647998214, Validation Loss: 0.0027195024578010336\n",
      "Epoch [6/25], Train Loss: 0.000576782098505646, Validation Loss: 0.0027285557701995896\n",
      "Epoch [6/25], Train Loss: 0.0003890426887664944, Validation Loss: 0.002732740991021029\n",
      "Epoch [6/25], Train Loss: 0.00038362195482477546, Validation Loss: 0.002735288778492132\n",
      "Epoch [6/25], Train Loss: 0.0005226979847066104, Validation Loss: 0.002742778539203671\n",
      "Epoch [7/25], Train Loss: 0.0004213257343508303, Validation Loss: 0.0027510506852271677\n",
      "Epoch [7/25], Train Loss: 0.000478581030620262, Validation Loss: 0.0027529956526396904\n",
      "Epoch [7/25], Train Loss: 0.00043730923789553344, Validation Loss: 0.002750766498795828\n",
      "Epoch [7/25], Train Loss: 0.0004614985082298517, Validation Loss: 0.0027497411145977362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Train Loss: 0.00045075154048390687, Validation Loss: 0.0027407691324865366\n",
      "Epoch [7/25], Train Loss: 0.0004937975900247693, Validation Loss: 0.0027249578676116068\n",
      "Epoch [7/25], Train Loss: 0.0005116804968565702, Validation Loss: 0.0027090209622343047\n",
      "Epoch [7/25], Train Loss: 0.0004217052774038166, Validation Loss: 0.0027007827507879805\n",
      "Epoch [7/25], Train Loss: 0.00036349729634821415, Validation Loss: 0.002701607086703557\n",
      "Epoch [7/25], Train Loss: 0.00040406978223472834, Validation Loss: 0.0027096696264509645\n",
      "Epoch [7/25], Train Loss: 0.00046536047011613846, Validation Loss: 0.0027247121686726308\n",
      "Epoch [7/25], Train Loss: 0.0005052973283454776, Validation Loss: 0.0027406533236983195\n",
      "Epoch [7/25], Train Loss: 0.00044724909821525216, Validation Loss: 0.002761083543535416\n",
      "Epoch [7/25], Train Loss: 0.000460057461168617, Validation Loss: 0.0027688357122085928\n",
      "Epoch [7/25], Train Loss: 0.00047102823737077415, Validation Loss: 0.002765852034467609\n",
      "Epoch [8/25], Train Loss: 0.00035409332485869527, Validation Loss: 0.0027511048428088174\n",
      "Epoch [8/25], Train Loss: 0.00046918648877181113, Validation Loss: 0.002733952843216287\n",
      "Epoch [8/25], Train Loss: 0.0003164330555591732, Validation Loss: 0.002716359159979625\n",
      "Epoch [8/25], Train Loss: 0.00037252347101457417, Validation Loss: 0.0026998996828534023\n",
      "Epoch [8/25], Train Loss: 0.0005366216646507382, Validation Loss: 0.0027034668615792228\n",
      "Epoch [8/25], Train Loss: 0.00046767148887738585, Validation Loss: 0.0027170937856612334\n",
      "Epoch [8/25], Train Loss: 0.0005744480295106769, Validation Loss: 0.0027291506681466054\n",
      "Epoch [8/25], Train Loss: 0.0005188049981370568, Validation Loss: 0.0027420587931610957\n",
      "Epoch [8/25], Train Loss: 0.000349254347383976, Validation Loss: 0.0027494292839297225\n",
      "Epoch [8/25], Train Loss: 0.0004690215573646128, Validation Loss: 0.002747248607773741\n",
      "Epoch [8/25], Train Loss: 0.0004327940405346453, Validation Loss: 0.0027421509217032615\n",
      "Epoch [8/25], Train Loss: 0.0003718456719070673, Validation Loss: 0.0027383946101455126\n",
      "Epoch [8/25], Train Loss: 0.0003680900845210999, Validation Loss: 0.002730310581499288\n",
      "Epoch [8/25], Train Loss: 0.0006111877737566829, Validation Loss: 0.0027295373274529933\n",
      "Epoch [8/25], Train Loss: 0.0006024652975611389, Validation Loss: 0.0027409381225329487\n",
      "Epoch [9/25], Train Loss: 0.0004663670260924846, Validation Loss: 0.0027009373953362473\n",
      "Epoch [9/25], Train Loss: 0.0005257857847027481, Validation Loss: 0.0027016696340146184\n",
      "Epoch [9/25], Train Loss: 0.000471960665890947, Validation Loss: 0.002738043028047355\n",
      "Epoch [9/25], Train Loss: 0.0005513810901902616, Validation Loss: 0.0027659962075065915\n",
      "Epoch [9/25], Train Loss: 0.0005271227564662695, Validation Loss: 0.0027436682398767533\n",
      "Epoch [9/25], Train Loss: 0.00039409598684869707, Validation Loss: 0.0027197760944606877\n",
      "Epoch [9/25], Train Loss: 0.00034412488457746804, Validation Loss: 0.002719657646272989\n",
      "Epoch [9/25], Train Loss: 0.00038300201413221657, Validation Loss: 0.002736134168950199\n",
      "Epoch [9/25], Train Loss: 0.0004263575538061559, Validation Loss: 0.0027564524187698586\n",
      "Epoch [9/25], Train Loss: 0.00047796443686820567, Validation Loss: 0.0027380436737121654\n",
      "Epoch [9/25], Train Loss: 0.0003593380097299814, Validation Loss: 0.002725742585143121\n",
      "Epoch [9/25], Train Loss: 0.0005863191327080131, Validation Loss: 0.002719696163113753\n",
      "Epoch [9/25], Train Loss: 0.00045494764344766736, Validation Loss: 0.0027557683740781634\n",
      "Epoch [9/25], Train Loss: 0.00034346155007369816, Validation Loss: 0.0027552247458301924\n",
      "Epoch [9/25], Train Loss: 0.0004509634163696319, Validation Loss: 0.0026991101737063724\n",
      "Epoch [10/25], Train Loss: 0.00037959596375003457, Validation Loss: 0.002716092676503443\n",
      "Epoch [10/25], Train Loss: 0.0006162704084999859, Validation Loss: 0.00278203869081971\n",
      "Epoch [10/25], Train Loss: 0.0004550130106508732, Validation Loss: 0.0027237891516953454\n",
      "Epoch [10/25], Train Loss: 0.00029722595354542136, Validation Loss: 0.0026830092049008156\n",
      "Epoch [10/25], Train Loss: 0.0004899919149465859, Validation Loss: 0.0026865514100944043\n",
      "Epoch [10/25], Train Loss: 0.000484732270706445, Validation Loss: 0.0027612224769066364\n",
      "Epoch [10/25], Train Loss: 0.0004155531642027199, Validation Loss: 0.0027786833566746543\n",
      "Epoch [10/25], Train Loss: 0.0004812825354747474, Validation Loss: 0.0027500209359436474\n",
      "Epoch [10/25], Train Loss: 0.0004182381380815059, Validation Loss: 0.002705087818737541\n",
      "Epoch [10/25], Train Loss: 0.0005399162764661014, Validation Loss: 0.0026877811022040224\n",
      "Epoch [10/25], Train Loss: 0.0003920394810847938, Validation Loss: 0.0027451226820370983\n",
      "Epoch [10/25], Train Loss: 0.000467834179289639, Validation Loss: 0.0027609875057863086\n",
      "Epoch [10/25], Train Loss: 0.00043081582407467067, Validation Loss: 0.0027349468167996455\n",
      "Epoch [10/25], Train Loss: 0.00043456972343847156, Validation Loss: 0.002724634432586051\n",
      "Epoch [10/25], Train Loss: 0.00037159936618991196, Validation Loss: 0.0027183541742411732\n",
      "Epoch [11/25], Train Loss: 0.0004574726917780936, Validation Loss: 0.0027391650017444826\n",
      "Epoch [11/25], Train Loss: 0.00044596611405722797, Validation Loss: 0.0027211676097373253\n",
      "Epoch [11/25], Train Loss: 0.0004099045181646943, Validation Loss: 0.0026848223412550298\n",
      "Epoch [11/25], Train Loss: 0.0003584593068808317, Validation Loss: 0.002695320878459626\n",
      "Epoch [11/25], Train Loss: 0.0004640948318410665, Validation Loss: 0.0027748901385250464\n",
      "Epoch [11/25], Train Loss: 0.0004280563734937459, Validation Loss: 0.0027705912591748628\n",
      "Epoch [11/25], Train Loss: 0.0005003312835469842, Validation Loss: 0.002689114458733747\n",
      "Epoch [11/25], Train Loss: 0.00045156010310165584, Validation Loss: 0.002674081566266152\n",
      "Epoch [11/25], Train Loss: 0.00040378887206315994, Validation Loss: 0.00271304979129937\n",
      "Epoch [11/25], Train Loss: 0.000341998937074095, Validation Loss: 0.00272001137276094\n",
      "Epoch [11/25], Train Loss: 0.00042885183938778937, Validation Loss: 0.0026503262607020742\n",
      "Epoch [11/25], Train Loss: 0.0005164732574485242, Validation Loss: 0.002783638994530344\n",
      "Epoch [11/25], Train Loss: 0.0004260051064193249, Validation Loss: 0.002790247504951573\n",
      "Epoch [11/25], Train Loss: 0.0004844925133511424, Validation Loss: 0.002680792653260111\n",
      "Epoch [11/25], Train Loss: 0.0003307141305413097, Validation Loss: 0.0027627424638280097\n",
      "Epoch [12/25], Train Loss: 0.00039012599154375494, Validation Loss: 0.002749634599469665\n",
      "Epoch [12/25], Train Loss: 0.0005351286963559687, Validation Loss: 0.002665749115149389\n",
      "Epoch [12/25], Train Loss: 0.0004244046867825091, Validation Loss: 0.0026704616836827843\n",
      "Epoch [12/25], Train Loss: 0.00038314537960104644, Validation Loss: 0.002817785829117819\n",
      "Epoch [12/25], Train Loss: 0.00041460400098003447, Validation Loss: 0.00273632093238793\n",
      "Epoch [12/25], Train Loss: 0.0003906434867531061, Validation Loss: 0.002705415808634848\n",
      "Epoch [12/25], Train Loss: 0.0005707370000891387, Validation Loss: 0.002778590422029756\n",
      "Epoch [12/25], Train Loss: 0.0003693797334562987, Validation Loss: 0.00268985062227154\n",
      "Epoch [12/25], Train Loss: 0.00037219334626570344, Validation Loss: 0.002739216932760818\n",
      "Epoch [12/25], Train Loss: 0.00039008542080409825, Validation Loss: 0.0027585802012819703\n",
      "Epoch [12/25], Train Loss: 0.0003041948366444558, Validation Loss: 0.0026185728860001604\n",
      "Epoch [12/25], Train Loss: 0.0003054093976970762, Validation Loss: 0.0027062349811885038\n",
      "Epoch [12/25], Train Loss: 0.00039292211295105517, Validation Loss: 0.0028427505235680763\n",
      "Epoch [12/25], Train Loss: 0.000326575362123549, Validation Loss: 0.0026741217461829425\n",
      "Epoch [12/25], Train Loss: 0.00037685289862565696, Validation Loss: 0.002705452621311826\n",
      "Epoch [13/25], Train Loss: 0.0003703040420077741, Validation Loss: 0.0027314103169351063\n",
      "Epoch [13/25], Train Loss: 0.0003932954277843237, Validation Loss: 0.002682732351656471\n",
      "Epoch [13/25], Train Loss: 0.0003353868378326297, Validation Loss: 0.002751391680378999\n",
      "Epoch [13/25], Train Loss: 0.00039024167926982045, Validation Loss: 0.0027548122971060396\n",
      "Epoch [13/25], Train Loss: 0.0003319876850582659, Validation Loss: 0.002679206957226666\n",
      "Epoch [13/25], Train Loss: 0.0003450351650826633, Validation Loss: 0.002697516939047129\n",
      "Epoch [13/25], Train Loss: 0.00032422764343209565, Validation Loss: 0.0026318174845562513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Train Loss: 0.00029142879066057503, Validation Loss: 0.0027153767535791676\n",
      "Epoch [13/25], Train Loss: 0.00036504535819403827, Validation Loss: 0.0027288869336372663\n",
      "Epoch [13/25], Train Loss: 0.00035688994103111327, Validation Loss: 0.0027595575988887486\n",
      "Epoch [13/25], Train Loss: 0.00037101321504451334, Validation Loss: 0.0027462556163835175\n",
      "Epoch [13/25], Train Loss: 0.0002882085391320288, Validation Loss: 0.002683883039828609\n",
      "Epoch [13/25], Train Loss: 0.00029277821886353195, Validation Loss: 0.002719680913684874\n",
      "Epoch [13/25], Train Loss: 0.0003045405319426209, Validation Loss: 0.0026028893512337146\n",
      "Epoch [13/25], Train Loss: 0.00039045416633598506, Validation Loss: 0.002771585213192621\n",
      "Epoch [14/25], Train Loss: 0.0003527869121171534, Validation Loss: 0.0026364786268657995\n",
      "Epoch [14/25], Train Loss: 0.00029886531410738826, Validation Loss: 0.002646136046739436\n",
      "Epoch [14/25], Train Loss: 0.0003577128518372774, Validation Loss: 0.0027793983913206753\n",
      "Epoch [14/25], Train Loss: 0.00041441278881393373, Validation Loss: 0.002746692324495491\n",
      "Epoch [14/25], Train Loss: 0.0003769381728488952, Validation Loss: 0.0027157753615976634\n",
      "Epoch [14/25], Train Loss: 0.00029006446129642427, Validation Loss: 0.002750587898126545\n",
      "Epoch [14/25], Train Loss: 0.000361362675903365, Validation Loss: 0.0027724497368512282\n",
      "Epoch [14/25], Train Loss: 0.0002783646050374955, Validation Loss: 0.00273423341196813\n",
      "Epoch [14/25], Train Loss: 0.00031272790511138737, Validation Loss: 0.00266706696398431\n",
      "Epoch [14/25], Train Loss: 0.0003437204868532717, Validation Loss: 0.002708820514615719\n",
      "Epoch [14/25], Train Loss: 0.0002996270777657628, Validation Loss: 0.0026688568603184795\n",
      "Epoch [14/25], Train Loss: 0.00028163401293568313, Validation Loss: 0.002669552356756034\n",
      "Epoch [14/25], Train Loss: 0.0003085118078161031, Validation Loss: 0.002705961649752214\n",
      "Epoch [14/25], Train Loss: 0.00025420376914553344, Validation Loss: 0.002730356085215922\n",
      "Epoch [14/25], Train Loss: 0.0002186535857617855, Validation Loss: 0.002711188774008085\n",
      "Epoch [15/25], Train Loss: 0.00032091481261886656, Validation Loss: 0.0027187768616439667\n",
      "Epoch [15/25], Train Loss: 0.00032768509117886424, Validation Loss: 0.0027392349448524603\n",
      "Epoch [15/25], Train Loss: 0.0003257922944612801, Validation Loss: 0.002710827291716172\n",
      "Epoch [15/25], Train Loss: 0.0003471026720944792, Validation Loss: 0.002700902838572985\n",
      "Epoch [15/25], Train Loss: 0.00020412357116583735, Validation Loss: 0.0027068462007592956\n",
      "Epoch [15/25], Train Loss: 0.00030717538902536035, Validation Loss: 0.0027031815462127454\n",
      "Epoch [15/25], Train Loss: 0.00032678566640242934, Validation Loss: 0.0026992590659681488\n",
      "Epoch [15/25], Train Loss: 0.00029739338788203895, Validation Loss: 0.0027105741852409197\n",
      "Epoch [15/25], Train Loss: 0.0002299816842423752, Validation Loss: 0.002727446934803068\n",
      "Epoch [15/25], Train Loss: 0.00030527610215358436, Validation Loss: 0.0027263222727924585\n",
      "Epoch [15/25], Train Loss: 0.0003266234998591244, Validation Loss: 0.002729260006590801\n",
      "Epoch [15/25], Train Loss: 0.0003014506946783513, Validation Loss: 0.002734800063057983\n",
      "Epoch [15/25], Train Loss: 0.0002461707917973399, Validation Loss: 0.0027194801510601234\n",
      "Epoch [15/25], Train Loss: 0.00031756560201756656, Validation Loss: 0.002710905802600524\n",
      "Epoch [15/25], Train Loss: 0.000289096002234146, Validation Loss: 0.0027066327204943457\n",
      "Epoch [16/25], Train Loss: 0.00019354918913450092, Validation Loss: 0.002712956541648307\n",
      "Epoch [16/25], Train Loss: 0.0003342309792060405, Validation Loss: 0.002728976876701878\n",
      "Epoch [16/25], Train Loss: 0.00027080276049673557, Validation Loss: 0.0027021046536152855\n",
      "Epoch [16/25], Train Loss: 0.00027580547612160444, Validation Loss: 0.0026880799100084717\n",
      "Epoch [16/25], Train Loss: 0.000268288393272087, Validation Loss: 0.0026935896613173374\n",
      "Epoch [16/25], Train Loss: 0.0002839550725184381, Validation Loss: 0.0027016296751891113\n",
      "Epoch [16/25], Train Loss: 0.00029441429069265723, Validation Loss: 0.0027158736317817655\n",
      "Epoch [16/25], Train Loss: 0.00036987694329582155, Validation Loss: 0.002738523535712176\n",
      "Epoch [16/25], Train Loss: 0.0002589436189737171, Validation Loss: 0.0027381295314794327\n",
      "Epoch [16/25], Train Loss: 0.0003394496161490679, Validation Loss: 0.0027296937935586485\n",
      "Epoch [16/25], Train Loss: 0.00028366109472699463, Validation Loss: 0.0027198405552874595\n",
      "Epoch [16/25], Train Loss: 0.00026091159088537097, Validation Loss: 0.0027075833034459033\n",
      "Epoch [16/25], Train Loss: 0.0003247402491979301, Validation Loss: 0.0027083003826971564\n",
      "Epoch [16/25], Train Loss: 0.00033492353395558894, Validation Loss: 0.0027218309148926943\n",
      "Epoch [16/25], Train Loss: 0.00029811618151143193, Validation Loss: 0.0027340989728153004\n",
      "Epoch [17/25], Train Loss: 0.0003248243301641196, Validation Loss: 0.0027323544804132033\n",
      "Epoch [17/25], Train Loss: 0.00031160228536464274, Validation Loss: 0.002723365088830851\n",
      "Epoch [17/25], Train Loss: 0.00024690359714441, Validation Loss: 0.0027069784055205703\n",
      "Epoch [17/25], Train Loss: 0.0003286405699327588, Validation Loss: 0.0027017969571586165\n",
      "Epoch [17/25], Train Loss: 0.00028559393831528723, Validation Loss: 0.0027033802798847443\n",
      "Epoch [17/25], Train Loss: 0.0002747420803643763, Validation Loss: 0.002714664868994796\n",
      "Epoch [17/25], Train Loss: 0.0003054665576200932, Validation Loss: 0.002728667204119578\n",
      "Epoch [17/25], Train Loss: 0.0003156998718623072, Validation Loss: 0.002734810781093831\n",
      "Epoch [17/25], Train Loss: 0.00031474101706407964, Validation Loss: 0.002734148530524318\n",
      "Epoch [17/25], Train Loss: 0.00026900763623416424, Validation Loss: 0.0027271587393802254\n",
      "Epoch [17/25], Train Loss: 0.00031319932895712554, Validation Loss: 0.0027178191372482967\n",
      "Epoch [17/25], Train Loss: 0.0002539736742619425, Validation Loss: 0.0027121130549231497\n",
      "Epoch [17/25], Train Loss: 0.0002856061328202486, Validation Loss: 0.0027108366890739996\n",
      "Epoch [17/25], Train Loss: 0.0002317610924364999, Validation Loss: 0.002711606703056883\n",
      "Epoch [17/25], Train Loss: 0.00029866863042116165, Validation Loss: 0.0027196915338927206\n",
      "Epoch [18/25], Train Loss: 0.000348982575815171, Validation Loss: 0.0027285716829023193\n",
      "Epoch [18/25], Train Loss: 0.000350582180544734, Validation Loss: 0.002723003031310289\n",
      "Epoch [18/25], Train Loss: 0.00029817759059369564, Validation Loss: 0.0027175987777182786\n",
      "Epoch [18/25], Train Loss: 0.00027491876971907914, Validation Loss: 0.002719659686965101\n",
      "Epoch [18/25], Train Loss: 0.00027360301464796066, Validation Loss: 0.0027132044025861167\n",
      "Epoch [18/25], Train Loss: 0.0003115030121989548, Validation Loss: 0.00271388834944981\n",
      "Epoch [18/25], Train Loss: 0.0003177272737957537, Validation Loss: 0.002724513196300308\n",
      "Epoch [18/25], Train Loss: 0.0002148304192814976, Validation Loss: 0.0027225519307427296\n",
      "Epoch [18/25], Train Loss: 0.00025148625718429685, Validation Loss: 0.0027256112432248202\n",
      "Epoch [18/25], Train Loss: 0.00022965930111240596, Validation Loss: 0.002731970278546214\n",
      "Epoch [18/25], Train Loss: 0.00026620185235515237, Validation Loss: 0.002724661333329913\n",
      "Epoch [18/25], Train Loss: 0.0002670910907909274, Validation Loss: 0.002721874702706182\n",
      "Epoch [18/25], Train Loss: 0.00029152457136660814, Validation Loss: 0.00272034843914023\n",
      "Epoch [18/25], Train Loss: 0.0003873661335092038, Validation Loss: 0.0027150464842894246\n",
      "Epoch [18/25], Train Loss: 0.0002619891311042011, Validation Loss: 0.002713306286536595\n",
      "Epoch [19/25], Train Loss: 0.00020834477618336678, Validation Loss: 0.0027157266628184988\n",
      "Epoch [19/25], Train Loss: 0.0002670463582035154, Validation Loss: 0.0027169787125395878\n",
      "Epoch [19/25], Train Loss: 0.00021929210925009102, Validation Loss: 0.002722615929821334\n",
      "Epoch [19/25], Train Loss: 0.0002961498103104532, Validation Loss: 0.0027224110721164392\n",
      "Epoch [19/25], Train Loss: 0.00030615238938480616, Validation Loss: 0.0027196486500099676\n",
      "Epoch [19/25], Train Loss: 0.0002927750756498426, Validation Loss: 0.002716364127685543\n",
      "Epoch [19/25], Train Loss: 0.00029945970163680613, Validation Loss: 0.0027148819845482833\n",
      "Epoch [19/25], Train Loss: 0.00025288877077400684, Validation Loss: 0.0027169722187168456\n",
      "Epoch [19/25], Train Loss: 0.00023407039407175034, Validation Loss: 0.002722046248019994\n",
      "Epoch [19/25], Train Loss: 0.0002590275544207543, Validation Loss: 0.002730018347736542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], Train Loss: 0.0004003746435046196, Validation Loss: 0.0027329971047290484\n",
      "Epoch [19/25], Train Loss: 0.0003221461665816605, Validation Loss: 0.002731161217494797\n",
      "Epoch [19/25], Train Loss: 0.00037562623037956655, Validation Loss: 0.0027302502020567403\n",
      "Epoch [19/25], Train Loss: 0.0003373035870026797, Validation Loss: 0.002723047959798274\n",
      "Epoch [19/25], Train Loss: 0.0002647694491315633, Validation Loss: 0.0027170617176423303\n",
      "Epoch [20/25], Train Loss: 0.0003099013410974294, Validation Loss: 0.0027180972739960217\n",
      "Epoch [20/25], Train Loss: 0.000253626931225881, Validation Loss: 0.002716937190422616\n",
      "Epoch [20/25], Train Loss: 0.00029233700479380786, Validation Loss: 0.0027174904429893786\n",
      "Epoch [20/25], Train Loss: 0.0002301684726262465, Validation Loss: 0.002720539880712994\n",
      "Epoch [20/25], Train Loss: 0.000276475417194888, Validation Loss: 0.0027147671188220007\n",
      "Epoch [20/25], Train Loss: 0.00024873350048437715, Validation Loss: 0.0027139527515797806\n",
      "Epoch [20/25], Train Loss: 0.0002916630473919213, Validation Loss: 0.002724209092087856\n",
      "Epoch [20/25], Train Loss: 0.0003120184992440045, Validation Loss: 0.0027237615426767524\n",
      "Epoch [20/25], Train Loss: 0.0003320082905702293, Validation Loss: 0.0027247297248857863\n",
      "Epoch [20/25], Train Loss: 0.000260953966062516, Validation Loss: 0.002728731183632582\n",
      "Epoch [20/25], Train Loss: 0.0002805604599416256, Validation Loss: 0.002722956977800286\n",
      "Epoch [20/25], Train Loss: 0.00030344651895575225, Validation Loss: 0.002723083764846836\n",
      "Epoch [20/25], Train Loss: 0.0003011981025338173, Validation Loss: 0.0027211053656930684\n",
      "Epoch [20/25], Train Loss: 0.00031403935281559825, Validation Loss: 0.002721776268171037\n",
      "Epoch [20/25], Train Loss: 0.0003319731040392071, Validation Loss: 0.0027277199571028726\n",
      "Epoch [21/25], Train Loss: 0.00026464302209205925, Validation Loss: 0.0027286488907176908\n",
      "Epoch [21/25], Train Loss: 0.0002587360795587301, Validation Loss: 0.0027265386194178535\n",
      "Epoch [21/25], Train Loss: 0.00020591894281096756, Validation Loss: 0.002728961893365163\n",
      "Epoch [21/25], Train Loss: 0.00028176335035823286, Validation Loss: 0.0027226030987006528\n",
      "Epoch [21/25], Train Loss: 0.00027107528876513243, Validation Loss: 0.0027215285441924294\n",
      "Epoch [21/25], Train Loss: 0.0002795263135340065, Validation Loss: 0.002724223271278398\n",
      "Epoch [21/25], Train Loss: 0.0002711914712563157, Validation Loss: 0.002718810919484421\n",
      "Epoch [21/25], Train Loss: 0.0003253016038797796, Validation Loss: 0.002720385173554806\n",
      "Epoch [21/25], Train Loss: 0.0003137425519526005, Validation Loss: 0.002722102784778641\n",
      "Epoch [21/25], Train Loss: 0.00026641463045962155, Validation Loss: 0.002718971126532855\n",
      "Epoch [21/25], Train Loss: 0.0003515378339216113, Validation Loss: 0.0027216063018011697\n",
      "Epoch [21/25], Train Loss: 0.0002758349000941962, Validation Loss: 0.0027226986707884725\n",
      "Epoch [21/25], Train Loss: 0.0003214780008420348, Validation Loss: 0.002730297441242122\n",
      "Epoch [21/25], Train Loss: 0.0003412352816667408, Validation Loss: 0.0027292657060501704\n",
      "Epoch [21/25], Train Loss: 0.0002836854837369174, Validation Loss: 0.002734596103414142\n",
      "Epoch [22/25], Train Loss: 0.0002637048892211169, Validation Loss: 0.002729142319504954\n",
      "Epoch [22/25], Train Loss: 0.0002592753153294325, Validation Loss: 0.0027207684323598607\n",
      "Epoch [22/25], Train Loss: 0.0002627146022859961, Validation Loss: 0.0027198206159841864\n",
      "Epoch [22/25], Train Loss: 0.0002111757203238085, Validation Loss: 0.0027172422807413238\n",
      "Epoch [22/25], Train Loss: 0.00029252111562527716, Validation Loss: 0.0027109912866648254\n",
      "Epoch [22/25], Train Loss: 0.00028328539337962866, Validation Loss: 0.0027196182900679714\n",
      "Epoch [22/25], Train Loss: 0.00024427546304650605, Validation Loss: 0.0027235345249729498\n",
      "Epoch [22/25], Train Loss: 0.00036869297036901116, Validation Loss: 0.0027231416438056643\n",
      "Epoch [22/25], Train Loss: 0.00021329957235138863, Validation Loss: 0.002730546597422672\n",
      "Epoch [22/25], Train Loss: 0.00031358207343146205, Validation Loss: 0.0027308305960242487\n",
      "Epoch [22/25], Train Loss: 0.0003102839400526136, Validation Loss: 0.0027301094314756513\n",
      "Epoch [22/25], Train Loss: 0.00030158081790432334, Validation Loss: 0.002735372176863441\n",
      "Epoch [22/25], Train Loss: 0.0002974465023726225, Validation Loss: 0.0027251120856297867\n",
      "Epoch [22/25], Train Loss: 0.0003536667791195214, Validation Loss: 0.0027193290107108714\n",
      "Epoch [22/25], Train Loss: 0.00036210971302352846, Validation Loss: 0.0027230763025268786\n",
      "Epoch [23/25], Train Loss: 0.0003106084477622062, Validation Loss: 0.0027154987783586026\n",
      "Epoch [23/25], Train Loss: 0.0003061670868191868, Validation Loss: 0.002722164291199766\n",
      "Epoch [23/25], Train Loss: 0.0003097232256550342, Validation Loss: 0.002734086977145752\n",
      "Epoch [23/25], Train Loss: 0.0002676532312761992, Validation Loss: 0.002732973498832278\n",
      "Epoch [23/25], Train Loss: 0.00029513105982914567, Validation Loss: 0.0027365574100594813\n",
      "Epoch [23/25], Train Loss: 0.00023476654314436018, Validation Loss: 0.0027341196164801843\n",
      "Epoch [23/25], Train Loss: 0.0002919357502833009, Validation Loss: 0.0027218692732520963\n",
      "Epoch [23/25], Train Loss: 0.00028716662200167775, Validation Loss: 0.0027151661846755433\n",
      "Epoch [23/25], Train Loss: 0.00032913702307268977, Validation Loss: 0.0027113471066720096\n",
      "Epoch [23/25], Train Loss: 0.0002331726427655667, Validation Loss: 0.0027071796611984485\n",
      "Epoch [23/25], Train Loss: 0.00031159407808445394, Validation Loss: 0.0027114541168097687\n",
      "Epoch [23/25], Train Loss: 0.0003161770582664758, Validation Loss: 0.0027172005747077093\n",
      "Epoch [23/25], Train Loss: 0.00028468170785345137, Validation Loss: 0.0027255049315789916\n",
      "Epoch [23/25], Train Loss: 0.00024303093960043043, Validation Loss: 0.002737650759283359\n",
      "Epoch [23/25], Train Loss: 0.0002606487541925162, Validation Loss: 0.0027382066610324033\n",
      "Epoch [24/25], Train Loss: 0.00032179596018977463, Validation Loss: 0.002735675813458046\n",
      "Epoch [24/25], Train Loss: 0.0003036946873180568, Validation Loss: 0.002732749590102364\n",
      "Epoch [24/25], Train Loss: 0.0003273747570347041, Validation Loss: 0.002721982354595631\n",
      "Epoch [24/25], Train Loss: 0.00037657833308912814, Validation Loss: 0.002713694728269171\n",
      "Epoch [24/25], Train Loss: 0.00026380608323961496, Validation Loss: 0.0027132089868062686\n",
      "Epoch [24/25], Train Loss: 0.00027225675876252353, Validation Loss: 0.0027123946176074885\n",
      "Epoch [24/25], Train Loss: 0.00022326978796627373, Validation Loss: 0.0027187547368631394\n",
      "Epoch [24/25], Train Loss: 0.0002198408474214375, Validation Loss: 0.002729842271029699\n",
      "Epoch [24/25], Train Loss: 0.00031548680271953344, Validation Loss: 0.0027360630166881226\n",
      "Epoch [24/25], Train Loss: 0.0002667113149072975, Validation Loss: 0.0027398914920494836\n",
      "Epoch [24/25], Train Loss: 0.00030460100970230997, Validation Loss: 0.0027352977297542725\n",
      "Epoch [24/25], Train Loss: 0.00023247352510225028, Validation Loss: 0.0027238925363273683\n",
      "Epoch [24/25], Train Loss: 0.00022528147383127362, Validation Loss: 0.002715656964280525\n",
      "Epoch [24/25], Train Loss: 0.00030002230778336525, Validation Loss: 0.002706151576947515\n",
      "Epoch [24/25], Train Loss: 0.0003069753583986312, Validation Loss: 0.0027081874148341036\n",
      "Epoch [25/25], Train Loss: 0.00030123605392873287, Validation Loss: 0.0027215151945833398\n",
      "Epoch [25/25], Train Loss: 0.0003012727538589388, Validation Loss: 0.0027240782099611617\n",
      "Epoch [25/25], Train Loss: 0.000270680378889665, Validation Loss: 0.002735045797215039\n",
      "Epoch [25/25], Train Loss: 0.00027151312679052353, Validation Loss: 0.0027452652683058955\n",
      "Epoch [25/25], Train Loss: 0.00031637243228033185, Validation Loss: 0.0027352191621296796\n",
      "Epoch [25/25], Train Loss: 0.00022532106959261, Validation Loss: 0.002728067579123528\n",
      "Epoch [25/25], Train Loss: 0.0003575401788111776, Validation Loss: 0.002720704239581813\n",
      "Epoch [25/25], Train Loss: 0.0002520744747016579, Validation Loss: 0.002706543409398624\n",
      "Epoch [25/25], Train Loss: 0.0002961022546514869, Validation Loss: 0.002709177359860359\n",
      "Epoch [25/25], Train Loss: 0.00026483708643354475, Validation Loss: 0.0027139644968096448\n",
      "Epoch [25/25], Train Loss: 0.00022046251979190856, Validation Loss: 0.0027225896082192407\n",
      "Epoch [25/25], Train Loss: 0.00027236167807132006, Validation Loss: 0.002737241472360216\n",
      "Epoch [25/25], Train Loss: 0.00029064336558803916, Validation Loss: 0.0027401917887961164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], Train Loss: 0.00028272837516851723, Validation Loss: 0.0027382810514013306\n",
      "Epoch [25/25], Train Loss: 0.00031939303153194487, Validation Loss: 0.002732982293569616\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hyperparameters\n",
    "input_channels = 16  # NUM_CHANNELS\n",
    "num_filters = 32\n",
    "batch_size = 16  # Batch size increased\n",
    "learning_rate = 0.001\n",
    "num_epochs = 25\n",
    "\n",
    "# Initialize the model\n",
    "model_preictal = DDPM(input_channels, num_filters)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for reconstruction\n",
    "optimizer = optim.Adam(model_preictal.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses_preictal = []\n",
    "val_losses_preictal = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model_preictal.train()  # Set model to training mode\n",
    "    for i, (batch_X, _) in enumerate(dataloader):\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed = model_preictal(batch_X)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(reconstructed, batch_X)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append losses to the lists after each epoch\n",
    "        train_losses_preictal.append(loss.item())\n",
    "        \n",
    "        \n",
    "        # Print progress\n",
    "       # print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], Loss: {loss.item()}\")\n",
    "        \n",
    "        # Validation\n",
    "        model_preictal.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X_val, _ in val_dataloader:\n",
    "                reconstructed_val = model_preictal(batch_X_val)\n",
    "                val_loss += criterion(reconstructed_val, batch_X_val).item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_losses_preictal.append(val_loss)\n",
    "\n",
    "        # Print training and validation loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()}, Validation Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train_losses_preictal and val_losses_preictal contain the loss values across epochs\n",
    "# Plotting training and validation loss\n",
    "epochs = range(1, len(train_losses_preictal) + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_losses_preictal, label='Training Loss')\n",
    "plt.plot(epochs, val_losses_preictal, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss for Preictal data generation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Train Loss: 0.005651488434523344, Validation Loss: 0.0007026414166815078\n",
      "Epoch [1/25], Train Loss: 0.0023823590017855167, Validation Loss: 0.014187154281853126\n",
      "Epoch [1/25], Train Loss: 0.006827794015407562, Validation Loss: 0.0023995428209138267\n",
      "Epoch [1/25], Train Loss: 0.0006195706664584577, Validation Loss: 0.0006550967115165369\n",
      "Epoch [1/25], Train Loss: 0.0017112356144934893, Validation Loss: 0.0004221482083549676\n",
      "Epoch [1/25], Train Loss: 0.002285252558067441, Validation Loss: 0.0004042473848138255\n",
      "Epoch [1/25], Train Loss: 0.0025269074831157923, Validation Loss: 0.00044888362974249325\n",
      "Epoch [1/25], Train Loss: 0.0024740141816437244, Validation Loss: 0.0005600549514941527\n",
      "Epoch [1/25], Train Loss: 0.002156396396458149, Validation Loss: 0.000792909788746512\n",
      "Epoch [1/25], Train Loss: 0.001940809772349894, Validation Loss: 0.0012280890080263885\n",
      "Epoch [1/25], Train Loss: 0.0013697471003979445, Validation Loss: 0.001983702390165139\n",
      "Epoch [1/25], Train Loss: 0.0012055778643116355, Validation Loss: 0.003015054890713772\n",
      "Epoch [1/25], Train Loss: 0.0012581957271322608, Validation Loss: 0.003747452754809075\n",
      "Epoch [1/25], Train Loss: 0.0011751188430935144, Validation Loss: 0.00375353847434666\n",
      "Epoch [1/25], Train Loss: 0.0010938645573332906, Validation Loss: 0.0032865549574660905\n",
      "Epoch [2/25], Train Loss: 0.0008556810207664967, Validation Loss: 0.0027754887715451607\n",
      "Epoch [2/25], Train Loss: 0.0006290278979577124, Validation Loss: 0.002427798050197483\n",
      "Epoch [2/25], Train Loss: 0.0006256779306568205, Validation Loss: 0.002267693427680921\n",
      "Epoch [2/25], Train Loss: 0.0005915889050811529, Validation Loss: 0.0022260628938048825\n",
      "Epoch [2/25], Train Loss: 0.0006476501584984362, Validation Loss: 0.0022892527020058964\n",
      "Epoch [2/25], Train Loss: 0.0004932245356030762, Validation Loss: 0.002498430530579776\n",
      "Epoch [2/25], Train Loss: 0.000607490015681833, Validation Loss: 0.002893942386741523\n",
      "Epoch [2/25], Train Loss: 0.0006069816299714148, Validation Loss: 0.0032808685972660527\n",
      "Epoch [2/25], Train Loss: 0.0005675256252288818, Validation Loss: 0.0033155054190516973\n",
      "Epoch [2/25], Train Loss: 0.0005162874585948884, Validation Loss: 0.003167457307274101\n",
      "Epoch [2/25], Train Loss: 0.0006455840775743127, Validation Loss: 0.002996622578890509\n",
      "Epoch [2/25], Train Loss: 0.00043494923738762736, Validation Loss: 0.002942627963038678\n",
      "Epoch [2/25], Train Loss: 0.0004767407663166523, Validation Loss: 0.002942645362727031\n",
      "Epoch [2/25], Train Loss: 0.0005462364060804248, Validation Loss: 0.00295446885107946\n",
      "Epoch [2/25], Train Loss: 0.00032893873867578804, Validation Loss: 0.002976455950733869\n",
      "Epoch [3/25], Train Loss: 0.0005750261479988694, Validation Loss: 0.003005540377984778\n",
      "Epoch [3/25], Train Loss: 0.00048065645387396216, Validation Loss: 0.0030353810665581155\n",
      "Epoch [3/25], Train Loss: 0.0005752681172452867, Validation Loss: 0.003059788274445704\n",
      "Epoch [3/25], Train Loss: 0.0005139873828738928, Validation Loss: 0.00307427513013993\n",
      "Epoch [3/25], Train Loss: 0.0005254529532976449, Validation Loss: 0.003073450715961952\n",
      "Epoch [3/25], Train Loss: 0.0004764259501826018, Validation Loss: 0.0030550868602610436\n",
      "Epoch [3/25], Train Loss: 0.0004028634284622967, Validation Loss: 0.003023879285588735\n",
      "Epoch [3/25], Train Loss: 0.0005194312543608248, Validation Loss: 0.00298421221448719\n",
      "Epoch [3/25], Train Loss: 0.000409935339121148, Validation Loss: 0.002941534240111834\n",
      "Epoch [3/25], Train Loss: 0.0003740318934433162, Validation Loss: 0.0029016620334738692\n",
      "Epoch [3/25], Train Loss: 0.00035680868313647807, Validation Loss: 0.0028652029538511477\n",
      "Epoch [3/25], Train Loss: 0.0004925295943394303, Validation Loss: 0.0028394388500601053\n",
      "Epoch [3/25], Train Loss: 0.00043257896322757006, Validation Loss: 0.0028240739489209\n",
      "Epoch [3/25], Train Loss: 0.0004995962372049689, Validation Loss: 0.002820352100211532\n",
      "Epoch [3/25], Train Loss: 0.0004990394227206707, Validation Loss: 0.0028266661150866197\n",
      "Epoch [4/25], Train Loss: 0.00043746904702857137, Validation Loss: 0.002840968797828595\n",
      "Epoch [4/25], Train Loss: 0.0005040235118940473, Validation Loss: 0.002858706918239844\n",
      "Epoch [4/25], Train Loss: 0.0003570986445993185, Validation Loss: 0.0028788353093438038\n",
      "Epoch [4/25], Train Loss: 0.0004978845245204866, Validation Loss: 0.0028989052560430867\n",
      "Epoch [4/25], Train Loss: 0.00045531729119829834, Validation Loss: 0.002915868015192887\n",
      "Epoch [4/25], Train Loss: 0.00039675377774983644, Validation Loss: 0.002926681934221953\n",
      "Epoch [4/25], Train Loss: 0.00039911855128593743, Validation Loss: 0.002929241320180918\n",
      "Epoch [4/25], Train Loss: 0.0004957375349476933, Validation Loss: 0.0029256733940491654\n",
      "Epoch [4/25], Train Loss: 0.000511091435328126, Validation Loss: 0.0029187345380323523\n",
      "Epoch [4/25], Train Loss: 0.0004077772027812898, Validation Loss: 0.002909795193299025\n",
      "Epoch [4/25], Train Loss: 0.0005438184016384184, Validation Loss: 0.002901294016271454\n",
      "Epoch [4/25], Train Loss: 0.00043990995618514717, Validation Loss: 0.002897323590523305\n",
      "Epoch [4/25], Train Loss: 0.0005543336155824363, Validation Loss: 0.0029002110289894985\n",
      "Epoch [4/25], Train Loss: 0.0004618557868525386, Validation Loss: 0.0029099751185594487\n",
      "Epoch [4/25], Train Loss: 0.0003805185842793435, Validation Loss: 0.0029254252670191916\n",
      "Epoch [5/25], Train Loss: 0.00046905738417990506, Validation Loss: 0.0029511789990855114\n",
      "Epoch [5/25], Train Loss: 0.0004481026844587177, Validation Loss: 0.0032394937642638674\n",
      "Epoch [5/25], Train Loss: 0.0003649214340839535, Validation Loss: 0.0029185407564137913\n",
      "Epoch [5/25], Train Loss: 0.000496699009090662, Validation Loss: 0.002882887004725948\n",
      "Epoch [5/25], Train Loss: 0.00045000051613897085, Validation Loss: 0.0028668532672361667\n",
      "Epoch [5/25], Train Loss: 0.00036417233059182763, Validation Loss: 0.0028687821028112364\n",
      "Epoch [5/25], Train Loss: 0.00044435664312914014, Validation Loss: 0.0028872793078610374\n",
      "Epoch [5/25], Train Loss: 0.0006202292279340327, Validation Loss: 0.00291756064310169\n",
      "Epoch [5/25], Train Loss: 0.0004263928858563304, Validation Loss: 0.0029534397357213896\n",
      "Epoch [5/25], Train Loss: 0.000445054960437119, Validation Loss: 0.0029889668275390855\n",
      "Epoch [5/25], Train Loss: 0.00046878831926733255, Validation Loss: 0.003013910539839573\n",
      "Epoch [5/25], Train Loss: 0.0004619980463758111, Validation Loss: 0.0030252293159230416\n",
      "Epoch [5/25], Train Loss: 0.00043033395195379853, Validation Loss: 0.003019900564026056\n",
      "Epoch [5/25], Train Loss: 0.00047768736840225756, Validation Loss: 0.0029994322147470815\n",
      "Epoch [5/25], Train Loss: 0.0004397732554934919, Validation Loss: 0.0029679805455150223\n",
      "Epoch [6/25], Train Loss: 0.0005474209901876748, Validation Loss: 0.0029359447374595563\n",
      "Epoch [6/25], Train Loss: 0.0005570436478592455, Validation Loss: 0.0029082657953239037\n",
      "Epoch [6/25], Train Loss: 0.00034392139059491456, Validation Loss: 0.002890376014118435\n",
      "Epoch [6/25], Train Loss: 0.00041833138675428927, Validation Loss: 0.0028841975184173145\n",
      "Epoch [6/25], Train Loss: 0.00037727627204731107, Validation Loss: 0.0028922479803196524\n",
      "Epoch [6/25], Train Loss: 0.00044951753807254136, Validation Loss: 0.00290976556119736\n",
      "Epoch [6/25], Train Loss: 0.00037267699372023344, Validation Loss: 0.0029305234265771985\n",
      "Epoch [6/25], Train Loss: 0.00042419665260240436, Validation Loss: 0.002949860621979382\n",
      "Epoch [6/25], Train Loss: 0.00040556417661719024, Validation Loss: 0.0029628390638048157\n",
      "Epoch [6/25], Train Loss: 0.00047786763752810657, Validation Loss: 0.002966218062543443\n",
      "Epoch [6/25], Train Loss: 0.0005093434592708945, Validation Loss: 0.0029606110838969715\n",
      "Epoch [6/25], Train Loss: 0.0005550457863137126, Validation Loss: 0.0029505037315221143\n",
      "Epoch [6/25], Train Loss: 0.0005188376526348293, Validation Loss: 0.0029366189347846167\n",
      "Epoch [6/25], Train Loss: 0.0003475857665762305, Validation Loss: 0.002922727438785574\n",
      "Epoch [6/25], Train Loss: 0.0004925198736600578, Validation Loss: 0.0029162573862914778\n",
      "Epoch [7/25], Train Loss: 0.0005539864650927484, Validation Loss: 0.002914717561807953\n",
      "Epoch [7/25], Train Loss: 0.00038852717261761427, Validation Loss: 0.0029192959259757225\n",
      "Epoch [7/25], Train Loss: 0.0003895408590324223, Validation Loss: 0.0029308419115058764\n",
      "Epoch [7/25], Train Loss: 0.0005774367600679398, Validation Loss: 0.0029440935399821576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Train Loss: 0.00033196850563399494, Validation Loss: 0.002956173282914928\n",
      "Epoch [7/25], Train Loss: 0.00037561688804998994, Validation Loss: 0.0029638928846463936\n",
      "Epoch [7/25], Train Loss: 0.00047984858974814415, Validation Loss: 0.0029672012222195124\n",
      "Epoch [7/25], Train Loss: 0.0004264290037099272, Validation Loss: 0.00296500648622315\n",
      "Epoch [7/25], Train Loss: 0.00043839795398525894, Validation Loss: 0.0029579492994122394\n",
      "Epoch [7/25], Train Loss: 0.00045974485692568123, Validation Loss: 0.002947632855336581\n",
      "Epoch [7/25], Train Loss: 0.0005207651411183178, Validation Loss: 0.0029394503334584108\n",
      "Epoch [7/25], Train Loss: 0.0005867669824510813, Validation Loss: 0.0029340573926964976\n",
      "Epoch [7/25], Train Loss: 0.0004184334247838706, Validation Loss: 0.0029314820744692025\n",
      "Epoch [7/25], Train Loss: 0.0003800309496000409, Validation Loss: 0.002930228938857297\n",
      "Epoch [7/25], Train Loss: 0.0004591791657730937, Validation Loss: 0.0029310838812414337\n",
      "Epoch [8/25], Train Loss: 0.0004493261221796274, Validation Loss: 0.002931992570329614\n",
      "Epoch [8/25], Train Loss: 0.0004928032867610455, Validation Loss: 0.002933621805693422\n",
      "Epoch [8/25], Train Loss: 0.00048282751231454313, Validation Loss: 0.0029376886644158044\n",
      "Epoch [8/25], Train Loss: 0.00039503074367530644, Validation Loss: 0.002943624272754713\n",
      "Epoch [8/25], Train Loss: 0.0004897160106338561, Validation Loss: 0.0029513499730837948\n",
      "Epoch [8/25], Train Loss: 0.0003496918943710625, Validation Loss: 0.0029560972431658945\n",
      "Epoch [8/25], Train Loss: 0.0004170176980551332, Validation Loss: 0.0029572197294817503\n",
      "Epoch [8/25], Train Loss: 0.0005335501627996564, Validation Loss: 0.0029542785638771137\n",
      "Epoch [8/25], Train Loss: 0.0003285291895736009, Validation Loss: 0.002948706382650788\n",
      "Epoch [8/25], Train Loss: 0.0005768141709268093, Validation Loss: 0.002939545036833577\n",
      "Epoch [8/25], Train Loss: 0.0004033776349388063, Validation Loss: 0.0029327909074755025\n",
      "Epoch [8/25], Train Loss: 0.00037833419628441334, Validation Loss: 0.002930648527069002\n",
      "Epoch [8/25], Train Loss: 0.0005732322460971773, Validation Loss: 0.0029322486272973926\n",
      "Epoch [8/25], Train Loss: 0.0004517108900472522, Validation Loss: 0.002939205980632736\n",
      "Epoch [8/25], Train Loss: 0.00046560526243411005, Validation Loss: 0.0029467755729066475\n",
      "Epoch [9/25], Train Loss: 0.0004720025463029742, Validation Loss: 0.00295196012828593\n",
      "Epoch [9/25], Train Loss: 0.0005536294775083661, Validation Loss: 0.0029542953042047365\n",
      "Epoch [9/25], Train Loss: 0.000638129364233464, Validation Loss: 0.002951182863291572\n",
      "Epoch [9/25], Train Loss: 0.0004404215142130852, Validation Loss: 0.002945585293742288\n",
      "Epoch [9/25], Train Loss: 0.00040494059794582427, Validation Loss: 0.0029381236214065503\n",
      "Epoch [9/25], Train Loss: 0.00047395500587299466, Validation Loss: 0.002930337520112761\n",
      "Epoch [9/25], Train Loss: 0.00040715470095165074, Validation Loss: 0.0029269669228429054\n",
      "Epoch [9/25], Train Loss: 0.00035515843774192035, Validation Loss: 0.002929145611133896\n",
      "Epoch [9/25], Train Loss: 0.00034208345459774137, Validation Loss: 0.0029338937342761693\n",
      "Epoch [9/25], Train Loss: 0.0005818036734126508, Validation Loss: 0.002941688336823292\n",
      "Epoch [9/25], Train Loss: 0.0004164865822531283, Validation Loss: 0.0029484377997856933\n",
      "Epoch [9/25], Train Loss: 0.00043481827015057206, Validation Loss: 0.0029542287889899326\n",
      "Epoch [9/25], Train Loss: 0.00040316328522749245, Validation Loss: 0.0029569527392565203\n",
      "Epoch [9/25], Train Loss: 0.00037047889782115817, Validation Loss: 0.002955942046867699\n",
      "Epoch [9/25], Train Loss: 0.0004964808467775583, Validation Loss: 0.002949059134771844\n",
      "Epoch [10/25], Train Loss: 0.00045703945215791464, Validation Loss: 0.002939642855052312\n",
      "Epoch [10/25], Train Loss: 0.0005099264672026038, Validation Loss: 0.0029327473309704987\n",
      "Epoch [10/25], Train Loss: 0.0003778713580686599, Validation Loss: 0.0029283747871351592\n",
      "Epoch [10/25], Train Loss: 0.0004504347743932158, Validation Loss: 0.0029272251887669585\n",
      "Epoch [10/25], Train Loss: 0.0004591725883074105, Validation Loss: 0.002931589902449055\n",
      "Epoch [10/25], Train Loss: 0.00038628195761702955, Validation Loss: 0.0029382358222980708\n",
      "Epoch [10/25], Train Loss: 0.00040740572148934007, Validation Loss: 0.002946123609929776\n",
      "Epoch [10/25], Train Loss: 0.00046786191524006426, Validation Loss: 0.0029520036363113325\n",
      "Epoch [10/25], Train Loss: 0.0004591426986735314, Validation Loss: 0.0029543485832909324\n",
      "Epoch [10/25], Train Loss: 0.0004240254929754883, Validation Loss: 0.0029538958744310282\n",
      "Epoch [10/25], Train Loss: 0.0004980615922249854, Validation Loss: 0.0029501289935359933\n",
      "Epoch [10/25], Train Loss: 0.0004996893694624305, Validation Loss: 0.002946814404753577\n",
      "Epoch [10/25], Train Loss: 0.0004522768722381443, Validation Loss: 0.002943636685171548\n",
      "Epoch [10/25], Train Loss: 0.00042131979716941714, Validation Loss: 0.002940320028305179\n",
      "Epoch [10/25], Train Loss: 0.0005261850310489535, Validation Loss: 0.0029392443898626987\n",
      "Epoch [11/25], Train Loss: 0.00046419622958637774, Validation Loss: 0.0029428703240862416\n",
      "Epoch [11/25], Train Loss: 0.0004449486150406301, Validation Loss: 0.0029440186546035424\n",
      "Epoch [11/25], Train Loss: 0.0003823900187853724, Validation Loss: 0.0029445415648373485\n",
      "Epoch [11/25], Train Loss: 0.000478714588098228, Validation Loss: 0.0029412888913971035\n",
      "Epoch [11/25], Train Loss: 0.0004957573255524039, Validation Loss: 0.0029379876150491357\n",
      "Epoch [11/25], Train Loss: 0.0005362980300560594, Validation Loss: 0.0029382692031687547\n",
      "Epoch [11/25], Train Loss: 0.0003790069604292512, Validation Loss: 0.0029361508062751103\n",
      "Epoch [11/25], Train Loss: 0.00045385549310594797, Validation Loss: 0.0029356803748944475\n",
      "Epoch [11/25], Train Loss: 0.0004654664371628314, Validation Loss: 0.002938755335943664\n",
      "Epoch [11/25], Train Loss: 0.00039210409158840775, Validation Loss: 0.0029416356877494257\n",
      "Epoch [11/25], Train Loss: 0.000492304505314678, Validation Loss: 0.0029448758842063553\n",
      "Epoch [11/25], Train Loss: 0.00040494909626431763, Validation Loss: 0.002948477492519036\n",
      "Epoch [11/25], Train Loss: 0.00046384878805838525, Validation Loss: 0.002948170287158935\n",
      "Epoch [11/25], Train Loss: 0.0004434124566614628, Validation Loss: 0.0029478173589474765\n",
      "Epoch [11/25], Train Loss: 0.0004978345823474228, Validation Loss: 0.0029480789040661408\n",
      "Epoch [12/25], Train Loss: 0.0004274826787877828, Validation Loss: 0.0029461833319681533\n",
      "Epoch [12/25], Train Loss: 0.0004430201952345669, Validation Loss: 0.002943857461448853\n",
      "Epoch [12/25], Train Loss: 0.000493074709083885, Validation Loss: 0.002944322334242468\n",
      "Epoch [12/25], Train Loss: 0.0005674974527209997, Validation Loss: 0.002945142279637336\n",
      "Epoch [12/25], Train Loss: 0.00043770342017523944, Validation Loss: 0.0029433407202488233\n",
      "Epoch [12/25], Train Loss: 0.0004877900064457208, Validation Loss: 0.002942214731690513\n",
      "Epoch [12/25], Train Loss: 0.000448293169029057, Validation Loss: 0.00293967522829458\n",
      "Epoch [12/25], Train Loss: 0.0003391631180420518, Validation Loss: 0.002937798067426481\n",
      "Epoch [12/25], Train Loss: 0.0004794798733200878, Validation Loss: 0.0029360741189047067\n",
      "Epoch [12/25], Train Loss: 0.0004948113928548992, Validation Loss: 0.002936160152762377\n",
      "Epoch [12/25], Train Loss: 0.00040810255450196564, Validation Loss: 0.0029421809242897424\n",
      "Epoch [12/25], Train Loss: 0.00030447691096924245, Validation Loss: 0.002948795345478824\n",
      "Epoch [12/25], Train Loss: 0.0005336906760931015, Validation Loss: 0.002953128738547949\n",
      "Epoch [12/25], Train Loss: 0.0004280374269001186, Validation Loss: 0.0029556360134870567\n",
      "Epoch [12/25], Train Loss: 0.0004947890993207693, Validation Loss: 0.0029540451032211298\n",
      "Epoch [13/25], Train Loss: 0.0004306833434384316, Validation Loss: 0.0029514586052098194\n",
      "Epoch [13/25], Train Loss: 0.0004567914584185928, Validation Loss: 0.0029468496834874906\n",
      "Epoch [13/25], Train Loss: 0.00046555284643545747, Validation Loss: 0.0029417978141832502\n",
      "Epoch [13/25], Train Loss: 0.000520179804880172, Validation Loss: 0.0029374773657152883\n",
      "Epoch [13/25], Train Loss: 0.0005659456946887076, Validation Loss: 0.0029314687326863534\n",
      "Epoch [13/25], Train Loss: 0.0003692441969178617, Validation Loss: 0.002929895983210632\n",
      "Epoch [13/25], Train Loss: 0.00030316889751702547, Validation Loss: 0.002932264602610043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Train Loss: 0.0005228117224760354, Validation Loss: 0.00293718116600536\n",
      "Epoch [13/25], Train Loss: 0.0003461040905676782, Validation Loss: 0.00294490614827691\n",
      "Epoch [13/25], Train Loss: 0.000517639855388552, Validation Loss: 0.002952612177351443\n",
      "Epoch [13/25], Train Loss: 0.00037125410744920373, Validation Loss: 0.0029546416466351318\n",
      "Epoch [13/25], Train Loss: 0.0004966951673850417, Validation Loss: 0.0029535226293002106\n",
      "Epoch [13/25], Train Loss: 0.00045197593863122165, Validation Loss: 0.0029489503285119764\n",
      "Epoch [13/25], Train Loss: 0.00041970485472120345, Validation Loss: 0.002940448120377269\n",
      "Epoch [13/25], Train Loss: 0.0005689170211553574, Validation Loss: 0.002934924405099464\n",
      "Epoch [14/25], Train Loss: 0.0004349318041931838, Validation Loss: 0.002933371393401332\n",
      "Epoch [14/25], Train Loss: 0.0004523875250015408, Validation Loss: 0.0029375413589242125\n",
      "Epoch [14/25], Train Loss: 0.00038567144656553864, Validation Loss: 0.0029424681355628896\n",
      "Epoch [14/25], Train Loss: 0.0005312419962137938, Validation Loss: 0.002946664800303949\n",
      "Epoch [14/25], Train Loss: 0.00044077864731661975, Validation Loss: 0.002950447010846824\n",
      "Epoch [14/25], Train Loss: 0.00044929378782399, Validation Loss: 0.002948469642800193\n",
      "Epoch [14/25], Train Loss: 0.00038716482231393456, Validation Loss: 0.0029421916403690306\n",
      "Epoch [14/25], Train Loss: 0.0004222197167109698, Validation Loss: 0.002935268745968948\n",
      "Epoch [14/25], Train Loss: 0.0004954968462698162, Validation Loss: 0.002932623490503355\n",
      "Epoch [14/25], Train Loss: 0.0005533581716008484, Validation Loss: 0.002935867157897779\n",
      "Epoch [14/25], Train Loss: 0.00040989037370309234, Validation Loss: 0.0029408032653712424\n",
      "Epoch [14/25], Train Loss: 0.00046920665772631764, Validation Loss: 0.0029456411437483645\n",
      "Epoch [14/25], Train Loss: 0.00044752436224371195, Validation Loss: 0.002948855216215764\n",
      "Epoch [14/25], Train Loss: 0.0005396266933530569, Validation Loss: 0.0029513283159208147\n",
      "Epoch [14/25], Train Loss: 0.00032500820816494524, Validation Loss: 0.0029514496285123985\n",
      "Epoch [15/25], Train Loss: 0.0004641078121494502, Validation Loss: 0.0029493655301160923\n",
      "Epoch [15/25], Train Loss: 0.0004760310985147953, Validation Loss: 0.002944579514275704\n",
      "Epoch [15/25], Train Loss: 0.00047863792860880494, Validation Loss: 0.0029401057693293365\n",
      "Epoch [15/25], Train Loss: 0.00037562334910035133, Validation Loss: 0.0029370044495033868\n",
      "Epoch [15/25], Train Loss: 0.00040765554876998067, Validation Loss: 0.0029331629845399817\n",
      "Epoch [15/25], Train Loss: 0.0003791104245465249, Validation Loss: 0.0029337181330134137\n",
      "Epoch [15/25], Train Loss: 0.00033013903885148466, Validation Loss: 0.0029365105198367555\n",
      "Epoch [15/25], Train Loss: 0.0004437397001311183, Validation Loss: 0.00294097004841794\n",
      "Epoch [15/25], Train Loss: 0.00047443591756746173, Validation Loss: 0.0029438074282957475\n",
      "Epoch [15/25], Train Loss: 0.0004865801311098039, Validation Loss: 0.0029432474334231194\n",
      "Epoch [15/25], Train Loss: 0.00048271173727698624, Validation Loss: 0.002943424044270851\n",
      "Epoch [15/25], Train Loss: 0.0006137388991191983, Validation Loss: 0.002943657062744268\n",
      "Epoch [15/25], Train Loss: 0.00041307302308268845, Validation Loss: 0.0029439738122041988\n",
      "Epoch [15/25], Train Loss: 0.00046639080392196774, Validation Loss: 0.002943690156000627\n",
      "Epoch [15/25], Train Loss: 0.0004856289888266474, Validation Loss: 0.0029399456366301837\n",
      "Epoch [16/25], Train Loss: 0.0006171190761961043, Validation Loss: 0.002932688524602216\n",
      "Epoch [16/25], Train Loss: 0.00046053205733187497, Validation Loss: 0.00293008320252685\n",
      "Epoch [16/25], Train Loss: 0.0004264464951120317, Validation Loss: 0.0029327862019486286\n",
      "Epoch [16/25], Train Loss: 0.0004342086904216558, Validation Loss: 0.0029394623056492384\n",
      "Epoch [16/25], Train Loss: 0.00037500535836443305, Validation Loss: 0.0029465813667145595\n",
      "Epoch [16/25], Train Loss: 0.00041170077747665346, Validation Loss: 0.0029504399535347932\n",
      "Epoch [16/25], Train Loss: 0.0005070812767371535, Validation Loss: 0.002952496437042826\n",
      "Epoch [16/25], Train Loss: 0.0004354854463599622, Validation Loss: 0.0029516732887591886\n",
      "Epoch [16/25], Train Loss: 0.0005072398926131427, Validation Loss: 0.002946118833966741\n",
      "Epoch [16/25], Train Loss: 0.0004075524630025029, Validation Loss: 0.0029418622437050613\n",
      "Epoch [16/25], Train Loss: 0.0004962305538356304, Validation Loss: 0.002936979871196281\n",
      "Epoch [16/25], Train Loss: 0.0004948730347678065, Validation Loss: 0.0029388970887178883\n",
      "Epoch [16/25], Train Loss: 0.0003103428753092885, Validation Loss: 0.0029407673781471594\n",
      "Epoch [16/25], Train Loss: 0.00035768633824773133, Validation Loss: 0.0029456524135341414\n",
      "Epoch [16/25], Train Loss: 0.0005542032886296511, Validation Loss: 0.0029486827200137767\n",
      "Epoch [17/25], Train Loss: 0.0004163158591836691, Validation Loss: 0.0029487199200896406\n",
      "Epoch [17/25], Train Loss: 0.0003720431122928858, Validation Loss: 0.002941888802050918\n",
      "Epoch [17/25], Train Loss: 0.00045149977086111903, Validation Loss: 0.0029369824499424014\n",
      "Epoch [17/25], Train Loss: 0.00048736220924183726, Validation Loss: 0.002935180180322598\n",
      "Epoch [17/25], Train Loss: 0.000529152515809983, Validation Loss: 0.002936036024680909\n",
      "Epoch [17/25], Train Loss: 0.000382330414140597, Validation Loss: 0.002936553425241669\n",
      "Epoch [17/25], Train Loss: 0.0004843819187954068, Validation Loss: 0.0029377808751334914\n",
      "Epoch [17/25], Train Loss: 0.0004301100561860949, Validation Loss: 0.002941456766204298\n",
      "Epoch [17/25], Train Loss: 0.0004227934405207634, Validation Loss: 0.0029427243020980296\n",
      "Epoch [17/25], Train Loss: 0.0005427164724096656, Validation Loss: 0.0029432545689975515\n",
      "Epoch [17/25], Train Loss: 0.0004792950348928571, Validation Loss: 0.0029413951365198907\n",
      "Epoch [17/25], Train Loss: 0.0004220944829285145, Validation Loss: 0.0029376670718193054\n",
      "Epoch [17/25], Train Loss: 0.0004849732795264572, Validation Loss: 0.002936255147664988\n",
      "Epoch [17/25], Train Loss: 0.0004144097038079053, Validation Loss: 0.0029349228476776798\n",
      "Epoch [17/25], Train Loss: 0.00043856693082489073, Validation Loss: 0.0029350886485312415\n",
      "Epoch [18/25], Train Loss: 0.0005313794827088714, Validation Loss: 0.0029382958671688533\n",
      "Epoch [18/25], Train Loss: 0.00047504357644356787, Validation Loss: 0.0029415548231233567\n",
      "Epoch [18/25], Train Loss: 0.00034910545218735933, Validation Loss: 0.0029441528550560483\n",
      "Epoch [18/25], Train Loss: 0.00039435658254660666, Validation Loss: 0.0029464997781612793\n",
      "Epoch [18/25], Train Loss: 0.0004390902176965028, Validation Loss: 0.0029450481377948983\n",
      "Epoch [18/25], Train Loss: 0.0005344866658560932, Validation Loss: 0.0029407536196170234\n",
      "Epoch [18/25], Train Loss: 0.0004471798019949347, Validation Loss: 0.0029383854737051393\n",
      "Epoch [18/25], Train Loss: 0.00044991346658207476, Validation Loss: 0.002938421936157872\n",
      "Epoch [18/25], Train Loss: 0.000490823935251683, Validation Loss: 0.002939693144514781\n",
      "Epoch [18/25], Train Loss: 0.00042637920705601573, Validation Loss: 0.002943564472453935\n",
      "Epoch [18/25], Train Loss: 0.0004972288734279573, Validation Loss: 0.0029476061072044014\n",
      "Epoch [18/25], Train Loss: 0.0004485483223106712, Validation Loss: 0.002947931911624154\n",
      "Epoch [18/25], Train Loss: 0.00042659431346692145, Validation Loss: 0.0029467950856798338\n",
      "Epoch [18/25], Train Loss: 0.0003646931436378509, Validation Loss: 0.0029422352990495556\n",
      "Epoch [18/25], Train Loss: 0.0004931900766678154, Validation Loss: 0.002939257344246662\n",
      "Epoch [19/25], Train Loss: 0.0005025569116696715, Validation Loss: 0.002939908712429284\n",
      "Epoch [19/25], Train Loss: 0.00042460463009774685, Validation Loss: 0.0029389171454147632\n",
      "Epoch [19/25], Train Loss: 0.0004988345899619162, Validation Loss: 0.0029355691092386215\n",
      "Epoch [19/25], Train Loss: 0.0004330980300437659, Validation Loss: 0.0029333709668712456\n",
      "Epoch [19/25], Train Loss: 0.0003965588111896068, Validation Loss: 0.0029335556876603046\n",
      "Epoch [19/25], Train Loss: 0.0004710559151135385, Validation Loss: 0.002936294552784006\n",
      "Epoch [19/25], Train Loss: 0.0004361922910902649, Validation Loss: 0.0029385575296811198\n",
      "Epoch [19/25], Train Loss: 0.00037023628829047084, Validation Loss: 0.0029391462116369177\n",
      "Epoch [19/25], Train Loss: 0.00048740216880105436, Validation Loss: 0.002937818562392803\n",
      "Epoch [19/25], Train Loss: 0.00047974567860364914, Validation Loss: 0.002935069169019576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], Train Loss: 0.0003805181768257171, Validation Loss: 0.0029340671089736104\n",
      "Epoch [19/25], Train Loss: 0.0005145164323039353, Validation Loss: 0.0029356689661929085\n",
      "Epoch [19/25], Train Loss: 0.0004818130692001432, Validation Loss: 0.0029386083200229566\n",
      "Epoch [19/25], Train Loss: 0.000570605625398457, Validation Loss: 0.00294305695839101\n",
      "Epoch [19/25], Train Loss: 0.0002606702328193933, Validation Loss: 0.0029449182261219796\n",
      "Epoch [20/25], Train Loss: 0.00043642264790832996, Validation Loss: 0.0029471622361261552\n",
      "Epoch [20/25], Train Loss: 0.00032042100792750716, Validation Loss: 0.002946258178215568\n",
      "Epoch [20/25], Train Loss: 0.00046130959526635706, Validation Loss: 0.0029436999955410217\n",
      "Epoch [20/25], Train Loss: 0.0005225361092016101, Validation Loss: 0.0029410192148149514\n",
      "Epoch [20/25], Train Loss: 0.000515047344379127, Validation Loss: 0.0029368791533555803\n",
      "Epoch [20/25], Train Loss: 0.0004757551068905741, Validation Loss: 0.0029378614697542762\n",
      "Epoch [20/25], Train Loss: 0.0003926970821339637, Validation Loss: 0.0029402254540629746\n",
      "Epoch [20/25], Train Loss: 0.0005677109002135694, Validation Loss: 0.0029421441037865248\n",
      "Epoch [20/25], Train Loss: 0.00045029568718746305, Validation Loss: 0.002941698528544492\n",
      "Epoch [20/25], Train Loss: 0.0004095589101780206, Validation Loss: 0.002939280658615988\n",
      "Epoch [20/25], Train Loss: 0.00037630368024110794, Validation Loss: 0.0029353393777860563\n",
      "Epoch [20/25], Train Loss: 0.00038689939538016915, Validation Loss: 0.002932898694367594\n",
      "Epoch [20/25], Train Loss: 0.0005176347331143916, Validation Loss: 0.00293191902910774\n",
      "Epoch [20/25], Train Loss: 0.0004478466580621898, Validation Loss: 0.002932182012298027\n",
      "Epoch [20/25], Train Loss: 0.00047240033745765686, Validation Loss: 0.0029331860367302633\n",
      "Epoch [21/25], Train Loss: 0.0004831468977499753, Validation Loss: 0.0029355832766898037\n",
      "Epoch [21/25], Train Loss: 0.00040252754115499556, Validation Loss: 0.0029372116218187977\n",
      "Epoch [21/25], Train Loss: 0.0003863598685711622, Validation Loss: 0.002937991381427195\n",
      "Epoch [21/25], Train Loss: 0.0005233152187429368, Validation Loss: 0.0029393135933909846\n",
      "Epoch [21/25], Train Loss: 0.0004112659371457994, Validation Loss: 0.002938367600529259\n",
      "Epoch [21/25], Train Loss: 0.0005122491274960339, Validation Loss: 0.0029379309354616062\n",
      "Epoch [21/25], Train Loss: 0.0005646944046020508, Validation Loss: 0.00294035643597423\n",
      "Epoch [21/25], Train Loss: 0.00030280734063126147, Validation Loss: 0.0029425356623192294\n",
      "Epoch [21/25], Train Loss: 0.0005954523803666234, Validation Loss: 0.002944230115698541\n",
      "Epoch [21/25], Train Loss: 0.0004645122098736465, Validation Loss: 0.0029435091428928264\n",
      "Epoch [21/25], Train Loss: 0.0003616538888309151, Validation Loss: 0.0029415303778623334\n",
      "Epoch [21/25], Train Loss: 0.00039828428998589516, Validation Loss: 0.002938688507679255\n",
      "Epoch [21/25], Train Loss: 0.0004192771448288113, Validation Loss: 0.0029350015424786744\n",
      "Epoch [21/25], Train Loss: 0.0004363800981082022, Validation Loss: 0.0029334714968821834\n",
      "Epoch [21/25], Train Loss: 0.0004830950638279319, Validation Loss: 0.002936133970076046\n",
      "Epoch [22/25], Train Loss: 0.000425643811468035, Validation Loss: 0.0029386964219645794\n",
      "Epoch [22/25], Train Loss: 0.00046349471085704863, Validation Loss: 0.002940294937379345\n",
      "Epoch [22/25], Train Loss: 0.00033981972956098616, Validation Loss: 0.002938239130841083\n",
      "Epoch [22/25], Train Loss: 0.00039368588477373123, Validation Loss: 0.0029364925820943937\n",
      "Epoch [22/25], Train Loss: 0.0003971074183937162, Validation Loss: 0.002936148558187635\n",
      "Epoch [22/25], Train Loss: 0.00044681047438643873, Validation Loss: 0.0029355364875132297\n",
      "Epoch [22/25], Train Loss: 0.0004556931962724775, Validation Loss: 0.0029352690961931935\n",
      "Epoch [22/25], Train Loss: 0.00053758779540658, Validation Loss: 0.0029353035120841336\n",
      "Epoch [22/25], Train Loss: 0.0005732611753046513, Validation Loss: 0.002937826408198526\n",
      "Epoch [22/25], Train Loss: 0.00038775429129600525, Validation Loss: 0.0029385584805692944\n",
      "Epoch [22/25], Train Loss: 0.00047406632802449167, Validation Loss: 0.002937376972663553\n",
      "Epoch [22/25], Train Loss: 0.0004712564405053854, Validation Loss: 0.0029337646326191035\n",
      "Epoch [22/25], Train Loss: 0.0003946127253584564, Validation Loss: 0.002930532242836697\n",
      "Epoch [22/25], Train Loss: 0.0003871413937304169, Validation Loss: 0.0029305539997842383\n",
      "Epoch [22/25], Train Loss: 0.0006123207858763635, Validation Loss: 0.0029359567703037453\n",
      "Epoch [23/25], Train Loss: 0.0005369827267713845, Validation Loss: 0.0029413220081321834\n",
      "Epoch [23/25], Train Loss: 0.0004416202427819371, Validation Loss: 0.002943056005546275\n",
      "Epoch [23/25], Train Loss: 0.0005168511997908354, Validation Loss: 0.002940080183393815\n",
      "Epoch [23/25], Train Loss: 0.00036045859451405704, Validation Loss: 0.002934426977103498\n",
      "Epoch [23/25], Train Loss: 0.00047077599447220564, Validation Loss: 0.002931868559641748\n",
      "Epoch [23/25], Train Loss: 0.0004187215818092227, Validation Loss: 0.002933862282573676\n",
      "Epoch [23/25], Train Loss: 0.0003906281490344554, Validation Loss: 0.0029394469016521168\n",
      "Epoch [23/25], Train Loss: 0.00042723293881863356, Validation Loss: 0.0029446802927697658\n",
      "Epoch [23/25], Train Loss: 0.0003664231044240296, Validation Loss: 0.0029444121560003576\n",
      "Epoch [23/25], Train Loss: 0.0004527742275968194, Validation Loss: 0.0029371321972209113\n",
      "Epoch [23/25], Train Loss: 0.00039265333907678723, Validation Loss: 0.0029297590827052833\n",
      "Epoch [23/25], Train Loss: 0.00041426075040362775, Validation Loss: 0.0029282168907841214\n",
      "Epoch [23/25], Train Loss: 0.0005751229473389685, Validation Loss: 0.0029318055701481193\n",
      "Epoch [23/25], Train Loss: 0.000482666480820626, Validation Loss: 0.0029362256094782042\n",
      "Epoch [23/25], Train Loss: 0.0004539450746960938, Validation Loss: 0.002934664276530262\n",
      "Epoch [24/25], Train Loss: 0.0002647080400492996, Validation Loss: 0.0029290763215169685\n",
      "Epoch [24/25], Train Loss: 0.00033877737587317824, Validation Loss: 0.002924012256974057\n",
      "Epoch [24/25], Train Loss: 0.00045394120388664305, Validation Loss: 0.002925663219937006\n",
      "Epoch [24/25], Train Loss: 0.000386585365049541, Validation Loss: 0.0029355930047062764\n",
      "Epoch [24/25], Train Loss: 0.0004173247143626213, Validation Loss: 0.0029450374999780114\n",
      "Epoch [24/25], Train Loss: 0.0004586166178341955, Validation Loss: 0.0029439153814954416\n",
      "Epoch [24/25], Train Loss: 0.00040968478424474597, Validation Loss: 0.00293958156776591\n",
      "Epoch [24/25], Train Loss: 0.0004907312104478478, Validation Loss: 0.0029345699605539816\n",
      "Epoch [24/25], Train Loss: 0.0005241523031145334, Validation Loss: 0.002933888091557041\n",
      "Epoch [24/25], Train Loss: 0.0004554756742436439, Validation Loss: 0.0029334507827811383\n",
      "Epoch [24/25], Train Loss: 0.00047842529602348804, Validation Loss: 0.0029367278330028057\n",
      "Epoch [24/25], Train Loss: 0.0006123612984083593, Validation Loss: 0.0029383132238128854\n",
      "Epoch [24/25], Train Loss: 0.00043639130308292806, Validation Loss: 0.002929233679813998\n",
      "Epoch [24/25], Train Loss: 0.0004814869025722146, Validation Loss: 0.0029204688387132493\n",
      "Epoch [24/25], Train Loss: 0.0004653845971915871, Validation Loss: 0.0029190108356136482\n",
      "Epoch [25/25], Train Loss: 0.0004682015278376639, Validation Loss: 0.0029298603522959128\n",
      "Epoch [25/25], Train Loss: 0.000498439883813262, Validation Loss: 0.002945508017447315\n",
      "Epoch [25/25], Train Loss: 0.0005396638298407197, Validation Loss: 0.0029541226749566674\n",
      "Epoch [25/25], Train Loss: 0.0003927625948563218, Validation Loss: 0.0029448351408002758\n",
      "Epoch [25/25], Train Loss: 0.00025064294459298253, Validation Loss: 0.002933703112502058\n",
      "Epoch [25/25], Train Loss: 0.00047284469474107027, Validation Loss: 0.002927346933758309\n",
      "Epoch [25/25], Train Loss: 0.0005356255569495261, Validation Loss: 0.0029318683463767045\n",
      "Epoch [25/25], Train Loss: 0.0004089491267222911, Validation Loss: 0.0029418569531667383\n",
      "Epoch [25/25], Train Loss: 0.0004962942912243307, Validation Loss: 0.0029464758866067442\n",
      "Epoch [25/25], Train Loss: 0.0004023780347779393, Validation Loss: 0.0029332829490617044\n",
      "Epoch [25/25], Train Loss: 0.0004980444209650159, Validation Loss: 0.002927433556540548\n",
      "Epoch [25/25], Train Loss: 0.0003961441107094288, Validation Loss: 0.002916369503050917\n",
      "Epoch [25/25], Train Loss: 0.0003549739776644856, Validation Loss: 0.002916940812710203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], Train Loss: 0.00041258276905864477, Validation Loss: 0.002929305624482887\n",
      "Epoch [25/25], Train Loss: 0.0005142524605616927, Validation Loss: 0.0029370651850398597\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Hyperparameters\n",
    "input_channels = 16  # NUM_CHANNELS\n",
    "num_filters = 32\n",
    "batch_size = 64  # Batch size increased\n",
    "learning_rate = 0.01 #learning rate increased\n",
    "num_epochs = 25\n",
    "\n",
    "# Initialize the model\n",
    "model_preictal = DDPM(input_channels, num_filters)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for reconstruction\n",
    "optimizer = optim.Adam(model_preictal.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses_preictal = []\n",
    "val_losses_preictal = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model_preictal.train()  # Set model to training mode\n",
    "    for i, (batch_X, _) in enumerate(dataloader):\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed = model_preictal(batch_X)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(reconstructed, batch_X)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append losses to the lists after each epoch\n",
    "        train_losses_preictal.append(loss.item())\n",
    "        \n",
    "        \n",
    "        # Print progress\n",
    "       # print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], Loss: {loss.item()}\")\n",
    "        \n",
    "        # Validation\n",
    "        model_preictal.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X_val, _ in val_dataloader:\n",
    "                reconstructed_val = model_preictal(batch_X_val)\n",
    "                val_loss += criterion(reconstructed_val, batch_X_val).item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_losses_preictal.append(val_loss)\n",
    "\n",
    "        # Print training and validation loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()}, Validation Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train_losses_preictal and val_losses_preictal contain the loss values across epochs\n",
    "# Plotting training and validation loss\n",
    "epochs = range(1, len(train_losses_preictal) + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_losses_preictal, label='Training Loss')\n",
    "plt.plot(epochs, val_losses_preictal, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss for Preictal data generation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_channels = 16  # NUM_CHANNELS\n",
    "num_filters = 32\n",
    "batch_size = 32  # Batch size changed from 64 to 32 as it's taking a long time to compile\n",
    "learning_rate = 0.01 #learning rate increased\n",
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_pre_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_pre_train, dtype=torch.float32)\n",
    "\n",
    "# Convert validation data to PyTorch tensors\n",
    "X_val_tensor = torch.tensor(X_pre_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_pre_val, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoader for training set\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create DataLoader for validation set\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Train Loss: 0.0003634310560300946, Validation Loss: 0.00033239719341509043\n",
      "Epoch [1/25], Train Loss: 0.00035255818511359394, Validation Loss: 0.00022631968895439058\n",
      "Epoch [1/25], Train Loss: 0.0001784216583473608, Validation Loss: 0.0002294671906080718\n",
      "Epoch [1/25], Train Loss: 0.00023663591127842665, Validation Loss: 0.00022665416375578691\n",
      "Epoch [1/25], Train Loss: 0.00018465671746525913, Validation Loss: 0.00021316689332403864\n",
      "Epoch [1/25], Train Loss: 0.00019479417824186385, Validation Loss: 0.00021958829796252152\n",
      "Epoch [1/25], Train Loss: 0.00019333513046149164, Validation Loss: 0.00020668123615905643\n",
      "Epoch [1/25], Train Loss: 0.00023731899273116142, Validation Loss: 0.0002087169739146096\n",
      "Epoch [1/25], Train Loss: 0.0002358132041990757, Validation Loss: 0.00020799105792927244\n",
      "Epoch [1/25], Train Loss: 0.00016822638281155378, Validation Loss: 0.00020677670399891213\n",
      "Epoch [1/25], Train Loss: 0.000167599311680533, Validation Loss: 0.00020703596237581223\n",
      "Epoch [1/25], Train Loss: 0.00022317146067507565, Validation Loss: 0.00020609032896269733\n",
      "Epoch [1/25], Train Loss: 0.0002604732580948621, Validation Loss: 0.00020577817970964438\n",
      "Epoch [1/25], Train Loss: 0.00021847779862582684, Validation Loss: 0.0002060451573925093\n",
      "Epoch [1/25], Train Loss: 0.00016166553541552275, Validation Loss: 0.0002055491044302471\n",
      "Epoch [1/25], Train Loss: 0.00018055917462334037, Validation Loss: 0.00020484552320946628\n",
      "Epoch [1/25], Train Loss: 0.00020265669445507228, Validation Loss: 0.00020507123263087125\n",
      "Epoch [1/25], Train Loss: 0.000310232222545892, Validation Loss: 0.00020518321204387273\n",
      "Epoch [1/25], Train Loss: 0.00019901343330275267, Validation Loss: 0.0002048181161323252\n",
      "Epoch [1/25], Train Loss: 0.00021019515406806022, Validation Loss: 0.00020454311597859487\n",
      "Epoch [1/25], Train Loss: 0.0002258343156427145, Validation Loss: 0.00020461782745163267\n",
      "Epoch [1/25], Train Loss: 0.00022729176271241158, Validation Loss: 0.00020475636653524514\n",
      "Epoch [1/25], Train Loss: 0.00027488174964673817, Validation Loss: 0.00020475066024422023\n",
      "Epoch [1/25], Train Loss: 0.00022546309628523886, Validation Loss: 0.00020461019157664851\n",
      "Epoch [1/25], Train Loss: 0.0002668720262590796, Validation Loss: 0.00020447466667974367\n",
      "Epoch [1/25], Train Loss: 0.00021701797959394753, Validation Loss: 0.0002044797244404132\n",
      "Epoch [1/25], Train Loss: 0.00021326853311620653, Validation Loss: 0.00020458436774788423\n",
      "Epoch [1/25], Train Loss: 0.0002027585869655013, Validation Loss: 0.0002046127905487083\n",
      "Epoch [1/25], Train Loss: 0.00024262435908894986, Validation Loss: 0.0002044890827770966\n",
      "Epoch [1/25], Train Loss: 0.00014677945000585169, Validation Loss: 0.00020435842307051644\n",
      "Epoch [1/25], Train Loss: 0.00024871647474355996, Validation Loss: 0.00020434863933284456\n",
      "Epoch [1/25], Train Loss: 0.00021366770670283586, Validation Loss: 0.0002044012740952894\n",
      "Epoch [1/25], Train Loss: 0.00011348452972015366, Validation Loss: 0.0002043893148462909\n",
      "Epoch [1/25], Train Loss: 0.00017501195543445647, Validation Loss: 0.00020433095972596977\n",
      "Epoch [1/25], Train Loss: 0.00023200792202260345, Validation Loss: 0.0002042781561613083\n",
      "Epoch [1/25], Train Loss: 0.00021073344396427274, Validation Loss: 0.000204278742603492\n",
      "Epoch [1/25], Train Loss: 0.00024342302640434355, Validation Loss: 0.0002043277306559806\n",
      "Epoch [1/25], Train Loss: 0.00022178504150360823, Validation Loss: 0.00020433542910420027\n",
      "Epoch [1/25], Train Loss: 0.0001655452506383881, Validation Loss: 0.00020426427266405275\n",
      "Epoch [1/25], Train Loss: 0.00011180018918821588, Validation Loss: 0.0002042168232340676\n",
      "Epoch [1/25], Train Loss: 0.0001535093761049211, Validation Loss: 0.0002042574638229174\n",
      "Epoch [1/25], Train Loss: 0.00016004023200366646, Validation Loss: 0.00020427447404169167\n",
      "Epoch [1/25], Train Loss: 0.00019822269678115845, Validation Loss: 0.00020422445668373258\n",
      "Epoch [1/25], Train Loss: 0.0002563376328907907, Validation Loss: 0.00020417046034708618\n",
      "Epoch [1/25], Train Loss: 0.0003219486679881811, Validation Loss: 0.00020414744600808868\n",
      "Epoch [1/25], Train Loss: 0.0002201484894612804, Validation Loss: 0.00020416204739982883\n",
      "Epoch [1/25], Train Loss: 0.00025092237046919763, Validation Loss: 0.00020417436899151654\n",
      "Epoch [1/25], Train Loss: 0.00013126578414812684, Validation Loss: 0.00020416063052834942\n",
      "Epoch [1/25], Train Loss: 0.00023686127678956836, Validation Loss: 0.00020413084954877073\n",
      "Epoch [1/25], Train Loss: 0.00020632713858503848, Validation Loss: 0.00020411250977000843\n",
      "Epoch [1/25], Train Loss: 0.00019301965949125588, Validation Loss: 0.00020409422140801325\n",
      "Epoch [1/25], Train Loss: 0.00019699019321706146, Validation Loss: 0.00020417230407474563\n",
      "Epoch [1/25], Train Loss: 0.00020160154963377863, Validation Loss: 0.00020416468663218742\n",
      "Epoch [1/25], Train Loss: 0.00024572599795646966, Validation Loss: 0.00020413911649181198\n",
      "Epoch [1/25], Train Loss: 0.0002227816585218534, Validation Loss: 0.00020407992609155674\n",
      "Epoch [1/25], Train Loss: 0.000224255069042556, Validation Loss: 0.00020413270249264315\n",
      "Epoch [1/25], Train Loss: 0.00017676323477644473, Validation Loss: 0.00020415845792740584\n",
      "Epoch [1/25], Train Loss: 0.0002415060152998194, Validation Loss: 0.00020404093229444697\n",
      "Epoch [1/25], Train Loss: 0.00023379908816423267, Validation Loss: 0.00020405285225327436\n",
      "Epoch [1/25], Train Loss: 0.0002242252667201683, Validation Loss: 0.00020409740682225674\n",
      "Epoch [1/25], Train Loss: 0.0002009663003263995, Validation Loss: 0.00020403939076156044\n",
      "Epoch [1/25], Train Loss: 0.00024315589689649642, Validation Loss: 0.00020396856950052704\n",
      "Epoch [1/25], Train Loss: 0.0002444592537358403, Validation Loss: 0.00020396789186634124\n",
      "Epoch [1/25], Train Loss: 0.0002075987431453541, Validation Loss: 0.00020391023378275955\n",
      "Epoch [1/25], Train Loss: 0.00018038207781501114, Validation Loss: 0.00020384405991838623\n",
      "Epoch [1/25], Train Loss: 0.00020788307301700115, Validation Loss: 0.00020381076319608837\n",
      "Epoch [1/25], Train Loss: 0.00024453637888655066, Validation Loss: 0.00020373944522968183\n",
      "Epoch [1/25], Train Loss: 0.00030213347054086626, Validation Loss: 0.00020374984160298482\n",
      "Epoch [1/25], Train Loss: 0.0002817708009388298, Validation Loss: 0.00020370232038355123\n",
      "Epoch [1/25], Train Loss: 0.0002417046343907714, Validation Loss: 0.00020361311762826518\n",
      "Epoch [1/25], Train Loss: 0.00019755057292059064, Validation Loss: 0.00020357169026586537\n",
      "Epoch [1/25], Train Loss: 0.0002277786552440375, Validation Loss: 0.00020354427203225593\n",
      "Epoch [1/25], Train Loss: 0.00027397225494496524, Validation Loss: 0.00020345484275215617\n",
      "Epoch [1/25], Train Loss: 0.00019662200065795332, Validation Loss: 0.00020339545493091766\n",
      "Epoch [1/25], Train Loss: 0.0001931528386194259, Validation Loss: 0.0002033713996449175\n",
      "Epoch [1/25], Train Loss: 0.00023373772273771465, Validation Loss: 0.00020327814030072963\n",
      "Epoch [1/25], Train Loss: 0.00017537249368615448, Validation Loss: 0.00020313203713158147\n",
      "Epoch [1/25], Train Loss: 0.00018097992870025337, Validation Loss: 0.00020295408030506222\n",
      "Epoch [1/25], Train Loss: 0.0002082573773805052, Validation Loss: 0.0002029496778656418\n",
      "Epoch [1/25], Train Loss: 0.0001360482128802687, Validation Loss: 0.00020263115923929337\n",
      "Epoch [1/25], Train Loss: 0.00018515811825636774, Validation Loss: 0.00020242016325937583\n",
      "Epoch [1/25], Train Loss: 0.0001994193298742175, Validation Loss: 0.00020222034848605593\n",
      "Epoch [1/25], Train Loss: 0.0002646076900418848, Validation Loss: 0.00020202181622153148\n",
      "Epoch [1/25], Train Loss: 0.0002241126203443855, Validation Loss: 0.00020203370853171993\n",
      "Epoch [1/25], Train Loss: 0.00017205896438099444, Validation Loss: 0.00020433429551000397\n",
      "Epoch [1/25], Train Loss: 0.0002646739885676652, Validation Loss: 0.0002107904549726906\n",
      "Epoch [1/25], Train Loss: 0.0002767109253909439, Validation Loss: 0.00021075644278122733\n",
      "Epoch [1/25], Train Loss: 0.0001981709647225216, Validation Loss: 0.0002069045658572577\n",
      "Epoch [1/25], Train Loss: 0.00016477267490699887, Validation Loss: 0.00020536940670960271\n",
      "Epoch [1/25], Train Loss: 0.00018366205040365458, Validation Loss: 0.0002048103201862735\n",
      "Epoch [1/25], Train Loss: 0.00021435126836877316, Validation Loss: 0.0002052001499881347\n",
      "Epoch [1/25], Train Loss: 0.00013854439021088183, Validation Loss: 0.00020579764241119848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Train Loss: 0.00018491891387384385, Validation Loss: 0.0002058043750973108\n",
      "Epoch [1/25], Train Loss: 0.00024574811686761677, Validation Loss: 0.0002064545638859272\n",
      "Epoch [1/25], Train Loss: 0.00014883615949656814, Validation Loss: 0.00020544502864746998\n",
      "Epoch [1/25], Train Loss: 0.00020448990107979625, Validation Loss: 0.00020489274902502075\n",
      "Epoch [1/25], Train Loss: 0.00024596680304966867, Validation Loss: 0.00020451515932412196\n",
      "Epoch [1/25], Train Loss: 0.0001973531034309417, Validation Loss: 0.00020425622448480378\n",
      "Epoch [1/25], Train Loss: 0.00021249096607789397, Validation Loss: 0.00020478175041110564\n",
      "Epoch [1/25], Train Loss: 0.00020087895973119885, Validation Loss: 0.00020454388820022966\n",
      "Epoch [1/25], Train Loss: 0.00014270249812398106, Validation Loss: 0.00020471050035363684\n",
      "Epoch [1/25], Train Loss: 0.00030888072797097266, Validation Loss: 0.00020481222551704074\n",
      "Epoch [1/25], Train Loss: 0.00020357640460133553, Validation Loss: 0.00020431058170894783\n",
      "Epoch [1/25], Train Loss: 0.0002099461853504181, Validation Loss: 0.00020423716632649301\n",
      "Epoch [1/25], Train Loss: 0.00023468009021598846, Validation Loss: 0.00020404503690466905\n",
      "Epoch [1/25], Train Loss: 0.00028414191910997033, Validation Loss: 0.00020410598663147538\n",
      "Epoch [1/25], Train Loss: 0.00021668271801900119, Validation Loss: 0.00020392684818943964\n",
      "Epoch [1/25], Train Loss: 0.00018621455819811672, Validation Loss: 0.00020397740736370905\n",
      "Epoch [1/25], Train Loss: 0.00016431929543614388, Validation Loss: 0.0002041143161477521\n",
      "Epoch [1/25], Train Loss: 0.00018434168305248022, Validation Loss: 0.00020380814336628343\n",
      "Epoch [1/25], Train Loss: 0.0002482831187080592, Validation Loss: 0.00020377258624648675\n",
      "Epoch [1/25], Train Loss: 0.00020822409715037793, Validation Loss: 0.00020366123353596777\n",
      "Epoch [1/25], Train Loss: 0.00020794551528524607, Validation Loss: 0.0002035243339681377\n",
      "Epoch [1/25], Train Loss: 0.00023423814855050296, Validation Loss: 0.000203406063762183\n",
      "Epoch [1/25], Train Loss: 0.00020617229165509343, Validation Loss: 0.0002034329401794821\n",
      "Epoch [1/25], Train Loss: 0.0003036938142031431, Validation Loss: 0.00020336935364563639\n",
      "Epoch [1/25], Train Loss: 0.000205493182875216, Validation Loss: 0.00020312507937584692\n",
      "Epoch [1/25], Train Loss: 0.0002505604934412986, Validation Loss: 0.00020303971202035125\n",
      "Epoch [1/25], Train Loss: 0.00027639116160571575, Validation Loss: 0.00020286963069035362\n",
      "Epoch [1/25], Train Loss: 0.00018657904001884162, Validation Loss: 0.00020263953532169884\n",
      "Epoch [1/25], Train Loss: 0.00021742888202425092, Validation Loss: 0.00020237320228867853\n",
      "Epoch [1/25], Train Loss: 0.00027204627986066043, Validation Loss: 0.00020216534418674806\n",
      "Epoch [1/25], Train Loss: 0.00020890761516056955, Validation Loss: 0.00020185936543081577\n",
      "Epoch [1/25], Train Loss: 0.00017214078980032355, Validation Loss: 0.0002014358479452009\n",
      "Epoch [1/25], Train Loss: 0.00025280468980781734, Validation Loss: 0.00020099258496581268\n",
      "Epoch [1/25], Train Loss: 0.00023817831242922693, Validation Loss: 0.0002010408745263703\n",
      "Epoch [1/25], Train Loss: 0.00022485872614197433, Validation Loss: 0.0002039355657567891\n",
      "Epoch [1/25], Train Loss: 0.00017593470693100244, Validation Loss: 0.00020886701580214624\n",
      "Epoch [1/25], Train Loss: 0.00021531753009185195, Validation Loss: 0.00020185949203247824\n",
      "Epoch [1/25], Train Loss: 0.00032038657809607685, Validation Loss: 0.00020241080201230944\n",
      "Epoch [1/25], Train Loss: 0.00021707398991566151, Validation Loss: 0.00020159586856607347\n",
      "Epoch [1/25], Train Loss: 0.00020101699919905514, Validation Loss: 0.00019997890000619616\n",
      "Epoch [1/25], Train Loss: 0.00021801883121952415, Validation Loss: 0.00020071402104804292\n",
      "Epoch [1/25], Train Loss: 0.00020474240591283888, Validation Loss: 0.0001981934032907399\n",
      "Epoch [1/25], Train Loss: 0.00017099471006076783, Validation Loss: 0.0002001026460978513\n",
      "Epoch [1/25], Train Loss: 0.0002254753198940307, Validation Loss: 0.00019688146470192198\n",
      "Epoch [1/25], Train Loss: 0.00019913834694307297, Validation Loss: 0.00019880159137149653\n",
      "Epoch [1/25], Train Loss: 0.00013628507440444082, Validation Loss: 0.00019565838932370145\n",
      "Epoch [1/25], Train Loss: 0.0002658360172063112, Validation Loss: 0.00019718940942160164\n",
      "Epoch [1/25], Train Loss: 0.00022821985476184636, Validation Loss: 0.0001953785977093503\n",
      "Epoch [1/25], Train Loss: 0.00028993922751396894, Validation Loss: 0.00019289982543947796\n",
      "Epoch [1/25], Train Loss: 0.00020255829440429807, Validation Loss: 0.00019334976192718993\n",
      "Epoch [1/25], Train Loss: 0.00027703368687070906, Validation Loss: 0.00019599968703308454\n",
      "Epoch [1/25], Train Loss: 0.0001328421349171549, Validation Loss: 0.00020124230747266362\n",
      "Epoch [1/25], Train Loss: 0.0002057825040537864, Validation Loss: 0.00021305625317230199\n",
      "Epoch [1/25], Train Loss: 0.00017658359138295054, Validation Loss: 0.00019264340177566435\n",
      "Epoch [1/25], Train Loss: 0.00021638034377247095, Validation Loss: 0.00019283515769833077\n",
      "Epoch [1/25], Train Loss: 0.00019765333854593337, Validation Loss: 0.00020227766605482127\n",
      "Epoch [1/25], Train Loss: 0.00025017638108693063, Validation Loss: 0.0001899582576394702\n",
      "Epoch [1/25], Train Loss: 0.00025889163953252137, Validation Loss: 0.0001914742528848971\n",
      "Epoch [1/25], Train Loss: 0.0001629133039386943, Validation Loss: 0.00020024804107379168\n",
      "Epoch [1/25], Train Loss: 0.00019255101506132632, Validation Loss: 0.00018639532766731766\n",
      "Epoch [1/25], Train Loss: 0.00021265505347400904, Validation Loss: 0.00019317661693397288\n",
      "Epoch [1/25], Train Loss: 0.000226818592636846, Validation Loss: 0.0001915054293931462\n",
      "Epoch [1/25], Train Loss: 0.0002526142925489694, Validation Loss: 0.0001843753637028082\n",
      "Epoch [1/25], Train Loss: 0.00015474601241294295, Validation Loss: 0.00020417731866473332\n",
      "Epoch [1/25], Train Loss: 0.00022299644479062408, Validation Loss: 0.00020977404346922413\n",
      "Epoch [1/25], Train Loss: 0.00020049180602654815, Validation Loss: 0.00018214060740623\n",
      "Epoch [1/25], Train Loss: 0.0001884002413135022, Validation Loss: 0.00020849284531626228\n",
      "Epoch [1/25], Train Loss: 0.00020824458624701947, Validation Loss: 0.00018308951136229248\n",
      "Epoch [1/25], Train Loss: 0.00020261600730009377, Validation Loss: 0.00020131450680006916\n",
      "Epoch [1/25], Train Loss: 0.00013841896725352854, Validation Loss: 0.00018159360576343412\n",
      "Epoch [1/25], Train Loss: 0.00021028572518844157, Validation Loss: 0.0001988851732069937\n",
      "Epoch [1/25], Train Loss: 0.00024418046814389527, Validation Loss: 0.0001827557796787005\n",
      "Epoch [1/25], Train Loss: 0.0001741088490234688, Validation Loss: 0.00019404417447124918\n",
      "Epoch [1/25], Train Loss: 0.00022737887047696859, Validation Loss: 0.00018044013777398504\n",
      "Epoch [1/25], Train Loss: 0.0002298636536579579, Validation Loss: 0.0001930924428355259\n",
      "Epoch [1/25], Train Loss: 0.0001657383400015533, Validation Loss: 0.00017924905065835143\n",
      "Epoch [1/25], Train Loss: 0.00014935241779312491, Validation Loss: 0.0001932521826044346\n",
      "Epoch [1/25], Train Loss: 0.00028436092543415725, Validation Loss: 0.00019087818557939802\n",
      "Epoch [1/25], Train Loss: 0.00019383677863515913, Validation Loss: 0.00018575942813185974\n",
      "Epoch [1/25], Train Loss: 0.00030142933246679604, Validation Loss: 0.00018778908900761356\n",
      "Epoch [1/25], Train Loss: 0.00022942163923289627, Validation Loss: 0.0001829912856919691\n",
      "Epoch [1/25], Train Loss: 0.00022325487225316465, Validation Loss: 0.0001862532725984541\n",
      "Epoch [1/25], Train Loss: 0.00022935733431950212, Validation Loss: 0.00017730962693652448\n",
      "Epoch [1/25], Train Loss: 0.00019190885359421372, Validation Loss: 0.00018673596835772817\n",
      "Epoch [1/25], Train Loss: 0.0001652432547416538, Validation Loss: 0.00017301406066205042\n",
      "Epoch [1/25], Train Loss: 0.00023682141909375787, Validation Loss: 0.00018137406368623488\n",
      "Epoch [1/25], Train Loss: 0.00022299567353911698, Validation Loss: 0.00018101784953614697\n",
      "Epoch [1/25], Train Loss: 0.00023540151596534997, Validation Loss: 0.00017057583827408961\n",
      "Epoch [1/25], Train Loss: 0.000204714568099007, Validation Loss: 0.0001700396016531158\n",
      "Epoch [1/25], Train Loss: 0.00018606081721372902, Validation Loss: 0.00018455697427270933\n",
      "Epoch [1/25], Train Loss: 0.00020422744273673743, Validation Loss: 0.00025541154561021053\n",
      "Epoch [1/25], Train Loss: 0.00025246903533115983, Validation Loss: 0.00024577525667458154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Train Loss: 0.00030581504688598216, Validation Loss: 0.00017137238658809412\n",
      "Epoch [1/25], Train Loss: 0.00016926693206187338, Validation Loss: 0.0002122360902527968\n",
      "Epoch [1/25], Train Loss: 0.00023403824889101088, Validation Loss: 0.00017340660997433588\n",
      "Epoch [1/25], Train Loss: 0.00013912658323533833, Validation Loss: 0.00020225828920956702\n",
      "Epoch [1/25], Train Loss: 0.0002534888335503638, Validation Loss: 0.00018247115998140846\n",
      "Epoch [1/25], Train Loss: 0.0002173171960748732, Validation Loss: 0.00018348809195837626\n",
      "Epoch [1/25], Train Loss: 0.00018531843670643866, Validation Loss: 0.0001936939040509363\n",
      "Epoch [1/25], Train Loss: 0.00018114152771886438, Validation Loss: 0.00017714082835785424\n",
      "Epoch [1/25], Train Loss: 0.00020532016060315073, Validation Loss: 0.00018735821455872307\n",
      "Epoch [1/25], Train Loss: 0.00015396549133583903, Validation Loss: 0.00018787704417870069\n",
      "Epoch [1/25], Train Loss: 0.00021505549375433475, Validation Loss: 0.00017833198968825553\n",
      "Epoch [1/25], Train Loss: 0.0001578134106239304, Validation Loss: 0.00018545638643748438\n",
      "Epoch [1/25], Train Loss: 0.00017729777027852833, Validation Loss: 0.0001840006526132735\n",
      "Epoch [1/25], Train Loss: 0.00024819240206852555, Validation Loss: 0.00017779239982094927\n",
      "Epoch [1/25], Train Loss: 0.00019238943059463054, Validation Loss: 0.0001831278646326003\n",
      "Epoch [1/25], Train Loss: 0.000212251819903031, Validation Loss: 0.0001806506387462529\n",
      "Epoch [1/25], Train Loss: 0.00012690489529632032, Validation Loss: 0.00017639613918921289\n",
      "Epoch [1/25], Train Loss: 0.00020960417168680578, Validation Loss: 0.00018022901883038382\n",
      "Epoch [1/25], Train Loss: 0.0001804076600819826, Validation Loss: 0.00017588394233219635\n",
      "Epoch [1/25], Train Loss: 0.00021016343089286238, Validation Loss: 0.00017529149045003578\n",
      "Epoch [1/25], Train Loss: 0.00020919181406497955, Validation Loss: 0.00017595539320609533\n",
      "Epoch [1/25], Train Loss: 0.00017078629753086716, Validation Loss: 0.00017156196966728506\n",
      "Epoch [1/25], Train Loss: 0.00018737083883024752, Validation Loss: 0.0001736853167434068\n",
      "Epoch [1/25], Train Loss: 0.00014581480354536325, Validation Loss: 0.00016920760778399806\n",
      "Epoch [1/25], Train Loss: 0.00017259453306905925, Validation Loss: 0.00017096371714918254\n",
      "Epoch [1/25], Train Loss: 0.00022885919315740466, Validation Loss: 0.0001664007283883014\n",
      "Epoch [1/25], Train Loss: 0.00019028857059311122, Validation Loss: 0.00016800126977614127\n",
      "Epoch [1/25], Train Loss: 0.00023357378086075187, Validation Loss: 0.00016436616133432835\n",
      "Epoch [1/25], Train Loss: 0.00018220524361822754, Validation Loss: 0.0001629104415769689\n",
      "Epoch [1/25], Train Loss: 0.0001553364854771644, Validation Loss: 0.00016564560210099443\n",
      "Epoch [1/25], Train Loss: 0.00020843747188337147, Validation Loss: 0.00016591634339420126\n",
      "Epoch [1/25], Train Loss: 0.00023165027960203588, Validation Loss: 0.00016835306038653168\n",
      "Epoch [1/25], Train Loss: 0.0001349886297248304, Validation Loss: 0.00018157880889096606\n",
      "Epoch [1/25], Train Loss: 0.00017887148715090007, Validation Loss: 0.00023163921481076006\n",
      "Epoch [1/25], Train Loss: 0.00019280644482932985, Validation Loss: 0.00035673213618186614\n",
      "Epoch [1/25], Train Loss: 0.0004053063748870045, Validation Loss: 0.0002186948651797138\n",
      "Epoch [1/25], Train Loss: 0.00025549819110892713, Validation Loss: 0.00018386647813410187\n",
      "Epoch [1/25], Train Loss: 0.000191742175957188, Validation Loss: 0.00022047107195248826\n",
      "Epoch [1/25], Train Loss: 0.000250625133048743, Validation Loss: 0.00018268356070620938\n",
      "Epoch [1/25], Train Loss: 0.00019712955690920353, Validation Loss: 0.00018797838614167026\n",
      "Epoch [1/25], Train Loss: 0.00016045711527112871, Validation Loss: 0.0001949934686611717\n",
      "Epoch [1/25], Train Loss: 0.0002590676012914628, Validation Loss: 0.00016891048532367373\n",
      "Epoch [1/25], Train Loss: 0.0002015660284087062, Validation Loss: 0.00019148296608667199\n",
      "Epoch [1/25], Train Loss: 0.00028032404952682555, Validation Loss: 0.00018296411193053548\n",
      "Epoch [1/25], Train Loss: 0.00020450390002224594, Validation Loss: 0.00017100960030802526\n",
      "Epoch [1/25], Train Loss: 0.00023824299569241703, Validation Loss: 0.0001919215845797832\n",
      "Epoch [1/25], Train Loss: 0.0002156395639758557, Validation Loss: 0.00017313658366523063\n",
      "Epoch [1/25], Train Loss: 0.00019182714459020644, Validation Loss: 0.00017731824021514816\n",
      "Epoch [1/25], Train Loss: 0.00015661687939427793, Validation Loss: 0.000183902424275099\n",
      "Epoch [1/25], Train Loss: 0.00016647680604364723, Validation Loss: 0.00017711186083033681\n",
      "Epoch [1/25], Train Loss: 0.00016415372374467552, Validation Loss: 0.00017347018268386213\n",
      "Epoch [1/25], Train Loss: 0.00017813492740970105, Validation Loss: 0.00017921643157023937\n",
      "Epoch [1/25], Train Loss: 0.00016356175183318555, Validation Loss: 0.00017757531071159368\n",
      "Epoch [1/25], Train Loss: 0.00018767120491247624, Validation Loss: 0.0001714274180509771\n",
      "Epoch [1/25], Train Loss: 0.00018051675579044968, Validation Loss: 0.00017532222288233848\n",
      "Epoch [1/25], Train Loss: 0.0002455508220009506, Validation Loss: 0.00017123192956205457\n",
      "Epoch [1/25], Train Loss: 0.0001754620752763003, Validation Loss: 0.0001704663370522515\n",
      "Epoch [1/25], Train Loss: 0.00026050180895254016, Validation Loss: 0.00017184039631198782\n",
      "Epoch [1/25], Train Loss: 0.00018311811436433345, Validation Loss: 0.00016887545546827217\n",
      "Epoch [1/25], Train Loss: 0.00014734541764482856, Validation Loss: 0.0001665634942279818\n",
      "Epoch [1/25], Train Loss: 0.00014809193089604378, Validation Loss: 0.000167000593986207\n",
      "Epoch [1/25], Train Loss: 0.0002134835667675361, Validation Loss: 0.00016245744676173974\n",
      "Epoch [1/25], Train Loss: 0.00012725962733384222, Validation Loss: 0.0001654095258951808\n",
      "Epoch [1/25], Train Loss: 0.0001542264799354598, Validation Loss: 0.00016293388786531676\n",
      "Epoch [1/25], Train Loss: 0.00017512644990347326, Validation Loss: 0.0001592655093797172\n",
      "Epoch [1/25], Train Loss: 0.00015038241690490395, Validation Loss: 0.00015869512305168124\n",
      "Epoch [1/25], Train Loss: 0.0002373146708123386, Validation Loss: 0.00015913682388296973\n",
      "Epoch [1/25], Train Loss: 0.00017983600264415145, Validation Loss: 0.00016238246195522758\n",
      "Epoch [1/25], Train Loss: 0.00014993992226663977, Validation Loss: 0.00017862607007070134\n",
      "Epoch [1/25], Train Loss: 0.00018813325732480735, Validation Loss: 0.0002409112067349876\n",
      "Epoch [1/25], Train Loss: 0.00029277749126777053, Validation Loss: 0.0002612942572644291\n",
      "Epoch [1/25], Train Loss: 0.0002973365772049874, Validation Loss: 0.00016106675078238672\n",
      "Epoch [1/25], Train Loss: 0.00016047866665758193, Validation Loss: 0.00021029860702886555\n",
      "Epoch [1/25], Train Loss: 0.00017109497275669128, Validation Loss: 0.00016841661975680228\n",
      "Epoch [1/25], Train Loss: 0.00025609650765545666, Validation Loss: 0.0001967941318677428\n",
      "Epoch [1/25], Train Loss: 0.0002073076757369563, Validation Loss: 0.0001650395994753732\n",
      "Epoch [1/25], Train Loss: 0.00016791076632216573, Validation Loss: 0.00018558005782930801\n",
      "Epoch [1/25], Train Loss: 0.00017845626280177385, Validation Loss: 0.00017606602535427858\n",
      "Epoch [1/25], Train Loss: 0.00011784931120928377, Validation Loss: 0.00017059053861885333\n",
      "Epoch [1/25], Train Loss: 0.00015614279254805297, Validation Loss: 0.0001837609374585251\n",
      "Epoch [1/25], Train Loss: 0.0002270650293212384, Validation Loss: 0.00017437807036913\n",
      "Epoch [1/25], Train Loss: 0.0001488846173742786, Validation Loss: 0.00016981696874912206\n",
      "Epoch [1/25], Train Loss: 0.00011917549272766337, Validation Loss: 0.0001788024931253555\n",
      "Epoch [1/25], Train Loss: 0.00017337393364869058, Validation Loss: 0.00017326891368914707\n",
      "Epoch [2/25], Train Loss: 0.000248407683102414, Validation Loss: 0.00017003983884933405\n",
      "Epoch [2/25], Train Loss: 0.0001396680745529011, Validation Loss: 0.0001752104163945963\n",
      "Epoch [2/25], Train Loss: 0.00017696729628369212, Validation Loss: 0.00017383537512311403\n",
      "Epoch [2/25], Train Loss: 0.0001628367608645931, Validation Loss: 0.00016947956537478605\n",
      "Epoch [2/25], Train Loss: 0.000150371459312737, Validation Loss: 0.0001721216249279678\n",
      "Epoch [2/25], Train Loss: 0.0001604663993930444, Validation Loss: 0.0001719562106397158\n",
      "Epoch [2/25], Train Loss: 0.000177474576048553, Validation Loss: 0.00016790576410130598\n",
      "Epoch [2/25], Train Loss: 0.0002979971468448639, Validation Loss: 0.00016917107544334916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Train Loss: 0.00017089488392230123, Validation Loss: 0.00016916813983698377\n",
      "Epoch [2/25], Train Loss: 0.00020385721290949732, Validation Loss: 0.0001659660925118563\n",
      "Epoch [2/25], Train Loss: 0.0001672294456511736, Validation Loss: 0.00016694211202169149\n",
      "Epoch [2/25], Train Loss: 0.00020864751422777772, Validation Loss: 0.00016592981070668127\n",
      "Epoch [2/25], Train Loss: 0.0002485971199348569, Validation Loss: 0.0001633182939258404\n",
      "Epoch [2/25], Train Loss: 0.00017064179701264948, Validation Loss: 0.00016430583612721725\n",
      "Epoch [2/25], Train Loss: 0.00022358205751515925, Validation Loss: 0.00016167127638861228\n",
      "Epoch [2/25], Train Loss: 0.00018119373999070376, Validation Loss: 0.00016177846385592904\n",
      "Epoch [2/25], Train Loss: 0.00012697940110228956, Validation Loss: 0.00016047125997526262\n",
      "Epoch [2/25], Train Loss: 0.0001902818912640214, Validation Loss: 0.0001600316451610221\n",
      "Epoch [2/25], Train Loss: 0.00014317040040623397, Validation Loss: 0.00015949596830372076\n",
      "Epoch [2/25], Train Loss: 0.0002103085134876892, Validation Loss: 0.00015875994601325752\n",
      "Epoch [2/25], Train Loss: 0.0001613195927347988, Validation Loss: 0.0001581420721777249\n",
      "Epoch [2/25], Train Loss: 0.0001566456485306844, Validation Loss: 0.0001584600965240194\n",
      "Epoch [2/25], Train Loss: 0.0001091262383852154, Validation Loss: 0.00015697958394108962\n",
      "Epoch [2/25], Train Loss: 0.00016952402074821293, Validation Loss: 0.00015811043655655037\n",
      "Epoch [2/25], Train Loss: 0.0001938203495228663, Validation Loss: 0.00015756251571777586\n",
      "Epoch [2/25], Train Loss: 0.00019039904873352498, Validation Loss: 0.00015676149657034937\n",
      "Epoch [2/25], Train Loss: 0.00020570747437886894, Validation Loss: 0.0001578880214462212\n",
      "Epoch [2/25], Train Loss: 0.00017788450350053608, Validation Loss: 0.00015799432464215594\n",
      "Epoch [2/25], Train Loss: 0.00018362166883889586, Validation Loss: 0.00015669255202131657\n",
      "Epoch [2/25], Train Loss: 0.00019692859495989978, Validation Loss: 0.00015688794907570507\n",
      "Epoch [2/25], Train Loss: 0.00019044718646910042, Validation Loss: 0.00015811269113328308\n",
      "Epoch [2/25], Train Loss: 0.00023012945894151926, Validation Loss: 0.00015744881335801134\n",
      "Epoch [2/25], Train Loss: 0.0001818928576540202, Validation Loss: 0.0001567593504053851\n",
      "Epoch [2/25], Train Loss: 0.0001643827126827091, Validation Loss: 0.0001566041522892192\n",
      "Epoch [2/25], Train Loss: 0.00014012683823239058, Validation Loss: 0.00015788892366496536\n",
      "Epoch [2/25], Train Loss: 0.00014180158905219287, Validation Loss: 0.0001639414583526862\n",
      "Epoch [2/25], Train Loss: 0.00018585134239401668, Validation Loss: 0.00016638596474270647\n",
      "Epoch [2/25], Train Loss: 0.00018699656357057393, Validation Loss: 0.00016368140689640616\n",
      "Epoch [2/25], Train Loss: 0.0002028772869380191, Validation Loss: 0.00015650494145423485\n",
      "Epoch [2/25], Train Loss: 0.00016813032561913133, Validation Loss: 0.00015926528576528653\n",
      "Epoch [2/25], Train Loss: 0.00014694863057229668, Validation Loss: 0.0001638200298960631\n",
      "Epoch [2/25], Train Loss: 0.0001812169939512387, Validation Loss: 0.0001575691356265452\n",
      "Epoch [2/25], Train Loss: 0.00014433596516028047, Validation Loss: 0.0001571796388210108\n",
      "Epoch [2/25], Train Loss: 0.00015559619350824505, Validation Loss: 0.00016121099809727942\n",
      "Epoch [2/25], Train Loss: 0.0002599547151476145, Validation Loss: 0.00015630265140013458\n",
      "Epoch [2/25], Train Loss: 0.000203519084607251, Validation Loss: 0.00015841810818528756\n",
      "Epoch [2/25], Train Loss: 0.00016102542576845735, Validation Loss: 0.00015925876844751958\n",
      "Epoch [2/25], Train Loss: 0.000153509943629615, Validation Loss: 0.00015608168663068986\n",
      "Epoch [2/25], Train Loss: 0.00015539473679382354, Validation Loss: 0.00015944036592069704\n",
      "Epoch [2/25], Train Loss: 0.00015252895536832511, Validation Loss: 0.00015709532138619882\n",
      "Epoch [2/25], Train Loss: 0.000187693556654267, Validation Loss: 0.00015712265085312537\n",
      "Epoch [2/25], Train Loss: 0.0001951274462044239, Validation Loss: 0.00015817412713658996\n",
      "Epoch [2/25], Train Loss: 0.00015716953203082085, Validation Loss: 0.00015600379362391928\n",
      "Epoch [2/25], Train Loss: 0.00019424050697125494, Validation Loss: 0.00015769275681426126\n",
      "Epoch [2/25], Train Loss: 0.0001966629206435755, Validation Loss: 0.00015621035199728795\n",
      "Epoch [2/25], Train Loss: 0.000194107778952457, Validation Loss: 0.00015667414603133997\n",
      "Epoch [2/25], Train Loss: 0.00019034287834074348, Validation Loss: 0.00015702695794364747\n",
      "Epoch [2/25], Train Loss: 0.0001782342151273042, Validation Loss: 0.0001557817512851519\n",
      "Epoch [2/25], Train Loss: 0.00016430979303549975, Validation Loss: 0.0001569711561993851\n",
      "Epoch [2/25], Train Loss: 0.0001943313836818561, Validation Loss: 0.00015600989621210223\n",
      "Epoch [2/25], Train Loss: 0.00012236514885444194, Validation Loss: 0.00015589564524513358\n",
      "Epoch [2/25], Train Loss: 0.00019569514552131295, Validation Loss: 0.0001565179474710021\n",
      "Epoch [2/25], Train Loss: 0.0001571778120705858, Validation Loss: 0.00015552464925955672\n",
      "Epoch [2/25], Train Loss: 0.00016730975767131895, Validation Loss: 0.00015596360851001615\n",
      "Epoch [2/25], Train Loss: 0.0001342123287031427, Validation Loss: 0.00015615594820701518\n",
      "Epoch [2/25], Train Loss: 0.00014250585809350014, Validation Loss: 0.00015533373322493087\n",
      "Epoch [2/25], Train Loss: 0.00014052919868845493, Validation Loss: 0.00015580722125984418\n",
      "Epoch [2/25], Train Loss: 0.00019469563267193735, Validation Loss: 0.00015570908314354407\n",
      "Epoch [2/25], Train Loss: 0.00018420032574795187, Validation Loss: 0.000155250903723451\n",
      "Epoch [2/25], Train Loss: 0.00018106189963873476, Validation Loss: 0.00015555072435139057\n",
      "Epoch [2/25], Train Loss: 0.00018267329141963273, Validation Loss: 0.0001555546790768858\n",
      "Epoch [2/25], Train Loss: 0.00020680122543126345, Validation Loss: 0.00015517804325403025\n",
      "Epoch [2/25], Train Loss: 0.00016050669364631176, Validation Loss: 0.00015536243251214424\n",
      "Epoch [2/25], Train Loss: 0.00013438667519949377, Validation Loss: 0.00015563744309474715\n",
      "Epoch [2/25], Train Loss: 0.00013983927783556283, Validation Loss: 0.00015525062238642325\n",
      "Epoch [2/25], Train Loss: 0.0001044520759023726, Validation Loss: 0.00015505733511721095\n",
      "Epoch [2/25], Train Loss: 0.00016122478700708598, Validation Loss: 0.00015529832501973335\n",
      "Epoch [2/25], Train Loss: 0.0001621185801923275, Validation Loss: 0.0001552957561216317\n",
      "Epoch [2/25], Train Loss: 0.0001596657675690949, Validation Loss: 0.00015500283819468071\n",
      "Epoch [2/25], Train Loss: 8.907540905056521e-05, Validation Loss: 0.00015522402948893918\n",
      "Epoch [2/25], Train Loss: 0.0002097454125760123, Validation Loss: 0.00015549559927118632\n",
      "Epoch [2/25], Train Loss: 0.00017314733122475445, Validation Loss: 0.0001551042432159496\n",
      "Epoch [2/25], Train Loss: 0.0002215086860815063, Validation Loss: 0.0001549350296651634\n",
      "Epoch [2/25], Train Loss: 0.0001466151443310082, Validation Loss: 0.00015510427983826957\n",
      "Epoch [2/25], Train Loss: 0.00012486647756304592, Validation Loss: 0.0001551506997202523\n",
      "Epoch [2/25], Train Loss: 0.0002066070883302018, Validation Loss: 0.00015495248735533095\n",
      "Epoch [2/25], Train Loss: 0.00019323945161886513, Validation Loss: 0.00015487345808651298\n",
      "Epoch [2/25], Train Loss: 0.00012870760110672563, Validation Loss: 0.0001550661885024359\n",
      "Epoch [2/25], Train Loss: 0.00016021044575609267, Validation Loss: 0.00015500532899750396\n",
      "Epoch [2/25], Train Loss: 0.00013078615302219987, Validation Loss: 0.00015476715052500367\n",
      "Epoch [2/25], Train Loss: 0.00013763914466835558, Validation Loss: 0.00015481198085277962\n",
      "Epoch [2/25], Train Loss: 0.00012077163410140201, Validation Loss: 0.00015486407501157372\n",
      "Epoch [2/25], Train Loss: 0.0001406805677106604, Validation Loss: 0.00015478278655791655\n",
      "Epoch [2/25], Train Loss: 0.00019987754058092833, Validation Loss: 0.00015476996049983426\n",
      "Epoch [2/25], Train Loss: 0.00013213152124080807, Validation Loss: 0.000154974908461251\n",
      "Epoch [2/25], Train Loss: 0.000169505481608212, Validation Loss: 0.00015502169044339097\n",
      "Epoch [2/25], Train Loss: 0.0002108885528286919, Validation Loss: 0.00015477571626737092\n",
      "Epoch [2/25], Train Loss: 0.0001209320907946676, Validation Loss: 0.0001546707057665723\n",
      "Epoch [2/25], Train Loss: 0.00013314637180883437, Validation Loss: 0.00015492041250884844\n",
      "Epoch [2/25], Train Loss: 0.0001327271165791899, Validation Loss: 0.00015494475592277014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Train Loss: 0.0002000250678975135, Validation Loss: 0.0001546538941814409\n",
      "Epoch [2/25], Train Loss: 0.00019771570805460215, Validation Loss: 0.00015466628004408753\n",
      "Epoch [2/25], Train Loss: 0.0001978820946533233, Validation Loss: 0.0001548545798868872\n",
      "Epoch [2/25], Train Loss: 0.0001306091871811077, Validation Loss: 0.00015476500387497558\n",
      "Epoch [2/25], Train Loss: 0.00016831808898132294, Validation Loss: 0.0001545562183309812\n",
      "Epoch [2/25], Train Loss: 0.00015283857646863908, Validation Loss: 0.0001545580329548102\n",
      "Epoch [2/25], Train Loss: 0.00011124146840302274, Validation Loss: 0.0001547098159790039\n",
      "Epoch [2/25], Train Loss: 0.00019045008230023086, Validation Loss: 0.00015471278626743394\n",
      "Epoch [2/25], Train Loss: 0.00014401214139070362, Validation Loss: 0.00015455818211194128\n",
      "Epoch [2/25], Train Loss: 0.0001523954706499353, Validation Loss: 0.00015446431595288838\n",
      "Epoch [2/25], Train Loss: 0.0001662897993810475, Validation Loss: 0.00015447898040292783\n",
      "Epoch [2/25], Train Loss: 0.00014898317749612033, Validation Loss: 0.00015446166410886992\n",
      "Epoch [2/25], Train Loss: 0.00018446767353452742, Validation Loss: 0.00015438731109801058\n",
      "Epoch [2/25], Train Loss: 0.00016488597611896694, Validation Loss: 0.00015433705290585447\n",
      "Epoch [2/25], Train Loss: 0.0001372751867165789, Validation Loss: 0.00015433734151883982\n",
      "Epoch [2/25], Train Loss: 0.00016492826398462057, Validation Loss: 0.00015443681598602174\n",
      "Epoch [2/25], Train Loss: 0.00013205480354372412, Validation Loss: 0.0001544325355401573\n",
      "Epoch [2/25], Train Loss: 0.00016210188914556056, Validation Loss: 0.00015430761753426243\n",
      "Epoch [2/25], Train Loss: 0.00012582351337186992, Validation Loss: 0.00015426684549311176\n",
      "Epoch [2/25], Train Loss: 0.00023357223835773766, Validation Loss: 0.00015439702692674473\n",
      "Epoch [2/25], Train Loss: 0.00015713517495896667, Validation Loss: 0.00015452080236476226\n",
      "Epoch [2/25], Train Loss: 0.00014692926197312772, Validation Loss: 0.00015441864355428454\n",
      "Epoch [2/25], Train Loss: 0.0002166406047763303, Validation Loss: 0.00015430683342856354\n",
      "Epoch [2/25], Train Loss: 0.00013344726176001132, Validation Loss: 0.00015417092727147973\n",
      "Epoch [2/25], Train Loss: 0.0002845299313776195, Validation Loss: 0.00015416295330699842\n",
      "Epoch [2/25], Train Loss: 0.00018461549188941717, Validation Loss: 0.00015421962283047226\n",
      "Epoch [2/25], Train Loss: 0.00018856453243643045, Validation Loss: 0.0001541878865585507\n",
      "Epoch [2/25], Train Loss: 0.00014532363275066018, Validation Loss: 0.00015412112382667448\n",
      "Epoch [2/25], Train Loss: 0.00021925268811173737, Validation Loss: 0.00015421481560527658\n",
      "Epoch [2/25], Train Loss: 0.00017837841005530208, Validation Loss: 0.0001543962163850665\n",
      "Epoch [2/25], Train Loss: 0.0001744753826642409, Validation Loss: 0.0001543183607282117\n",
      "Epoch [2/25], Train Loss: 0.00017531018238514662, Validation Loss: 0.00015417557103016103\n",
      "Epoch [2/25], Train Loss: 0.00014780975470785052, Validation Loss: 0.0001542259977820019\n",
      "Epoch [2/25], Train Loss: 0.000192914143553935, Validation Loss: 0.00015412935220714038\n",
      "Epoch [2/25], Train Loss: 0.00015032767259981483, Validation Loss: 0.0001539423785288818\n",
      "Epoch [2/25], Train Loss: 0.00014877060311846435, Validation Loss: 0.00015392538358961853\n",
      "Epoch [2/25], Train Loss: 0.00013873811985831708, Validation Loss: 0.00015404410539000916\n",
      "Epoch [2/25], Train Loss: 0.00016325053002219647, Validation Loss: 0.0001540942927628445\n",
      "Epoch [2/25], Train Loss: 0.00017111636407207698, Validation Loss: 0.00015410181974099638\n",
      "Epoch [2/25], Train Loss: 0.0001505571126472205, Validation Loss: 0.0001540122456693401\n",
      "Epoch [2/25], Train Loss: 0.0001422649365849793, Validation Loss: 0.0001538571106114735\n",
      "Epoch [2/25], Train Loss: 0.00017646861670073122, Validation Loss: 0.00015377018765623993\n",
      "Epoch [2/25], Train Loss: 0.00019169058941770345, Validation Loss: 0.00015375412670740236\n",
      "Epoch [2/25], Train Loss: 0.0001612059131730348, Validation Loss: 0.00015372936735123707\n",
      "Epoch [2/25], Train Loss: 0.00014992734941188246, Validation Loss: 0.00015375211830056894\n",
      "Epoch [2/25], Train Loss: 0.0001985782728297636, Validation Loss: 0.00015373529167845846\n",
      "Epoch [2/25], Train Loss: 0.00015493411046918482, Validation Loss: 0.0001538002669500808\n",
      "Epoch [2/25], Train Loss: 0.00013286406465340406, Validation Loss: 0.00015375308042469744\n",
      "Epoch [2/25], Train Loss: 0.00018023524899035692, Validation Loss: 0.00015376519513665698\n",
      "Epoch [2/25], Train Loss: 0.00012756054638884962, Validation Loss: 0.00015363272880980123\n",
      "Epoch [2/25], Train Loss: 0.00017670477973297238, Validation Loss: 0.00015356282116651225\n",
      "Epoch [2/25], Train Loss: 0.0001299358409596607, Validation Loss: 0.000153494155529188\n",
      "Epoch [2/25], Train Loss: 0.00019220190006308258, Validation Loss: 0.00015354668139480054\n",
      "Epoch [2/25], Train Loss: 0.00012642932415474206, Validation Loss: 0.00015359287863248028\n",
      "Epoch [2/25], Train Loss: 0.00014512086636386812, Validation Loss: 0.00015362290578195824\n",
      "Epoch [2/25], Train Loss: 0.00017649686196818948, Validation Loss: 0.00015371063103278477\n",
      "Epoch [2/25], Train Loss: 0.00014828849816694856, Validation Loss: 0.00015381903576781042\n",
      "Epoch [2/25], Train Loss: 0.00013136285997461528, Validation Loss: 0.00015369027096312493\n",
      "Epoch [2/25], Train Loss: 0.00019851567049045116, Validation Loss: 0.0001536673749797046\n",
      "Epoch [2/25], Train Loss: 0.00010919800115516409, Validation Loss: 0.0001536456412092472\n",
      "Epoch [2/25], Train Loss: 0.0001929538993863389, Validation Loss: 0.00015377721186572065\n",
      "Epoch [2/25], Train Loss: 0.00017071168986149132, Validation Loss: 0.0001537204404788402\n",
      "Epoch [2/25], Train Loss: 0.00019822198373731226, Validation Loss: 0.00015387990376135955\n",
      "Epoch [2/25], Train Loss: 0.00016257718380074948, Validation Loss: 0.00015404819326552872\n",
      "Epoch [2/25], Train Loss: 0.00020530006440822035, Validation Loss: 0.00015430834173457697\n",
      "Epoch [2/25], Train Loss: 0.0001727993367239833, Validation Loss: 0.00015468795754713938\n",
      "Epoch [2/25], Train Loss: 0.0001728497736621648, Validation Loss: 0.0001555474977067206\n",
      "Epoch [2/25], Train Loss: 0.00014158616249915212, Validation Loss: 0.0001566562546940986\n",
      "Epoch [2/25], Train Loss: 0.00012696000339929014, Validation Loss: 0.0001585767315797663\n",
      "Epoch [2/25], Train Loss: 0.00011030169116565958, Validation Loss: 0.00016011458452946196\n",
      "Epoch [2/25], Train Loss: 0.0001820827164920047, Validation Loss: 0.0001624332920376522\n",
      "Epoch [2/25], Train Loss: 0.00018819994875229895, Validation Loss: 0.00016194124060954588\n",
      "Epoch [2/25], Train Loss: 0.00022450558026321232, Validation Loss: 0.0001600452090012065\n",
      "Epoch [2/25], Train Loss: 0.00022325955796986818, Validation Loss: 0.00015568897821746457\n",
      "Epoch [2/25], Train Loss: 0.00015965990314725786, Validation Loss: 0.0001532230247297169\n",
      "Epoch [2/25], Train Loss: 0.00019750671344809234, Validation Loss: 0.00015436408478611459\n",
      "Epoch [2/25], Train Loss: 0.00021642436331603676, Validation Loss: 0.00015634522763624165\n",
      "Epoch [2/25], Train Loss: 0.0001426318776793778, Validation Loss: 0.00015587461627243707\n",
      "Epoch [2/25], Train Loss: 0.00011888493463629857, Validation Loss: 0.00015394519238422314\n",
      "Epoch [2/25], Train Loss: 0.0001618692622287199, Validation Loss: 0.00015325133645092138\n",
      "Epoch [2/25], Train Loss: 0.00012887251796200871, Validation Loss: 0.00015474676474696024\n",
      "Epoch [2/25], Train Loss: 0.00017812973237596452, Validation Loss: 0.00015512604635053623\n",
      "Epoch [2/25], Train Loss: 0.00010915532766375691, Validation Loss: 0.00015412628393581448\n",
      "Epoch [2/25], Train Loss: 0.00012048589996993542, Validation Loss: 0.00015316102314197149\n",
      "Epoch [2/25], Train Loss: 0.00011810399155365303, Validation Loss: 0.00015398735267808662\n",
      "Epoch [2/25], Train Loss: 0.00011790645658038557, Validation Loss: 0.0001547490624943748\n",
      "Epoch [2/25], Train Loss: 0.0002052906493190676, Validation Loss: 0.00015395428078287903\n",
      "Epoch [2/25], Train Loss: 0.00016303153824992478, Validation Loss: 0.00015317065190174618\n",
      "Epoch [2/25], Train Loss: 0.0002168475475627929, Validation Loss: 0.00015350659280860174\n",
      "Epoch [2/25], Train Loss: 0.00010850455146282911, Validation Loss: 0.0001542062139681851\n",
      "Epoch [2/25], Train Loss: 0.0001493541494710371, Validation Loss: 0.00015388098569625678\n",
      "Epoch [2/25], Train Loss: 0.00014710583491250873, Validation Loss: 0.00015319955491577275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Train Loss: 0.00011433918552938849, Validation Loss: 0.00015291859454009683\n",
      "Epoch [2/25], Train Loss: 0.0001575723144924268, Validation Loss: 0.0001534944317730454\n",
      "Epoch [2/25], Train Loss: 0.00015415166853927076, Validation Loss: 0.00015407126678231484\n",
      "Epoch [2/25], Train Loss: 0.0001725593174342066, Validation Loss: 0.00015398496810424452\n",
      "Epoch [2/25], Train Loss: 0.00018625080701895058, Validation Loss: 0.00015338037216376202\n",
      "Epoch [2/25], Train Loss: 0.00016741811123210937, Validation Loss: 0.00015291574964066968\n",
      "Epoch [2/25], Train Loss: 0.00015159406757447869, Validation Loss: 0.00015270253425114788\n",
      "Epoch [2/25], Train Loss: 0.00014232630201149732, Validation Loss: 0.00015301487922746068\n",
      "Epoch [2/25], Train Loss: 0.00013201820547692478, Validation Loss: 0.00015317796326902073\n",
      "Epoch [2/25], Train Loss: 0.00013645237777382135, Validation Loss: 0.00015320941796138262\n",
      "Epoch [2/25], Train Loss: 0.00016609681188128889, Validation Loss: 0.00015296877778988953\n",
      "Epoch [2/25], Train Loss: 0.00020062037219759077, Validation Loss: 0.00015269128634827213\n",
      "Epoch [2/25], Train Loss: 0.00012692503514699638, Validation Loss: 0.00015275503295318534\n",
      "Epoch [2/25], Train Loss: 0.00013728797785006464, Validation Loss: 0.0001528471916875181\n",
      "Epoch [2/25], Train Loss: 0.00017420595395378768, Validation Loss: 0.00015271015702940835\n",
      "Epoch [2/25], Train Loss: 0.00015700847143307328, Validation Loss: 0.0001526712573346837\n",
      "Epoch [2/25], Train Loss: 0.00014729669783264399, Validation Loss: 0.00015263707440074843\n",
      "Epoch [2/25], Train Loss: 0.0001827115484047681, Validation Loss: 0.00015258518445383136\n",
      "Epoch [2/25], Train Loss: 0.00010755052062449977, Validation Loss: 0.00015257357251054297\n",
      "Epoch [2/25], Train Loss: 0.0001519515208201483, Validation Loss: 0.0001526322004792746\n",
      "Epoch [2/25], Train Loss: 0.00018304902187082916, Validation Loss: 0.00015271838322708693\n",
      "Epoch [2/25], Train Loss: 0.00017345615196973085, Validation Loss: 0.0001526696651126258\n",
      "Epoch [2/25], Train Loss: 0.00012732759932987392, Validation Loss: 0.00015267986285228592\n",
      "Epoch [2/25], Train Loss: 0.00021839953842572868, Validation Loss: 0.000152986034421095\n",
      "Epoch [2/25], Train Loss: 0.00018215904128737748, Validation Loss: 0.0001531256590775835\n",
      "Epoch [2/25], Train Loss: 9.513909026281908e-05, Validation Loss: 0.0001534204532314713\n",
      "Epoch [2/25], Train Loss: 0.00011482764239190146, Validation Loss: 0.00015364774129314658\n",
      "Epoch [2/25], Train Loss: 0.00018063632887788117, Validation Loss: 0.0001543965978877774\n",
      "Epoch [2/25], Train Loss: 0.0001644920266699046, Validation Loss: 0.00015484329196624458\n",
      "Epoch [2/25], Train Loss: 0.00026686431374400854, Validation Loss: 0.00015567967445046332\n",
      "Epoch [2/25], Train Loss: 0.00013380062591750175, Validation Loss: 0.00015596052762703038\n",
      "Epoch [2/25], Train Loss: 0.00019236592925153673, Validation Loss: 0.00015593385687679985\n",
      "Epoch [2/25], Train Loss: 0.0001517103228252381, Validation Loss: 0.0001550995131159046\n",
      "Epoch [2/25], Train Loss: 0.0002795862965285778, Validation Loss: 0.0001542044463955487\n",
      "Epoch [2/25], Train Loss: 0.00016780370788183063, Validation Loss: 0.00015321556469037508\n",
      "Epoch [2/25], Train Loss: 0.00015475308464374393, Validation Loss: 0.00015269385864182065\n",
      "Epoch [2/25], Train Loss: 0.00017081493570003659, Validation Loss: 0.00015232203149935232\n",
      "Epoch [2/25], Train Loss: 0.00015000572602730244, Validation Loss: 0.00015224677651228072\n",
      "Epoch [2/25], Train Loss: 0.00013162934919819236, Validation Loss: 0.00015248459652260257\n",
      "Epoch [2/25], Train Loss: 0.00017721879703458399, Validation Loss: 0.0001528156763621761\n",
      "Epoch [2/25], Train Loss: 0.00017270429816562682, Validation Loss: 0.00015324773800481732\n",
      "Epoch [2/25], Train Loss: 0.00011541732237674296, Validation Loss: 0.00015340388345066457\n",
      "Epoch [2/25], Train Loss: 0.0001978579821297899, Validation Loss: 0.0001532301847570731\n",
      "Epoch [2/25], Train Loss: 0.00017097705858759582, Validation Loss: 0.00015296406321188745\n",
      "Epoch [2/25], Train Loss: 0.00017746459343470633, Validation Loss: 0.00015264485070171457\n",
      "Epoch [2/25], Train Loss: 0.00016596073692198843, Validation Loss: 0.00015234194943332114\n",
      "Epoch [2/25], Train Loss: 9.840737766353413e-05, Validation Loss: 0.0001521684918164586\n",
      "Epoch [2/25], Train Loss: 0.00015311938477680087, Validation Loss: 0.00015218193463321464\n",
      "Epoch [2/25], Train Loss: 0.00017649168148636818, Validation Loss: 0.00015206396807722437\n",
      "Epoch [2/25], Train Loss: 0.00015373727364931256, Validation Loss: 0.00015211521240416915\n",
      "Epoch [2/25], Train Loss: 0.00018238711345475167, Validation Loss: 0.0001521266805260287\n",
      "Epoch [2/25], Train Loss: 0.00018525747873354703, Validation Loss: 0.0001523096778934511\n",
      "Epoch [2/25], Train Loss: 0.00018195118173025548, Validation Loss: 0.00015247941604078127\n",
      "Epoch [2/25], Train Loss: 0.00014405910042114556, Validation Loss: 0.0001530359035920507\n",
      "Epoch [2/25], Train Loss: 0.00016252690693363547, Validation Loss: 0.00015392996744291546\n",
      "Epoch [2/25], Train Loss: 0.0001556822971906513, Validation Loss: 0.00015581641394722586\n",
      "Epoch [2/25], Train Loss: 0.00018121728498954326, Validation Loss: 0.0001585379592142999\n",
      "Epoch [2/25], Train Loss: 0.00012787625018972903, Validation Loss: 0.0001642227024907091\n",
      "Epoch [2/25], Train Loss: 0.00016825713100843132, Validation Loss: 0.00017091574627556839\n",
      "Epoch [2/25], Train Loss: 0.0001691446959739551, Validation Loss: 0.0001822077450924553\n",
      "Epoch [2/25], Train Loss: 0.000151123502291739, Validation Loss: 0.00017857393492401268\n",
      "Epoch [2/25], Train Loss: 0.0001734019024297595, Validation Loss: 0.0001671643842807195\n",
      "Epoch [2/25], Train Loss: 0.00018002001161221415, Validation Loss: 0.00015276861668098718\n",
      "Epoch [2/25], Train Loss: 0.0001331061648670584, Validation Loss: 0.00015790752659086137\n",
      "Epoch [2/25], Train Loss: 0.0002098968398058787, Validation Loss: 0.00016439079360376733\n",
      "Epoch [2/25], Train Loss: 0.0002300154883414507, Validation Loss: 0.00015459493588423356\n",
      "Epoch [2/25], Train Loss: 0.00013588846195489168, Validation Loss: 0.00015552763992066805\n",
      "Epoch [2/25], Train Loss: 0.00020115345250815153, Validation Loss: 0.0001601498959644232\n",
      "Epoch [2/25], Train Loss: 0.00014453874609898776, Validation Loss: 0.00015341584755030151\n",
      "Epoch [2/25], Train Loss: 0.0001428932446287945, Validation Loss: 0.00015640011479263195\n",
      "Epoch [2/25], Train Loss: 0.00019894498109351844, Validation Loss: 0.00015750457532703876\n",
      "Epoch [2/25], Train Loss: 0.0001444513472961262, Validation Loss: 0.00015297323455646012\n",
      "Epoch [2/25], Train Loss: 0.0001782648905646056, Validation Loss: 0.00015690317935271498\n",
      "Epoch [2/25], Train Loss: 0.00015858137339819223, Validation Loss: 0.000155595595909593\n",
      "Epoch [2/25], Train Loss: 0.00016175108612515032, Validation Loss: 0.00015307555925877145\n",
      "Epoch [2/25], Train Loss: 0.00020286055223550647, Validation Loss: 0.00015766971991979518\n",
      "Epoch [3/25], Train Loss: 0.00017939336248673499, Validation Loss: 0.00015506759518757463\n",
      "Epoch [3/25], Train Loss: 0.00014308698882814497, Validation Loss: 0.00015299607524260256\n",
      "Epoch [3/25], Train Loss: 0.0001694792736088857, Validation Loss: 0.00015714265561352173\n",
      "Epoch [3/25], Train Loss: 0.0001475296594435349, Validation Loss: 0.00015511852761846968\n",
      "Epoch [3/25], Train Loss: 0.00022746081231161952, Validation Loss: 0.0001525845429569017\n",
      "Epoch [3/25], Train Loss: 0.00014317520253825933, Validation Loss: 0.00015664924940210766\n",
      "Epoch [3/25], Train Loss: 0.00020239329023752362, Validation Loss: 0.00015610873112260985\n",
      "Epoch [3/25], Train Loss: 0.00016141150263138115, Validation Loss: 0.00015213736648244473\n",
      "Epoch [3/25], Train Loss: 0.00015124035417102277, Validation Loss: 0.00015460679205716587\n",
      "Epoch [3/25], Train Loss: 0.000172258703969419, Validation Loss: 0.00015666481219038056\n",
      "Epoch [3/25], Train Loss: 0.00020064679847564548, Validation Loss: 0.00015295667520452602\n",
      "Epoch [3/25], Train Loss: 0.0001322853786405176, Validation Loss: 0.00015255502633711634\n",
      "Epoch [3/25], Train Loss: 0.00013494101585820317, Validation Loss: 0.00015526599114915976\n",
      "Epoch [3/25], Train Loss: 0.00014959326654206961, Validation Loss: 0.00015424661056992288\n",
      "Epoch [3/25], Train Loss: 0.000171379026141949, Validation Loss: 0.00015203366056084632\n",
      "Epoch [3/25], Train Loss: 0.00013325261534191668, Validation Loss: 0.00015231999932439067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Train Loss: 0.0001424458168912679, Validation Loss: 0.00015393471185234375\n",
      "Epoch [3/25], Train Loss: 0.00011117865506093949, Validation Loss: 0.0001532673804225245\n",
      "Epoch [3/25], Train Loss: 0.00020806158136110753, Validation Loss: 0.00015179524149668092\n",
      "Epoch [3/25], Train Loss: 0.00018662330694496632, Validation Loss: 0.00015262023929002073\n",
      "Epoch [3/25], Train Loss: 0.00010015135194407776, Validation Loss: 0.00015341292770851094\n",
      "Epoch [3/25], Train Loss: 0.0001921598013723269, Validation Loss: 0.00015245615601694834\n",
      "Epoch [3/25], Train Loss: 0.00017482566181570292, Validation Loss: 0.00015178023604676128\n",
      "Epoch [3/25], Train Loss: 0.00011544461449375376, Validation Loss: 0.00015204544203394714\n",
      "Epoch [3/25], Train Loss: 0.00020311710250098258, Validation Loss: 0.00015244209159088012\n",
      "Epoch [3/25], Train Loss: 0.00019345809414517134, Validation Loss: 0.00015228773190756328\n",
      "Epoch [3/25], Train Loss: 0.00015494291437789798, Validation Loss: 0.0001518127020972315\n",
      "Epoch [3/25], Train Loss: 0.00022377833374775946, Validation Loss: 0.00015189063900227968\n",
      "Epoch [3/25], Train Loss: 0.00018664970411919057, Validation Loss: 0.00015223362618902077\n",
      "Epoch [3/25], Train Loss: 0.0001400023902533576, Validation Loss: 0.00015214440669903222\n",
      "Epoch [3/25], Train Loss: 0.00018684841052163392, Validation Loss: 0.0001517984184223072\n",
      "Epoch [3/25], Train Loss: 0.00015424840967170894, Validation Loss: 0.00015170902867491046\n",
      "Epoch [3/25], Train Loss: 0.0001605619618203491, Validation Loss: 0.00015192700593615882\n",
      "Epoch [3/25], Train Loss: 0.00017071687034331262, Validation Loss: 0.00015200592679320835\n",
      "Epoch [3/25], Train Loss: 0.00013709126506000757, Validation Loss: 0.00015180050177150407\n",
      "Epoch [3/25], Train Loss: 0.00014674563135486096, Validation Loss: 0.00015158935251141276\n",
      "Epoch [3/25], Train Loss: 0.00021215269225649536, Validation Loss: 0.0001516258853371255\n",
      "Epoch [3/25], Train Loss: 0.0001382783375447616, Validation Loss: 0.00015172005635880244\n",
      "Epoch [3/25], Train Loss: 0.00020366927492432296, Validation Loss: 0.0001517425999433423\n",
      "Epoch [3/25], Train Loss: 0.00021874636877328157, Validation Loss: 0.00015167219874759514\n",
      "Epoch [3/25], Train Loss: 0.00012076201528543606, Validation Loss: 0.00015157329908106477\n",
      "Epoch [3/25], Train Loss: 0.00017591682262718678, Validation Loss: 0.0001515743705870894\n",
      "Epoch [3/25], Train Loss: 0.00013583601685240865, Validation Loss: 0.00015156432321721997\n",
      "Epoch [3/25], Train Loss: 0.00018906858167611063, Validation Loss: 0.00015153519537610313\n",
      "Epoch [3/25], Train Loss: 0.00013087941624689847, Validation Loss: 0.00015147792801144532\n",
      "Epoch [3/25], Train Loss: 0.000174644315848127, Validation Loss: 0.00015145421469545302\n",
      "Epoch [3/25], Train Loss: 0.0001884687808342278, Validation Loss: 0.0001514677052909974\n",
      "Epoch [3/25], Train Loss: 0.00018128295778296888, Validation Loss: 0.00015149724276852794\n",
      "Epoch [3/25], Train Loss: 0.00018898402049671859, Validation Loss: 0.00015152508009729597\n",
      "Epoch [3/25], Train Loss: 0.00020449985458981246, Validation Loss: 0.00015145948103357417\n",
      "Epoch [3/25], Train Loss: 0.00022245521540753543, Validation Loss: 0.00015136466875749952\n",
      "Epoch [3/25], Train Loss: 0.0001562538236612454, Validation Loss: 0.0001513793550354118\n",
      "Epoch [3/25], Train Loss: 0.00012890227662865072, Validation Loss: 0.00015140506584430114\n",
      "Epoch [3/25], Train Loss: 9.774605860002339e-05, Validation Loss: 0.00015139137637258196\n",
      "Epoch [3/25], Train Loss: 0.00012696729390881956, Validation Loss: 0.00015140955147217028\n",
      "Epoch [3/25], Train Loss: 0.00014202689635567367, Validation Loss: 0.00015136720176087693\n",
      "Epoch [3/25], Train Loss: 0.0001663363364059478, Validation Loss: 0.0001512624980629577\n",
      "Epoch [3/25], Train Loss: 0.00012209199485369027, Validation Loss: 0.00015130630369336966\n",
      "Epoch [3/25], Train Loss: 0.00018741714302450418, Validation Loss: 0.00015128523834088506\n",
      "Epoch [3/25], Train Loss: 0.0001354011765215546, Validation Loss: 0.00015132027522971232\n",
      "Epoch [3/25], Train Loss: 0.00022280240955296904, Validation Loss: 0.00015161241632692205\n",
      "Epoch [3/25], Train Loss: 0.00013140302326064557, Validation Loss: 0.00015160428035111787\n",
      "Epoch [3/25], Train Loss: 0.00018406505114398897, Validation Loss: 0.00015161635067973596\n",
      "Epoch [3/25], Train Loss: 0.00012994842836633325, Validation Loss: 0.00015150488834478893\n",
      "Epoch [3/25], Train Loss: 0.00017896803910844028, Validation Loss: 0.00015132656602266555\n",
      "Epoch [3/25], Train Loss: 0.00019028797396458685, Validation Loss: 0.00015127592317488355\n",
      "Epoch [3/25], Train Loss: 0.00013555090117733926, Validation Loss: 0.00015122669137781485\n",
      "Epoch [3/25], Train Loss: 0.00016274562221951783, Validation Loss: 0.00015122664359902652\n",
      "Epoch [3/25], Train Loss: 0.0001899551134556532, Validation Loss: 0.00015123755705038396\n",
      "Epoch [3/25], Train Loss: 0.0002096532261930406, Validation Loss: 0.00015122099745591792\n",
      "Epoch [3/25], Train Loss: 0.00011577487020986155, Validation Loss: 0.00015129933162825183\n",
      "Epoch [3/25], Train Loss: 0.0001221361744683236, Validation Loss: 0.0001513739400252234\n",
      "Epoch [3/25], Train Loss: 0.0001564247504575178, Validation Loss: 0.00015150742983678357\n",
      "Epoch [3/25], Train Loss: 0.00012272140884306282, Validation Loss: 0.0001515172712970525\n",
      "Epoch [3/25], Train Loss: 0.00019347702618688345, Validation Loss: 0.00015138412806360672\n",
      "Epoch [3/25], Train Loss: 0.00033085449831560254, Validation Loss: 0.00015150440061309685\n",
      "Epoch [3/25], Train Loss: 0.00016424160276073962, Validation Loss: 0.00015161767150857487\n",
      "Epoch [3/25], Train Loss: 0.00013166607823222876, Validation Loss: 0.00015146570658544078\n",
      "Epoch [3/25], Train Loss: 0.00017223483882844448, Validation Loss: 0.00015134741382401748\n",
      "Epoch [3/25], Train Loss: 0.00013796878920402378, Validation Loss: 0.00015136513660157409\n",
      "Epoch [3/25], Train Loss: 0.00014684401685371995, Validation Loss: 0.0001513028936945678\n",
      "Epoch [3/25], Train Loss: 0.0001415574224665761, Validation Loss: 0.00015123506430730533\n",
      "Epoch [3/25], Train Loss: 0.00017736236623022705, Validation Loss: 0.00015124536948860623\n",
      "Epoch [3/25], Train Loss: 0.00022413156693801284, Validation Loss: 0.00015112916444195434\n",
      "Epoch [3/25], Train Loss: 0.00013695040252059698, Validation Loss: 0.00015107759754755533\n",
      "Epoch [3/25], Train Loss: 0.0002112724760081619, Validation Loss: 0.00015110573197792596\n",
      "Epoch [3/25], Train Loss: 0.00014241276949178427, Validation Loss: 0.00015114395209820942\n",
      "Epoch [3/25], Train Loss: 0.00017253414262086153, Validation Loss: 0.00015154315454613728\n",
      "Epoch [3/25], Train Loss: 0.00013703681179322302, Validation Loss: 0.00015235729564058904\n",
      "Epoch [3/25], Train Loss: 0.00020813151786569506, Validation Loss: 0.0001533404584430779\n",
      "Epoch [3/25], Train Loss: 0.00015408349281642586, Validation Loss: 0.00015472174200112932\n",
      "Epoch [3/25], Train Loss: 0.00012205519306007773, Validation Loss: 0.00015689941258945813\n",
      "Epoch [3/25], Train Loss: 0.00015639010234735906, Validation Loss: 0.0001604963923455216\n",
      "Epoch [3/25], Train Loss: 0.00010649528849171475, Validation Loss: 0.0001629136439684468\n",
      "Epoch [3/25], Train Loss: 0.00016018698806874454, Validation Loss: 0.00016453875238463904\n",
      "Epoch [3/25], Train Loss: 0.0001344923221040517, Validation Loss: 0.0001598461327375844\n",
      "Epoch [3/25], Train Loss: 0.00012428064655978233, Validation Loss: 0.00015326621238879548\n",
      "Epoch [3/25], Train Loss: 0.00013737898552790284, Validation Loss: 0.00015154911307035945\n",
      "Epoch [3/25], Train Loss: 0.00015390476619359106, Validation Loss: 0.0001551169370941352\n",
      "Epoch [3/25], Train Loss: 0.00017854732868727297, Validation Loss: 0.0001551286710309796\n",
      "Epoch [3/25], Train Loss: 0.00018629070837050676, Validation Loss: 0.00015166405525330145\n",
      "Epoch [3/25], Train Loss: 0.00015453297237399966, Validation Loss: 0.00015307582822667124\n",
      "Epoch [3/25], Train Loss: 0.00016423095075879246, Validation Loss: 0.0001543879475017699\n",
      "Epoch [3/25], Train Loss: 0.00016792549286037683, Validation Loss: 0.00015204297378659248\n",
      "Epoch [3/25], Train Loss: 0.00012750757741741836, Validation Loss: 0.00015196864308866982\n",
      "Epoch [3/25], Train Loss: 0.00012710200098808855, Validation Loss: 0.0001535256378701888\n",
      "Epoch [3/25], Train Loss: 0.00019150441221427172, Validation Loss: 0.00015254384925356135\n",
      "Epoch [3/25], Train Loss: 0.00017049127200152725, Validation Loss: 0.00015133468938680986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Train Loss: 0.00013204845890868455, Validation Loss: 0.00015221934639460719\n",
      "Epoch [3/25], Train Loss: 0.00014737121819052845, Validation Loss: 0.00015248078101042967\n",
      "Epoch [3/25], Train Loss: 0.00020919636881444603, Validation Loss: 0.0001521977302521312\n",
      "Epoch [3/25], Train Loss: 0.0002027151349466294, Validation Loss: 0.00015127029473660514\n",
      "Epoch [3/25], Train Loss: 0.00014077730884309858, Validation Loss: 0.00015174755947858405\n",
      "Epoch [3/25], Train Loss: 0.0001606617879588157, Validation Loss: 0.00015301563907996752\n",
      "Epoch [3/25], Train Loss: 0.00018044516036752611, Validation Loss: 0.00015278726010971392\n",
      "Epoch [3/25], Train Loss: 0.00017245464550796896, Validation Loss: 0.00015204373048618435\n",
      "Epoch [3/25], Train Loss: 0.00016363860049750656, Validation Loss: 0.00015083376662611652\n",
      "Epoch [3/25], Train Loss: 0.00014875127817504108, Validation Loss: 0.00015181483371028056\n",
      "Epoch [3/25], Train Loss: 0.00012498621072154492, Validation Loss: 0.00015297823459453259\n",
      "Epoch [3/25], Train Loss: 0.00019873165001627058, Validation Loss: 0.000153647236826752\n",
      "Epoch [3/25], Train Loss: 0.00016757512639742345, Validation Loss: 0.00015308339109954734\n",
      "Epoch [3/25], Train Loss: 0.00017613859381526709, Validation Loss: 0.00015148014790611342\n",
      "Epoch [3/25], Train Loss: 0.0001562814723001793, Validation Loss: 0.0001507949949882459\n",
      "Epoch [3/25], Train Loss: 0.00013488656259141862, Validation Loss: 0.0001514173255903491\n",
      "Epoch [3/25], Train Loss: 0.00019097539188805968, Validation Loss: 0.0001517859973925321\n",
      "Epoch [3/25], Train Loss: 0.00015829694166313857, Validation Loss: 0.00015175145672401412\n",
      "Epoch [3/25], Train Loss: 0.00015420978888869286, Validation Loss: 0.0001514681969032002\n",
      "Epoch [3/25], Train Loss: 0.00020817568292841315, Validation Loss: 0.00015094327536644415\n",
      "Epoch [3/25], Train Loss: 0.00017563886649440974, Validation Loss: 0.00015075976407388225\n",
      "Epoch [3/25], Train Loss: 0.000192567880731076, Validation Loss: 0.0001510854713463535\n",
      "Epoch [3/25], Train Loss: 0.00016254893853329122, Validation Loss: 0.00015137278314796275\n",
      "Epoch [3/25], Train Loss: 0.00017171476793009788, Validation Loss: 0.0001513112455237812\n",
      "Epoch [3/25], Train Loss: 0.00020425760885700583, Validation Loss: 0.00015106174663136093\n",
      "Epoch [3/25], Train Loss: 0.00014305002696346492, Validation Loss: 0.0001508507331891451\n",
      "Epoch [3/25], Train Loss: 0.00013936312461737543, Validation Loss: 0.00015080726540569838\n",
      "Epoch [3/25], Train Loss: 0.00012919778237119317, Validation Loss: 0.00015102443115514081\n",
      "Epoch [3/25], Train Loss: 0.00011954683577641845, Validation Loss: 0.00015128065497265196\n",
      "Epoch [3/25], Train Loss: 0.00010721854778239504, Validation Loss: 0.00015116642898647115\n",
      "Epoch [3/25], Train Loss: 0.00017932539049070328, Validation Loss: 0.00015088161914415346\n",
      "Epoch [3/25], Train Loss: 0.00013391775428317487, Validation Loss: 0.0001506569661917941\n",
      "Epoch [3/25], Train Loss: 0.00020052619220223278, Validation Loss: 0.0001506899468949996\n",
      "Epoch [3/25], Train Loss: 0.00015219238412100822, Validation Loss: 0.00015097997723690544\n",
      "Epoch [3/25], Train Loss: 0.0001420521002728492, Validation Loss: 0.00015111085789006515\n",
      "Epoch [3/25], Train Loss: 0.0002165055338991806, Validation Loss: 0.00015091187306097708\n",
      "Epoch [3/25], Train Loss: 0.00018506887136027217, Validation Loss: 0.00015060914908341752\n",
      "Epoch [3/25], Train Loss: 0.00020364321244414896, Validation Loss: 0.00015053636210116868\n",
      "Epoch [3/25], Train Loss: 0.00016335202963091433, Validation Loss: 0.00015055250405566768\n",
      "Epoch [3/25], Train Loss: 0.00018279766663908958, Validation Loss: 0.00015072656290916105\n",
      "Epoch [3/25], Train Loss: 0.0001468787231715396, Validation Loss: 0.00015075555129442363\n",
      "Epoch [3/25], Train Loss: 0.0001624320138944313, Validation Loss: 0.00015059813837676\n",
      "Epoch [3/25], Train Loss: 0.00016771696391515434, Validation Loss: 0.0001506331958808005\n",
      "Epoch [3/25], Train Loss: 0.00014301203191280365, Validation Loss: 0.00015055342106885897\n",
      "Epoch [3/25], Train Loss: 0.0001245141465915367, Validation Loss: 0.0001504974631340398\n",
      "Epoch [3/25], Train Loss: 9.824000881053507e-05, Validation Loss: 0.00015066912286177588\n",
      "Epoch [3/25], Train Loss: 0.00017683937039691955, Validation Loss: 0.00015072289970703424\n",
      "Epoch [3/25], Train Loss: 0.0001411375415045768, Validation Loss: 0.00015063307000673376\n",
      "Epoch [3/25], Train Loss: 0.00021434956579469144, Validation Loss: 0.00015056502694884937\n",
      "Epoch [3/25], Train Loss: 0.0001295742840738967, Validation Loss: 0.00015054593604872933\n",
      "Epoch [3/25], Train Loss: 0.00011651957174763083, Validation Loss: 0.00015041937100856255\n",
      "Epoch [3/25], Train Loss: 0.0001151292963186279, Validation Loss: 0.0001504002975707408\n",
      "Epoch [3/25], Train Loss: 0.000147944301716052, Validation Loss: 0.00015042955880441392\n",
      "Epoch [3/25], Train Loss: 0.00012220386997796595, Validation Loss: 0.00015042232068177933\n",
      "Epoch [3/25], Train Loss: 0.0002204082120442763, Validation Loss: 0.0001504650332208257\n",
      "Epoch [3/25], Train Loss: 0.00013420957839116454, Validation Loss: 0.00015045315012685023\n",
      "Epoch [3/25], Train Loss: 0.00015047763008624315, Validation Loss: 0.00015037591898969064\n",
      "Epoch [3/25], Train Loss: 0.0001874391018645838, Validation Loss: 0.00015037204138934612\n",
      "Epoch [3/25], Train Loss: 0.00016602914547547698, Validation Loss: 0.0001503964211830559\n",
      "Epoch [3/25], Train Loss: 0.000151237501995638, Validation Loss: 0.0001505400417954661\n",
      "Epoch [3/25], Train Loss: 0.00016664000577293336, Validation Loss: 0.00015077811985975131\n",
      "Epoch [3/25], Train Loss: 0.00017768186808098108, Validation Loss: 0.00015098889780347237\n",
      "Epoch [3/25], Train Loss: 0.00018106773495674133, Validation Loss: 0.0001510435145367713\n",
      "Epoch [3/25], Train Loss: 0.00015932820679154247, Validation Loss: 0.00015093260541713485\n",
      "Epoch [3/25], Train Loss: 0.00017425825353711843, Validation Loss: 0.00015080564553500154\n",
      "Epoch [3/25], Train Loss: 0.0001359185262117535, Validation Loss: 0.00015060800239249754\n",
      "Epoch [3/25], Train Loss: 0.00014231899695005268, Validation Loss: 0.00015043340802852376\n",
      "Epoch [3/25], Train Loss: 0.00019753391097765416, Validation Loss: 0.00015033088930067607\n",
      "Epoch [3/25], Train Loss: 0.00014442969404626638, Validation Loss: 0.00015030448460796227\n",
      "Epoch [3/25], Train Loss: 0.00014765601372346282, Validation Loss: 0.00015025428729131818\n",
      "Epoch [3/25], Train Loss: 0.0001437673781765625, Validation Loss: 0.00015023629457573407\n",
      "Epoch [3/25], Train Loss: 0.00020384146773722023, Validation Loss: 0.00015025169874813098\n",
      "Epoch [3/25], Train Loss: 0.0002474201319273561, Validation Loss: 0.00015044855341936152\n",
      "Epoch [3/25], Train Loss: 0.00019815635459963232, Validation Loss: 0.00015047432995440128\n",
      "Epoch [3/25], Train Loss: 0.00014823823585174978, Validation Loss: 0.0001502943322217713\n",
      "Epoch [3/25], Train Loss: 0.00018633654690347612, Validation Loss: 0.00015027469392710677\n",
      "Epoch [3/25], Train Loss: 0.0001638098619878292, Validation Loss: 0.00015034553604588533\n",
      "Epoch [3/25], Train Loss: 0.00016246364975813776, Validation Loss: 0.00015050067474173072\n",
      "Epoch [3/25], Train Loss: 0.0001850428816396743, Validation Loss: 0.00015099422671482898\n",
      "Epoch [3/25], Train Loss: 0.0001095974221243523, Validation Loss: 0.00015183555199958695\n",
      "Epoch [3/25], Train Loss: 0.00017289453535340726, Validation Loss: 0.00015259870876131268\n",
      "Epoch [3/25], Train Loss: 0.00016758887795731425, Validation Loss: 0.0001533383127631775\n",
      "Epoch [3/25], Train Loss: 0.00016440980834886432, Validation Loss: 0.00015438161111281564\n",
      "Epoch [3/25], Train Loss: 0.00015381148841697723, Validation Loss: 0.00015696787644022456\n",
      "Epoch [3/25], Train Loss: 0.00021408630709629506, Validation Loss: 0.00016011369904542032\n",
      "Epoch [3/25], Train Loss: 0.0001662962167756632, Validation Loss: 0.0001640644373158769\n",
      "Epoch [3/25], Train Loss: 0.0001603928249096498, Validation Loss: 0.0001633001331356354\n",
      "Epoch [3/25], Train Loss: 0.0001988799194805324, Validation Loss: 0.00016163728117438343\n",
      "Epoch [3/25], Train Loss: 0.00010055762686533853, Validation Loss: 0.00015346628594367455\n",
      "Epoch [3/25], Train Loss: 0.00011832955351565033, Validation Loss: 0.00015053653090338532\n",
      "Epoch [3/25], Train Loss: 0.00019268238975200802, Validation Loss: 0.00015475271259977793\n",
      "Epoch [3/25], Train Loss: 0.00016919155314099044, Validation Loss: 0.00015685914283191475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Train Loss: 0.00017506717995274812, Validation Loss: 0.0001533097689389251\n",
      "Epoch [3/25], Train Loss: 0.00020754807337652892, Validation Loss: 0.00015066697087604553\n",
      "Epoch [3/25], Train Loss: 0.00018590390391182154, Validation Loss: 0.0001534113078378141\n",
      "Epoch [3/25], Train Loss: 0.0001282615412492305, Validation Loss: 0.00015474802567041479\n",
      "Epoch [3/25], Train Loss: 0.0001310384541284293, Validation Loss: 0.00015159977677588662\n",
      "Epoch [3/25], Train Loss: 0.00014521651610266417, Validation Loss: 0.00015078735620287868\n",
      "Epoch [3/25], Train Loss: 0.00017360871424898505, Validation Loss: 0.00015269646100932733\n",
      "Epoch [3/25], Train Loss: 0.00013398006558418274, Validation Loss: 0.0001522571677924134\n",
      "Epoch [3/25], Train Loss: 0.00017272790137212723, Validation Loss: 0.0001506903094802207\n",
      "Epoch [3/25], Train Loss: 0.00017265275528188795, Validation Loss: 0.0001507350127212703\n",
      "Epoch [3/25], Train Loss: 0.00015102943871170282, Validation Loss: 0.00015193199166484798\n",
      "Epoch [3/25], Train Loss: 0.000145686324685812, Validation Loss: 0.00015171384826923411\n",
      "Epoch [3/25], Train Loss: 0.00017263292102143168, Validation Loss: 0.0001503863502875902\n",
      "Epoch [3/25], Train Loss: 0.0001747820497257635, Validation Loss: 0.0001504865096649155\n",
      "Epoch [3/25], Train Loss: 0.0001678636617725715, Validation Loss: 0.0001517116482621835\n",
      "Epoch [3/25], Train Loss: 0.00021989981178194284, Validation Loss: 0.00015200427903134067\n",
      "Epoch [3/25], Train Loss: 0.00012579213944263756, Validation Loss: 0.00015097979946100774\n",
      "Epoch [3/25], Train Loss: 0.00014758553879801184, Validation Loss: 0.00015010904559555153\n",
      "Epoch [3/25], Train Loss: 0.00017996164388023317, Validation Loss: 0.00015006961305819762\n",
      "Epoch [3/25], Train Loss: 0.00015131919644773006, Validation Loss: 0.0001505209394963458\n",
      "Epoch [3/25], Train Loss: 0.0001333811815129593, Validation Loss: 0.0001508611984415135\n",
      "Epoch [3/25], Train Loss: 0.00016529687854927033, Validation Loss: 0.00015080813876314398\n",
      "Epoch [3/25], Train Loss: 0.00015923606406431645, Validation Loss: 0.0001502711939489624\n",
      "Epoch [3/25], Train Loss: 0.00013446198136080056, Validation Loss: 0.00014995476788802383\n",
      "Epoch [3/25], Train Loss: 0.00015508256910834461, Validation Loss: 0.00015006038762900668\n",
      "Epoch [3/25], Train Loss: 0.00015445842291228473, Validation Loss: 0.00015034235402708874\n",
      "Epoch [3/25], Train Loss: 0.0002022159460466355, Validation Loss: 0.00015060843749476288\n",
      "Epoch [3/25], Train Loss: 9.697669156594202e-05, Validation Loss: 0.0001505740011149707\n",
      "Epoch [3/25], Train Loss: 0.0001756393612595275, Validation Loss: 0.0001506087093730457\n",
      "Epoch [3/25], Train Loss: 0.00018399521650280803, Validation Loss: 0.0001502893615300612\n",
      "Epoch [3/25], Train Loss: 0.00018048113270197064, Validation Loss: 0.00015004469945173088\n",
      "Epoch [3/25], Train Loss: 0.0001405485236318782, Validation Loss: 0.000149886799287439\n",
      "Epoch [3/25], Train Loss: 0.00016985383990686387, Validation Loss: 0.0001499007232875253\n",
      "Epoch [3/25], Train Loss: 0.00011759142216760665, Validation Loss: 0.00015009750495664775\n",
      "Epoch [3/25], Train Loss: 0.00015312177129089832, Validation Loss: 0.00015026027637456233\n",
      "Epoch [3/25], Train Loss: 0.00016401580069214106, Validation Loss: 0.00015034956571374398\n",
      "Epoch [3/25], Train Loss: 0.00012063216854585335, Validation Loss: 0.0001502266648458317\n",
      "Epoch [3/25], Train Loss: 0.00012459604477044195, Validation Loss: 0.00014995051727358563\n",
      "Epoch [3/25], Train Loss: 0.00018692415324039757, Validation Loss: 0.00014979365053780687\n",
      "Epoch [3/25], Train Loss: 0.00014510717301163822, Validation Loss: 0.00014982016364228912\n",
      "Epoch [3/25], Train Loss: 0.00010388533701188862, Validation Loss: 0.00014998798821276674\n",
      "Epoch [3/25], Train Loss: 0.00021518471476156265, Validation Loss: 0.0001501818839945675\n",
      "Epoch [3/25], Train Loss: 0.00020247334032319486, Validation Loss: 0.0001501408163070058\n",
      "Epoch [3/25], Train Loss: 0.00020217742712702602, Validation Loss: 0.00015037841803859918\n",
      "Epoch [3/25], Train Loss: 0.00014379435742739588, Validation Loss: 0.00015067435791327928\n",
      "Epoch [3/25], Train Loss: 0.00016399250307586044, Validation Loss: 0.00015015711663484883\n",
      "Epoch [3/25], Train Loss: 0.00018041933071799576, Validation Loss: 0.00015001926585682667\n",
      "Epoch [3/25], Train Loss: 0.00016513961600139737, Validation Loss: 0.00015052391318022275\n",
      "Epoch [3/25], Train Loss: 0.00011076481314375997, Validation Loss: 0.00015052156450110488\n",
      "Epoch [3/25], Train Loss: 0.00013993479660712183, Validation Loss: 0.00015032640367280693\n",
      "Epoch [3/25], Train Loss: 0.00013269641203805804, Validation Loss: 0.00015038614874356425\n",
      "Epoch [3/25], Train Loss: 0.00014155151438899338, Validation Loss: 0.00015022321579939065\n",
      "Epoch [3/25], Train Loss: 0.00018394080689176917, Validation Loss: 0.000149923660743904\n",
      "Epoch [3/25], Train Loss: 0.00019239897665102035, Validation Loss: 0.00015000089091093589\n",
      "Epoch [3/25], Train Loss: 0.00020806427346542478, Validation Loss: 0.00015005073388844417\n",
      "Epoch [3/25], Train Loss: 0.00018957628344651312, Validation Loss: 0.0001498925744575293\n",
      "Epoch [3/25], Train Loss: 0.00011981081479461864, Validation Loss: 0.0001499737799652697\n",
      "Epoch [3/25], Train Loss: 0.00011188472853973508, Validation Loss: 0.00015009348038195943\n",
      "Epoch [3/25], Train Loss: 0.00012121490726713091, Validation Loss: 0.00015006002310353022\n",
      "Epoch [3/25], Train Loss: 0.00016562636301387101, Validation Loss: 0.00014996855024946855\n",
      "Epoch [3/25], Train Loss: 9.979369497159496e-05, Validation Loss: 0.00014993843506090344\n",
      "Epoch [3/25], Train Loss: 0.00016400041931774467, Validation Loss: 0.00014982462283417893\n",
      "Epoch [3/25], Train Loss: 0.00019428676750976592, Validation Loss: 0.00014975530915156316\n",
      "Epoch [3/25], Train Loss: 0.00017962777928914875, Validation Loss: 0.0001497779742445952\n",
      "Epoch [3/25], Train Loss: 0.000128499828861095, Validation Loss: 0.000149703901358104\n",
      "Epoch [3/25], Train Loss: 0.00013666656741406769, Validation Loss: 0.00014964681079921623\n",
      "Epoch [3/25], Train Loss: 0.00014726820518262684, Validation Loss: 0.00014968307223170996\n",
      "Epoch [3/25], Train Loss: 0.0002916690136771649, Validation Loss: 0.00014976262997758264\n",
      "Epoch [4/25], Train Loss: 0.00017904641572386026, Validation Loss: 0.00014976173115428538\n",
      "Epoch [4/25], Train Loss: 0.00016437559679616243, Validation Loss: 0.00014971169124085767\n",
      "Epoch [4/25], Train Loss: 0.00010510699212318286, Validation Loss: 0.00014974962735626225\n",
      "Epoch [4/25], Train Loss: 0.00014342331269290298, Validation Loss: 0.00014979586267145352\n",
      "Epoch [4/25], Train Loss: 0.0001530065928818658, Validation Loss: 0.0001496814203467996\n",
      "Epoch [4/25], Train Loss: 0.00019170044106431305, Validation Loss: 0.00014962784553063102\n",
      "Epoch [4/25], Train Loss: 0.0001803902123356238, Validation Loss: 0.00014972108134922262\n",
      "Epoch [4/25], Train Loss: 0.0001910283463075757, Validation Loss: 0.0001514381161541678\n",
      "Epoch [4/25], Train Loss: 0.00018985105270985514, Validation Loss: 0.00016409546418193107\n",
      "Epoch [4/25], Train Loss: 0.00016922237409744412, Validation Loss: 0.00020096640315993378\n",
      "Epoch [4/25], Train Loss: 0.00022812098904978484, Validation Loss: 0.0002660929613436262\n",
      "Epoch [4/25], Train Loss: 0.0002983668236993253, Validation Loss: 0.00023087470423585426\n",
      "Epoch [4/25], Train Loss: 0.00019679484830703586, Validation Loss: 0.0001597444500172666\n",
      "Epoch [4/25], Train Loss: 0.00014058138185646385, Validation Loss: 0.00017836949070139477\n",
      "Epoch [4/25], Train Loss: 0.00018655444728210568, Validation Loss: 0.0001739298808388412\n",
      "Epoch [4/25], Train Loss: 0.0002477576199453324, Validation Loss: 0.00015950014179300827\n",
      "Epoch [4/25], Train Loss: 0.00013623030099552125, Validation Loss: 0.00017392695517628453\n",
      "Epoch [4/25], Train Loss: 0.0002022422559093684, Validation Loss: 0.0001594207014325851\n",
      "Epoch [4/25], Train Loss: 0.00024515343829989433, Validation Loss: 0.00017097986443938377\n",
      "Epoch [4/25], Train Loss: 0.0002362721279496327, Validation Loss: 0.00015871188006713055\n",
      "Epoch [4/25], Train Loss: 0.00018301438831258565, Validation Loss: 0.0001659768822719343\n",
      "Epoch [4/25], Train Loss: 0.00020213323296047747, Validation Loss: 0.00015972233425903443\n",
      "Epoch [4/25], Train Loss: 0.00012294382031541318, Validation Loss: 0.00017180716119279775\n",
      "Epoch [4/25], Train Loss: 0.00015956694551277906, Validation Loss: 0.00015772884338124035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Train Loss: 0.00016232002235483378, Validation Loss: 0.00016470909661923846\n",
      "Epoch [4/25], Train Loss: 0.00014480046229436994, Validation Loss: 0.0001577498124485525\n",
      "Epoch [4/25], Train Loss: 0.00017421712982468307, Validation Loss: 0.00016417986698797905\n",
      "Epoch [4/25], Train Loss: 0.00019673138740472496, Validation Loss: 0.00015760929769991587\n",
      "Epoch [4/25], Train Loss: 0.00023489708837587386, Validation Loss: 0.00016247600578935816\n",
      "Epoch [4/25], Train Loss: 0.0001676773972576484, Validation Loss: 0.00015686419186143515\n",
      "Epoch [4/25], Train Loss: 0.00020741623302455992, Validation Loss: 0.00016082993946232212\n",
      "Epoch [4/25], Train Loss: 0.00020094790670555085, Validation Loss: 0.00015546255963272415\n",
      "Epoch [4/25], Train Loss: 0.0001422847853973508, Validation Loss: 0.0001590412832835379\n",
      "Epoch [4/25], Train Loss: 0.00017350484267808497, Validation Loss: 0.000154398090429216\n",
      "Epoch [4/25], Train Loss: 0.00017875472258310765, Validation Loss: 0.00015707745236189416\n",
      "Epoch [4/25], Train Loss: 0.00021173938876017928, Validation Loss: 0.0001542783795836537\n",
      "Epoch [4/25], Train Loss: 0.00021505910262931138, Validation Loss: 0.0001531079142296221\n",
      "Epoch [4/25], Train Loss: 0.00016285716264974326, Validation Loss: 0.00015545319038210437\n",
      "Epoch [4/25], Train Loss: 0.0002484879514668137, Validation Loss: 0.00015207925389404409\n",
      "Epoch [4/25], Train Loss: 0.0001256321120308712, Validation Loss: 0.0001511599269482152\n",
      "Epoch [4/25], Train Loss: 0.00011301579070277512, Validation Loss: 0.00015450152544265922\n",
      "Epoch [4/25], Train Loss: 0.00012324689305387437, Validation Loss: 0.00015697437023239522\n",
      "Epoch [4/25], Train Loss: 0.0001465262903366238, Validation Loss: 0.00015416188519642068\n",
      "Epoch [4/25], Train Loss: 0.00014473233022727072, Validation Loss: 0.00015140870042766133\n",
      "Epoch [4/25], Train Loss: 0.00019800661539193243, Validation Loss: 0.0001502380895544775\n",
      "Epoch [4/25], Train Loss: 0.00018565864593256265, Validation Loss: 0.00015122197971019583\n",
      "Epoch [4/25], Train Loss: 0.00014999767881818116, Validation Loss: 0.0001534420674336919\n",
      "Epoch [4/25], Train Loss: 0.00018042341980617493, Validation Loss: 0.0001557438163824069\n",
      "Epoch [4/25], Train Loss: 0.00021154920978005975, Validation Loss: 0.00016077001152249674\n",
      "Epoch [4/25], Train Loss: 0.00018937404092866927, Validation Loss: 0.0001613621102781811\n",
      "Epoch [4/25], Train Loss: 0.00021240586647763848, Validation Loss: 0.0001605191816148969\n",
      "Epoch [4/25], Train Loss: 0.00017710801330395043, Validation Loss: 0.00015325692923700746\n",
      "Epoch [4/25], Train Loss: 0.00017453980399295688, Validation Loss: 0.00015003457277392349\n",
      "Epoch [4/25], Train Loss: 0.00012441207945812494, Validation Loss: 0.00015260513440201368\n",
      "Epoch [4/25], Train Loss: 0.00019266977324150503, Validation Loss: 0.0001528828045896565\n",
      "Epoch [4/25], Train Loss: 0.00010258747352054343, Validation Loss: 0.00015030011297009576\n",
      "Epoch [4/25], Train Loss: 0.00016736016550567, Validation Loss: 0.00015085639291404125\n",
      "Epoch [4/25], Train Loss: 0.0001647191820666194, Validation Loss: 0.000152220148932732\n",
      "Epoch [4/25], Train Loss: 0.00016664128634147346, Validation Loss: 0.0001504467358851495\n",
      "Epoch [4/25], Train Loss: 0.00016859782044775784, Validation Loss: 0.00015059660484742683\n",
      "Epoch [4/25], Train Loss: 0.00017218371795024723, Validation Loss: 0.00015143995163574193\n",
      "Epoch [4/25], Train Loss: 0.00012947051436640322, Validation Loss: 0.00015026598169545953\n",
      "Epoch [4/25], Train Loss: 0.00016819503798615187, Validation Loss: 0.00015056572180280152\n",
      "Epoch [4/25], Train Loss: 0.00018338396330364048, Validation Loss: 0.00015121251805491436\n",
      "Epoch [4/25], Train Loss: 0.0001531277666799724, Validation Loss: 0.00015015742731823897\n",
      "Epoch [4/25], Train Loss: 0.00018725551490206271, Validation Loss: 0.00015049095260716666\n",
      "Epoch [4/25], Train Loss: 0.0001869799743872136, Validation Loss: 0.00015082574706563415\n",
      "Epoch [4/25], Train Loss: 0.00019559977226890624, Validation Loss: 0.00015002930182769585\n",
      "Epoch [4/25], Train Loss: 0.0001454748271498829, Validation Loss: 0.00015015364454787534\n",
      "Epoch [4/25], Train Loss: 0.0002238138986285776, Validation Loss: 0.0001502666673331987\n",
      "Epoch [4/25], Train Loss: 0.00014870986342430115, Validation Loss: 0.0001498605590313673\n",
      "Epoch [4/25], Train Loss: 0.0001786000793799758, Validation Loss: 0.00015003869072340118\n",
      "Epoch [4/25], Train Loss: 0.0001737493003020063, Validation Loss: 0.0001500303498081242\n",
      "Epoch [4/25], Train Loss: 0.0002161372103728354, Validation Loss: 0.0001499033821649694\n",
      "Epoch [4/25], Train Loss: 0.00022182133398018777, Validation Loss: 0.00014984499236258368\n",
      "Epoch [4/25], Train Loss: 0.00021171878324821591, Validation Loss: 0.0001498966662135596\n",
      "Epoch [4/25], Train Loss: 0.00016179848171304911, Validation Loss: 0.00014970485887412602\n",
      "Epoch [4/25], Train Loss: 0.00013142943498678505, Validation Loss: 0.00014958770261728205\n",
      "Epoch [4/25], Train Loss: 0.0001802047190722078, Validation Loss: 0.0001496304738490532\n",
      "Epoch [4/25], Train Loss: 0.0001365752686979249, Validation Loss: 0.00014958447184956942\n",
      "Epoch [4/25], Train Loss: 0.00012936715211253613, Validation Loss: 0.0001495046184572857\n",
      "Epoch [4/25], Train Loss: 0.000238250577240251, Validation Loss: 0.0001494694615151578\n",
      "Epoch [4/25], Train Loss: 0.00017252343241125345, Validation Loss: 0.00014944795620976947\n",
      "Epoch [4/25], Train Loss: 0.00013167908764444292, Validation Loss: 0.00014943061129694493\n",
      "Epoch [4/25], Train Loss: 0.0001664724259171635, Validation Loss: 0.00014941306920566907\n",
      "Epoch [4/25], Train Loss: 0.00015861312567722052, Validation Loss: 0.00014939896839981278\n",
      "Epoch [4/25], Train Loss: 0.00012225244427099824, Validation Loss: 0.00014938310341676696\n",
      "Epoch [4/25], Train Loss: 0.00013048513210378587, Validation Loss: 0.0001493589283199981\n",
      "Epoch [4/25], Train Loss: 0.00017655880947131664, Validation Loss: 0.00014938613482324097\n",
      "Epoch [4/25], Train Loss: 0.0001801375183276832, Validation Loss: 0.00014942032066755928\n",
      "Epoch [4/25], Train Loss: 0.000124292477266863, Validation Loss: 0.00014936118798990113\n",
      "Epoch [4/25], Train Loss: 0.00021033422672189772, Validation Loss: 0.00014933719163915762\n",
      "Epoch [4/25], Train Loss: 0.00016283561126329005, Validation Loss: 0.00014941503516941642\n",
      "Epoch [4/25], Train Loss: 0.00011911698675248772, Validation Loss: 0.00014942344423616306\n",
      "Epoch [4/25], Train Loss: 0.000183595169801265, Validation Loss: 0.0001493421312867819\n",
      "Epoch [4/25], Train Loss: 0.0002142829616786912, Validation Loss: 0.0001492989363517457\n",
      "Epoch [4/25], Train Loss: 0.00014531443594023585, Validation Loss: 0.00014927258986669283\n",
      "Epoch [4/25], Train Loss: 0.00018601806368678808, Validation Loss: 0.00014925885528403645\n",
      "Epoch [4/25], Train Loss: 0.0001202687926706858, Validation Loss: 0.0001492595052695833\n",
      "Epoch [4/25], Train Loss: 0.00015138660091906786, Validation Loss: 0.0001493065821705386\n",
      "Epoch [4/25], Train Loss: 0.0001494304306106642, Validation Loss: 0.0001493848498891263\n",
      "Epoch [4/25], Train Loss: 0.00020503840642049909, Validation Loss: 0.0001493451971327886\n",
      "Epoch [4/25], Train Loss: 0.00014371791621670127, Validation Loss: 0.00014940041898322913\n",
      "Epoch [4/25], Train Loss: 0.00014725411892868578, Validation Loss: 0.00014947678452396454\n",
      "Epoch [4/25], Train Loss: 0.00017290079267695546, Validation Loss: 0.00014943170244805515\n",
      "Epoch [4/25], Train Loss: 0.0001400142937200144, Validation Loss: 0.0001493485365548016\n",
      "Epoch [4/25], Train Loss: 0.00014814241148997098, Validation Loss: 0.00014932401342472682\n",
      "Epoch [4/25], Train Loss: 0.00011467983858892694, Validation Loss: 0.00014927583251846954\n",
      "Epoch [4/25], Train Loss: 0.0001559713709866628, Validation Loss: 0.00014918237138772383\n",
      "Epoch [4/25], Train Loss: 0.00017484769341535866, Validation Loss: 0.0001491170218893482\n",
      "Epoch [4/25], Train Loss: 0.0001824734645197168, Validation Loss: 0.00014916231569562418\n",
      "Epoch [4/25], Train Loss: 0.00015383592108264565, Validation Loss: 0.0001491689785325434\n",
      "Epoch [4/25], Train Loss: 0.0001288212661165744, Validation Loss: 0.0001491311772648866\n",
      "Epoch [4/25], Train Loss: 0.00010624151764204726, Validation Loss: 0.00014914807834429667\n",
      "Epoch [4/25], Train Loss: 0.00016271906497422606, Validation Loss: 0.00014917868829797954\n",
      "Epoch [4/25], Train Loss: 0.00014425987319555134, Validation Loss: 0.00014916507595141108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Train Loss: 0.0001294725079787895, Validation Loss: 0.00014913282672447773\n",
      "Epoch [4/25], Train Loss: 0.000101738769444637, Validation Loss: 0.00014911035250406713\n",
      "Epoch [4/25], Train Loss: 0.00014094181824475527, Validation Loss: 0.00014908475228973353\n",
      "Epoch [4/25], Train Loss: 0.00010703778389142826, Validation Loss: 0.0001490763597151575\n",
      "Epoch [4/25], Train Loss: 0.00016070055426098406, Validation Loss: 0.00014906311795736353\n",
      "Epoch [4/25], Train Loss: 0.00015511229867115617, Validation Loss: 0.000149030268463927\n",
      "Epoch [4/25], Train Loss: 0.00013274919183459133, Validation Loss: 0.00014901733617686357\n",
      "Epoch [4/25], Train Loss: 0.00012948007497470826, Validation Loss: 0.00014902309558237903\n",
      "Epoch [4/25], Train Loss: 0.00010930460848612711, Validation Loss: 0.00014901579828195585\n",
      "Epoch [4/25], Train Loss: 0.00013867588131688535, Validation Loss: 0.0001489778553756575\n",
      "Epoch [4/25], Train Loss: 0.00011578185512917116, Validation Loss: 0.0001489700475455417\n",
      "Epoch [4/25], Train Loss: 0.00016505851817782968, Validation Loss: 0.00014902195301450167\n",
      "Epoch [4/25], Train Loss: 0.00023654302640352398, Validation Loss: 0.00014905308513940934\n",
      "Epoch [4/25], Train Loss: 0.00013350605149753392, Validation Loss: 0.00014899551218453173\n",
      "Epoch [4/25], Train Loss: 0.00013920846686232835, Validation Loss: 0.0001489508586625258\n",
      "Epoch [4/25], Train Loss: 0.00016246209270320833, Validation Loss: 0.000148939156497363\n",
      "Epoch [4/25], Train Loss: 0.00015860954590607435, Validation Loss: 0.00014896885283330146\n",
      "Epoch [4/25], Train Loss: 0.0001452262804377824, Validation Loss: 0.00014898175092336411\n",
      "Epoch [4/25], Train Loss: 0.00013317118282429874, Validation Loss: 0.00014894147752784193\n",
      "Epoch [4/25], Train Loss: 0.00020289799431338906, Validation Loss: 0.00014891318302640382\n",
      "Epoch [4/25], Train Loss: 0.00015076147974468768, Validation Loss: 0.00014890361029150275\n",
      "Epoch [4/25], Train Loss: 0.00012886914191767573, Validation Loss: 0.0001488964934348284\n",
      "Epoch [4/25], Train Loss: 0.0001598707603989169, Validation Loss: 0.00014888488076394423\n",
      "Epoch [4/25], Train Loss: 0.00020750676048919559, Validation Loss: 0.00014888640240921328\n",
      "Epoch [4/25], Train Loss: 0.00016052494174800813, Validation Loss: 0.00014889904802354673\n",
      "Epoch [4/25], Train Loss: 0.00017686338105704635, Validation Loss: 0.00014891197327718448\n",
      "Epoch [4/25], Train Loss: 0.00016746549226809293, Validation Loss: 0.00014888398169811504\n",
      "Epoch [4/25], Train Loss: 0.00019847923249471933, Validation Loss: 0.00014883256905401748\n",
      "Epoch [4/25], Train Loss: 0.00010656765516614541, Validation Loss: 0.00014885547401111883\n",
      "Epoch [4/25], Train Loss: 0.00015242751396726817, Validation Loss: 0.0001488876524187314\n",
      "Epoch [4/25], Train Loss: 0.0001961880043381825, Validation Loss: 0.00014886388283533354\n",
      "Epoch [4/25], Train Loss: 0.00012840359704568982, Validation Loss: 0.00014885573352027375\n",
      "Epoch [4/25], Train Loss: 0.00011424034892115742, Validation Loss: 0.00014884301684408758\n",
      "Epoch [4/25], Train Loss: 0.00015305637498386204, Validation Loss: 0.00014888213700032794\n",
      "Epoch [4/25], Train Loss: 0.00019149828585796058, Validation Loss: 0.00014895421045366675\n",
      "Epoch [4/25], Train Loss: 0.0001611615443835035, Validation Loss: 0.0001489476232867067\n",
      "Epoch [4/25], Train Loss: 0.0001618791138753295, Validation Loss: 0.00014885817048101066\n",
      "Epoch [4/25], Train Loss: 0.00011879561498062685, Validation Loss: 0.00014883819627963628\n",
      "Epoch [4/25], Train Loss: 0.00014623069728258997, Validation Loss: 0.00014889491697734532\n",
      "Epoch [4/25], Train Loss: 0.0001686320611042902, Validation Loss: 0.00014891419535463987\n",
      "Epoch [4/25], Train Loss: 0.00021817510423716158, Validation Loss: 0.0001488830156934758\n",
      "Epoch [4/25], Train Loss: 0.00014489662135019898, Validation Loss: 0.00014881895937530015\n",
      "Epoch [4/25], Train Loss: 0.0001155437930719927, Validation Loss: 0.00014878257497912272\n",
      "Epoch [4/25], Train Loss: 0.00018301294767297804, Validation Loss: 0.0001488536409548639\n",
      "Epoch [4/25], Train Loss: 0.00019886648806277663, Validation Loss: 0.00014888841663681282\n",
      "Epoch [4/25], Train Loss: 0.00017368196859024465, Validation Loss: 0.000148799655289622\n",
      "Epoch [4/25], Train Loss: 0.00010744858445832506, Validation Loss: 0.00014873653247680826\n",
      "Epoch [4/25], Train Loss: 0.00015736394561827183, Validation Loss: 0.00014873556841242438\n",
      "Epoch [4/25], Train Loss: 0.0001511684968136251, Validation Loss: 0.00014874001305239897\n",
      "Epoch [4/25], Train Loss: 0.00010189072054345161, Validation Loss: 0.00014872364336042664\n",
      "Epoch [4/25], Train Loss: 0.0001602097909199074, Validation Loss: 0.00014868230840268856\n",
      "Epoch [4/25], Train Loss: 0.0001547060819575563, Validation Loss: 0.00014869204023852943\n",
      "Epoch [4/25], Train Loss: 0.00010782843310153112, Validation Loss: 0.00014868076614220626\n",
      "Epoch [4/25], Train Loss: 0.00018901356088463217, Validation Loss: 0.0001486426976043731\n",
      "Epoch [4/25], Train Loss: 0.00017558314721100032, Validation Loss: 0.00014864388528318767\n",
      "Epoch [4/25], Train Loss: 0.00015324715059250593, Validation Loss: 0.0001487117677849407\n",
      "Epoch [4/25], Train Loss: 0.00014467125583905727, Validation Loss: 0.00014874134625036579\n",
      "Epoch [4/25], Train Loss: 0.00020680639136116952, Validation Loss: 0.00014871471576043404\n",
      "Epoch [4/25], Train Loss: 0.0001683808513917029, Validation Loss: 0.00014869405907423546\n",
      "Epoch [4/25], Train Loss: 0.0001470382121624425, Validation Loss: 0.00014866024309109587\n",
      "Epoch [4/25], Train Loss: 9.17388970265165e-05, Validation Loss: 0.00014862505607500984\n",
      "Epoch [4/25], Train Loss: 0.0001880980998976156, Validation Loss: 0.0001485878681705799\n",
      "Epoch [4/25], Train Loss: 0.00014052219921723008, Validation Loss: 0.00014857033723577237\n",
      "Epoch [4/25], Train Loss: 0.000137796436320059, Validation Loss: 0.00014859453804092482\n",
      "Epoch [4/25], Train Loss: 0.0001319842122029513, Validation Loss: 0.0001486029002990108\n",
      "Epoch [4/25], Train Loss: 0.00013933476293459535, Validation Loss: 0.0001485830442106817\n",
      "Epoch [4/25], Train Loss: 0.00014878393267281353, Validation Loss: 0.00014854951223242097\n",
      "Epoch [4/25], Train Loss: 0.00018929630459751934, Validation Loss: 0.00014853747949625056\n",
      "Epoch [4/25], Train Loss: 0.00015810670447535813, Validation Loss: 0.00014853373262061116\n",
      "Epoch [4/25], Train Loss: 0.00013147338177077472, Validation Loss: 0.00014853768176787224\n",
      "Epoch [4/25], Train Loss: 0.00013207376468926668, Validation Loss: 0.0001485273300204426\n",
      "Epoch [4/25], Train Loss: 0.00022306336904875934, Validation Loss: 0.0001485649309567331\n",
      "Epoch [4/25], Train Loss: 0.0001304296456510201, Validation Loss: 0.00014860025888386493\n",
      "Epoch [4/25], Train Loss: 0.00020102632697671652, Validation Loss: 0.0001485603900315861\n",
      "Epoch [4/25], Train Loss: 0.00011209370859432966, Validation Loss: 0.00014850963440646108\n",
      "Epoch [4/25], Train Loss: 0.00013327988563105464, Validation Loss: 0.00014852322686541205\n",
      "Epoch [4/25], Train Loss: 0.0001718378916848451, Validation Loss: 0.00014850046621480337\n",
      "Epoch [4/25], Train Loss: 0.00020889710867777467, Validation Loss: 0.00014847113294914985\n",
      "Epoch [4/25], Train Loss: 0.00017954851500689983, Validation Loss: 0.0001485224051672655\n",
      "Epoch [4/25], Train Loss: 0.00021727749845013022, Validation Loss: 0.00014850395746179856\n",
      "Epoch [4/25], Train Loss: 0.00025450828252360225, Validation Loss: 0.00014845465654313253\n",
      "Epoch [4/25], Train Loss: 0.00013847608352079988, Validation Loss: 0.00014850459362302597\n",
      "Epoch [4/25], Train Loss: 0.000180225440999493, Validation Loss: 0.00014854878948729797\n",
      "Epoch [4/25], Train Loss: 0.00015343861014116555, Validation Loss: 0.00014847767597530036\n",
      "Epoch [4/25], Train Loss: 0.0001422646892024204, Validation Loss: 0.00014848584323772228\n",
      "Epoch [4/25], Train Loss: 0.00010269786434946582, Validation Loss: 0.00014853342533266794\n",
      "Epoch [4/25], Train Loss: 9.33585106395185e-05, Validation Loss: 0.00014853904892030793\n",
      "Epoch [4/25], Train Loss: 0.00015772109327372164, Validation Loss: 0.00014849452224249642\n",
      "Epoch [4/25], Train Loss: 0.00018491543596610427, Validation Loss: 0.000148488829775791\n",
      "Epoch [4/25], Train Loss: 0.00012371802586130798, Validation Loss: 0.0001485032174969092\n",
      "Epoch [4/25], Train Loss: 0.00019446290389169008, Validation Loss: 0.0001485650859346303\n",
      "Epoch [4/25], Train Loss: 0.0001615267392480746, Validation Loss: 0.0001486983043529714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Train Loss: 0.0001519036595709622, Validation Loss: 0.00014875316483085044\n",
      "Epoch [4/25], Train Loss: 0.00013550478615798056, Validation Loss: 0.00014867164548680497\n",
      "Epoch [4/25], Train Loss: 0.0002086464228341356, Validation Loss: 0.0001485173100566802\n",
      "Epoch [4/25], Train Loss: 0.00014926749281585217, Validation Loss: 0.0001483901030345199\n",
      "Epoch [4/25], Train Loss: 0.00011650264787022024, Validation Loss: 0.00014841120791970753\n",
      "Epoch [4/25], Train Loss: 0.00015987054212018847, Validation Loss: 0.00014851739688310773\n",
      "Epoch [4/25], Train Loss: 0.00014232125249691308, Validation Loss: 0.00014860292891777743\n",
      "Epoch [4/25], Train Loss: 0.0001503758248873055, Validation Loss: 0.00014858555659884586\n",
      "Epoch [4/25], Train Loss: 0.00010544514225330204, Validation Loss: 0.00014844723967447255\n",
      "Epoch [4/25], Train Loss: 0.00018952006939798594, Validation Loss: 0.00014835874705264966\n",
      "Epoch [4/25], Train Loss: 0.00011272745905444026, Validation Loss: 0.00014830721702310256\n",
      "Epoch [4/25], Train Loss: 0.00018497297423891723, Validation Loss: 0.00014830423412301268\n",
      "Epoch [4/25], Train Loss: 0.00013785695773549378, Validation Loss: 0.0001482832968273821\n",
      "Epoch [4/25], Train Loss: 0.0001230940397363156, Validation Loss: 0.00014831636750993008\n",
      "Epoch [4/25], Train Loss: 0.00013651746849063784, Validation Loss: 0.00014836593084813405\n",
      "Epoch [4/25], Train Loss: 0.0001284660684177652, Validation Loss: 0.00014835044736779917\n",
      "Epoch [4/25], Train Loss: 0.0001741813903208822, Validation Loss: 0.00014833788760976557\n",
      "Epoch [4/25], Train Loss: 0.00015421610441990197, Validation Loss: 0.0001483574099741721\n",
      "Epoch [4/25], Train Loss: 0.00016804030747152865, Validation Loss: 0.00014831459557171912\n",
      "Epoch [4/25], Train Loss: 0.00020372599828988314, Validation Loss: 0.00014827348980664585\n",
      "Epoch [4/25], Train Loss: 0.0001844223588705063, Validation Loss: 0.00014829071321097824\n",
      "Epoch [4/25], Train Loss: 0.00017902928811963648, Validation Loss: 0.0001483293017372489\n",
      "Epoch [4/25], Train Loss: 0.00013059558114036918, Validation Loss: 0.00014831770822638647\n",
      "Epoch [4/25], Train Loss: 0.00017765138181857765, Validation Loss: 0.0001483188910545626\n",
      "Epoch [4/25], Train Loss: 0.00013330696674529463, Validation Loss: 0.00014836692231862497\n",
      "Epoch [4/25], Train Loss: 0.0001194441138068214, Validation Loss: 0.00014832055045796249\n",
      "Epoch [4/25], Train Loss: 0.00015892471128609031, Validation Loss: 0.00014832904222809398\n",
      "Epoch [4/25], Train Loss: 0.0001671984209679067, Validation Loss: 0.00014835771410920035\n",
      "Epoch [4/25], Train Loss: 0.0001555197377456352, Validation Loss: 0.00014837936129576216\n",
      "Epoch [4/25], Train Loss: 0.00015312018513213843, Validation Loss: 0.00014833451374821985\n",
      "Epoch [4/25], Train Loss: 0.00014102178101893514, Validation Loss: 0.00014834021203569137\n",
      "Epoch [4/25], Train Loss: 0.00021077960263937712, Validation Loss: 0.00014838947051127132\n",
      "Epoch [4/25], Train Loss: 0.0001840197655837983, Validation Loss: 0.0001483526502852328\n",
      "Epoch [4/25], Train Loss: 0.0001509437570348382, Validation Loss: 0.00014829162537353114\n",
      "Epoch [4/25], Train Loss: 0.00017902151739690453, Validation Loss: 0.00014822799809432278\n",
      "Epoch [4/25], Train Loss: 0.00020347924146335572, Validation Loss: 0.00014832462693448178\n",
      "Epoch [4/25], Train Loss: 0.00012148188397986814, Validation Loss: 0.00014837875545102482\n",
      "Epoch [4/25], Train Loss: 0.00014743134670425206, Validation Loss: 0.00014833006377254303\n",
      "Epoch [4/25], Train Loss: 0.0001648252655286342, Validation Loss: 0.00014831367807346396\n",
      "Epoch [4/25], Train Loss: 0.00015760173846501857, Validation Loss: 0.00014835351806444426\n",
      "Epoch [4/25], Train Loss: 0.00019141726079396904, Validation Loss: 0.00014842654733608165\n",
      "Epoch [4/25], Train Loss: 0.0001394981809426099, Validation Loss: 0.00014844320976408197\n",
      "Epoch [4/25], Train Loss: 0.00019703897123690695, Validation Loss: 0.00014850696073456977\n",
      "Epoch [4/25], Train Loss: 0.0001040928837028332, Validation Loss: 0.00014851393013183648\n",
      "Epoch [4/25], Train Loss: 9.500583109911531e-05, Validation Loss: 0.00014853352355809573\n",
      "Epoch [4/25], Train Loss: 0.00019461842020973563, Validation Loss: 0.00014870172817609272\n",
      "Epoch [4/25], Train Loss: 0.00011840255319839343, Validation Loss: 0.00014907961837404098\n",
      "Epoch [4/25], Train Loss: 0.00014926143921911716, Validation Loss: 0.00014972584807158759\n",
      "Epoch [4/25], Train Loss: 0.00022449203243013471, Validation Loss: 0.00014975577796576544\n",
      "Epoch [4/25], Train Loss: 0.0001500267389928922, Validation Loss: 0.00015016316368322198\n",
      "Epoch [4/25], Train Loss: 0.00010390142415417358, Validation Loss: 0.00015014265009085647\n",
      "Epoch [4/25], Train Loss: 0.0001191019982798025, Validation Loss: 0.0001500606898237796\n",
      "Epoch [4/25], Train Loss: 0.00011735866428352892, Validation Loss: 0.000149611654827216\n",
      "Epoch [4/25], Train Loss: 0.00018501638260204345, Validation Loss: 0.00014905347755605664\n",
      "Epoch [4/25], Train Loss: 0.00010482146171852946, Validation Loss: 0.00014854759526012156\n",
      "Epoch [4/25], Train Loss: 0.00020568612671922892, Validation Loss: 0.00014817254438336628\n",
      "Epoch [4/25], Train Loss: 0.0001536397321615368, Validation Loss: 0.00014815327061417822\n",
      "Epoch [4/25], Train Loss: 0.00017865221889223903, Validation Loss: 0.00014845326271218558\n",
      "Epoch [4/25], Train Loss: 0.0001210700356750749, Validation Loss: 0.00014883110076577093\n",
      "Epoch [4/25], Train Loss: 0.00013520868378691375, Validation Loss: 0.00014895229590668654\n",
      "Epoch [5/25], Train Loss: 0.00019138786592520773, Validation Loss: 0.00014907431104802527\n",
      "Epoch [5/25], Train Loss: 0.00012997312296647578, Validation Loss: 0.00014898398197450054\n",
      "Epoch [5/25], Train Loss: 0.00010881359048653394, Validation Loss: 0.00014870441130672893\n",
      "Epoch [5/25], Train Loss: 0.0002096867247018963, Validation Loss: 0.00014837774894355487\n",
      "Epoch [5/25], Train Loss: 0.0001518573990324512, Validation Loss: 0.00014814934038440697\n",
      "Epoch [5/25], Train Loss: 0.00015847408212721348, Validation Loss: 0.00014804668026044965\n",
      "Epoch [5/25], Train Loss: 0.00011196458217455074, Validation Loss: 0.00014807945408392697\n",
      "Epoch [5/25], Train Loss: 0.00023854247410781682, Validation Loss: 0.00014848264375662742\n",
      "Epoch [5/25], Train Loss: 0.00019160211377311498, Validation Loss: 0.00014900401899164232\n",
      "Epoch [5/25], Train Loss: 0.00014692415425088257, Validation Loss: 0.00014947523595765233\n",
      "Epoch [5/25], Train Loss: 0.0001753499818732962, Validation Loss: 0.00014951860212022438\n",
      "Epoch [5/25], Train Loss: 0.0002028752351179719, Validation Loss: 0.00014936183045695847\n",
      "Epoch [5/25], Train Loss: 0.00013214562204666436, Validation Loss: 0.00014900069072609767\n",
      "Epoch [5/25], Train Loss: 0.00014263765478972346, Validation Loss: 0.00014859526612175008\n",
      "Epoch [5/25], Train Loss: 0.00017022184329107404, Validation Loss: 0.00014812836137328606\n",
      "Epoch [5/25], Train Loss: 0.00024084988399408758, Validation Loss: 0.00014796063624089584\n",
      "Epoch [5/25], Train Loss: 0.00014653970720246434, Validation Loss: 0.00014802610967308284\n",
      "Epoch [5/25], Train Loss: 0.00015744393749628216, Validation Loss: 0.00014832469023531302\n",
      "Epoch [5/25], Train Loss: 0.0001204504442284815, Validation Loss: 0.00014889456918657136\n",
      "Epoch [5/25], Train Loss: 0.0001502880040789023, Validation Loss: 0.00014962340428610333\n",
      "Epoch [5/25], Train Loss: 0.0001913392188725993, Validation Loss: 0.00015039087181018356\n",
      "Epoch [5/25], Train Loss: 0.00020253175171092153, Validation Loss: 0.0001508933795169772\n",
      "Epoch [5/25], Train Loss: 0.00015157723100855947, Validation Loss: 0.00015126689031603747\n",
      "Epoch [5/25], Train Loss: 0.00017941600526683033, Validation Loss: 0.00015087731032205435\n",
      "Epoch [5/25], Train Loss: 0.0001938254717970267, Validation Loss: 0.00015034912600337219\n",
      "Epoch [5/25], Train Loss: 0.00013676710659638047, Validation Loss: 0.00014931711217892977\n",
      "Epoch [5/25], Train Loss: 0.00019157577480655164, Validation Loss: 0.00014841952530938822\n",
      "Epoch [5/25], Train Loss: 0.00018363694834988564, Validation Loss: 0.00014795917595620268\n",
      "Epoch [5/25], Train Loss: 0.00022811139933764935, Validation Loss: 0.00014806195152535415\n",
      "Epoch [5/25], Train Loss: 0.0001939555659191683, Validation Loss: 0.0001488405267688601\n",
      "Epoch [5/25], Train Loss: 0.00013356086856219918, Validation Loss: 0.00014978120913535046\n",
      "Epoch [5/25], Train Loss: 0.00014484665007330477, Validation Loss: 0.0001504461150034331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], Train Loss: 0.00014065853611100465, Validation Loss: 0.0001495546952355653\n",
      "Epoch [5/25], Train Loss: 0.00010166280844714493, Validation Loss: 0.00014850903559514942\n",
      "Epoch [5/25], Train Loss: 0.00017361775098834187, Validation Loss: 0.00014792503425269387\n",
      "Epoch [5/25], Train Loss: 0.00013807954383082688, Validation Loss: 0.00014809946684787671\n",
      "Epoch [5/25], Train Loss: 0.00022236591030377895, Validation Loss: 0.0001485713587802214\n",
      "Epoch [5/25], Train Loss: 0.00012286807759664953, Validation Loss: 0.00014896304298114653\n",
      "Epoch [5/25], Train Loss: 0.00010177562944591045, Validation Loss: 0.00014914652395721836\n",
      "Epoch [5/25], Train Loss: 0.0001815544383134693, Validation Loss: 0.00014901365163192773\n",
      "Epoch [5/25], Train Loss: 0.00012758684169966727, Validation Loss: 0.00014857667022927974\n",
      "Epoch [5/25], Train Loss: 0.0001664699229877442, Validation Loss: 0.0001480439265530246\n",
      "Epoch [5/25], Train Loss: 0.00010876813757931814, Validation Loss: 0.00014783514610220057\n",
      "Epoch [5/25], Train Loss: 0.00021503903553821146, Validation Loss: 0.00014796865628644202\n",
      "Epoch [5/25], Train Loss: 0.00016282626893371344, Validation Loss: 0.00014825464556148896\n",
      "Epoch [5/25], Train Loss: 0.00017999783449340612, Validation Loss: 0.00014856240450171753\n",
      "Epoch [5/25], Train Loss: 0.00017850377480499446, Validation Loss: 0.00014868322808373098\n",
      "Epoch [5/25], Train Loss: 0.00020220494479872286, Validation Loss: 0.00014889030862832441\n",
      "Epoch [5/25], Train Loss: 0.00015747510769870132, Validation Loss: 0.00014859971512729922\n",
      "Epoch [5/25], Train Loss: 0.00017897367069963366, Validation Loss: 0.0001481998867044846\n",
      "Epoch [5/25], Train Loss: 0.00015751330647617579, Validation Loss: 0.00014793455072018938\n",
      "Epoch [5/25], Train Loss: 0.00015773485938552767, Validation Loss: 0.00014794009184697642\n",
      "Epoch [5/25], Train Loss: 0.00016771384980529547, Validation Loss: 0.00014825018976504604\n",
      "Epoch [5/25], Train Loss: 0.00011693781561916694, Validation Loss: 0.00014856321383073615\n",
      "Epoch [5/25], Train Loss: 0.00016228467575274408, Validation Loss: 0.00014875535804700728\n",
      "Epoch [5/25], Train Loss: 0.00010543657845119014, Validation Loss: 0.00014875139798580978\n",
      "Epoch [5/25], Train Loss: 0.00013071174907963723, Validation Loss: 0.00014852591969732504\n",
      "Epoch [5/25], Train Loss: 0.00014124032168183476, Validation Loss: 0.00014831150814037148\n",
      "Epoch [5/25], Train Loss: 0.00016297474212478846, Validation Loss: 0.00014816746285456855\n",
      "Epoch [5/25], Train Loss: 0.000169900493347086, Validation Loss: 0.00014798543318950881\n",
      "Epoch [5/25], Train Loss: 0.00015666763647459447, Validation Loss: 0.00014783812778963086\n",
      "Epoch [5/25], Train Loss: 0.00015309014997910708, Validation Loss: 0.00014773153889109382\n",
      "Epoch [5/25], Train Loss: 0.0001665956515353173, Validation Loss: 0.00014769804086730195\n",
      "Epoch [5/25], Train Loss: 0.0002177370188292116, Validation Loss: 0.00014770107494162707\n",
      "Epoch [5/25], Train Loss: 0.0001475952158216387, Validation Loss: 0.00014774701679319454\n",
      "Epoch [5/25], Train Loss: 0.000189901256817393, Validation Loss: 0.0001479520290255702\n",
      "Epoch [5/25], Train Loss: 0.00015841922140680254, Validation Loss: 0.00014820336412716035\n",
      "Epoch [5/25], Train Loss: 0.00011166460171807557, Validation Loss: 0.00014841758844947133\n",
      "Epoch [5/25], Train Loss: 0.0001588711456861347, Validation Loss: 0.00014905053831171244\n",
      "Epoch [5/25], Train Loss: 0.00016693070938345045, Validation Loss: 0.00015037630898101877\n",
      "Epoch [5/25], Train Loss: 0.00013905145169701427, Validation Loss: 0.00015191053607850337\n",
      "Epoch [5/25], Train Loss: 0.00020904258417431265, Validation Loss: 0.00015304954285966232\n",
      "Epoch [5/25], Train Loss: 0.00018444682064000517, Validation Loss: 0.00015293085574133631\n",
      "Epoch [5/25], Train Loss: 0.00024764484260231256, Validation Loss: 0.00015097306823008694\n",
      "Epoch [5/25], Train Loss: 0.0001554021000629291, Validation Loss: 0.00014886460388273312\n",
      "Epoch [5/25], Train Loss: 0.00019607866124715656, Validation Loss: 0.00014773376509159182\n",
      "Epoch [5/25], Train Loss: 0.0001806421932997182, Validation Loss: 0.00014800840532795215\n",
      "Epoch [5/25], Train Loss: 0.0001490381546318531, Validation Loss: 0.00014912556362105532\n",
      "Epoch [5/25], Train Loss: 0.0001904105010908097, Validation Loss: 0.0001509827724172889\n",
      "Epoch [5/25], Train Loss: 0.0001813170820241794, Validation Loss: 0.00015200402825333488\n",
      "Epoch [5/25], Train Loss: 0.00011156708205817267, Validation Loss: 0.0001520189798611682\n",
      "Epoch [5/25], Train Loss: 0.0001564618432894349, Validation Loss: 0.00015024949510310156\n",
      "Epoch [5/25], Train Loss: 0.00020330895495135337, Validation Loss: 0.0001483992593421135\n",
      "Epoch [5/25], Train Loss: 0.00014015210035722703, Validation Loss: 0.00014771538262721152\n",
      "Epoch [5/25], Train Loss: 0.00015620003978256136, Validation Loss: 0.0001486390334321186\n",
      "Epoch [5/25], Train Loss: 0.00014696005382575095, Validation Loss: 0.00014997710968600586\n",
      "Epoch [5/25], Train Loss: 0.00012486679770518094, Validation Loss: 0.00015004544863283324\n",
      "Epoch [5/25], Train Loss: 0.00014825021207798272, Validation Loss: 0.0001492244392769256\n",
      "Epoch [5/25], Train Loss: 0.000146752834552899, Validation Loss: 0.00014810420689173042\n",
      "Epoch [5/25], Train Loss: 0.00010604003182379529, Validation Loss: 0.00014758796727013153\n",
      "Epoch [5/25], Train Loss: 0.000160032301209867, Validation Loss: 0.00014794159045171303\n",
      "Epoch [5/25], Train Loss: 0.00010686681343941018, Validation Loss: 0.00014854258430811265\n",
      "Epoch [5/25], Train Loss: 0.00019964987586718053, Validation Loss: 0.0001494934937606255\n",
      "Epoch [5/25], Train Loss: 0.00014785505481995642, Validation Loss: 0.00014988406304231223\n",
      "Epoch [5/25], Train Loss: 0.00014057962107472122, Validation Loss: 0.00014933033307897858\n",
      "Epoch [5/25], Train Loss: 0.00021745366393588483, Validation Loss: 0.0001482968208923315\n",
      "Epoch [5/25], Train Loss: 0.00010821527394000441, Validation Loss: 0.00014757311534291755\n",
      "Epoch [5/25], Train Loss: 0.00012659425556194037, Validation Loss: 0.00014769538708302813\n",
      "Epoch [5/25], Train Loss: 0.0001396838779328391, Validation Loss: 0.00014833954458784622\n",
      "Epoch [5/25], Train Loss: 0.00020617127302102745, Validation Loss: 0.00014975079684518278\n",
      "Epoch [5/25], Train Loss: 0.00012605273514054716, Validation Loss: 0.00015053313715422215\n",
      "Epoch [5/25], Train Loss: 0.00015235818864312023, Validation Loss: 0.00015000186200874547\n",
      "Epoch [5/25], Train Loss: 0.00014857048518024385, Validation Loss: 0.00014850903365489407\n",
      "Epoch [5/25], Train Loss: 0.00015162395720835775, Validation Loss: 0.00014758918502290422\n",
      "Epoch [5/25], Train Loss: 0.00011198698484804481, Validation Loss: 0.00014776472962694243\n",
      "Epoch [5/25], Train Loss: 0.00014242384349927306, Validation Loss: 0.0001484922582070188\n",
      "Epoch [5/25], Train Loss: 0.0001861124183051288, Validation Loss: 0.0001490877667189731\n",
      "Epoch [5/25], Train Loss: 0.00013075993047095835, Validation Loss: 0.00014928731300945704\n",
      "Epoch [5/25], Train Loss: 0.00018349294259678572, Validation Loss: 0.00014933216201219087\n",
      "Epoch [5/25], Train Loss: 0.00018053376697935164, Validation Loss: 0.0001486805507738609\n",
      "Epoch [5/25], Train Loss: 0.00013871656847186387, Validation Loss: 0.00014784860322833993\n",
      "Epoch [5/25], Train Loss: 0.00015759032976347953, Validation Loss: 0.000147494045086205\n",
      "Epoch [5/25], Train Loss: 0.00017026730347424746, Validation Loss: 0.00014781757272430696\n",
      "Epoch [5/25], Train Loss: 0.00019228900782763958, Validation Loss: 0.00014844099132460542\n",
      "Epoch [5/25], Train Loss: 0.00015348635497502983, Validation Loss: 0.00014873629891856883\n",
      "Epoch [5/25], Train Loss: 0.0001430738775525242, Validation Loss: 0.0001485829069376147\n",
      "Epoch [5/25], Train Loss: 0.00012552307453006506, Validation Loss: 0.00014811530879039008\n",
      "Epoch [5/25], Train Loss: 0.00019433240231592208, Validation Loss: 0.00014759402498990917\n",
      "Epoch [5/25], Train Loss: 0.0002091596688842401, Validation Loss: 0.00014740825038946545\n",
      "Epoch [5/25], Train Loss: 0.00013382038741838187, Validation Loss: 0.00014755192799687696\n",
      "Epoch [5/25], Train Loss: 0.00012559939932543784, Validation Loss: 0.00014785288949497043\n",
      "Epoch [5/25], Train Loss: 0.0001581975375302136, Validation Loss: 0.00014872511577171583\n",
      "Epoch [5/25], Train Loss: 0.00012928117939736694, Validation Loss: 0.0001505943274726936\n",
      "Epoch [5/25], Train Loss: 0.00018970289966091514, Validation Loss: 0.00015381689008790999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], Train Loss: 0.0001641734124859795, Validation Loss: 0.00016168550428119488\n",
      "Epoch [5/25], Train Loss: 0.00014981227286625654, Validation Loss: 0.00016505803893475482\n",
      "Epoch [5/25], Train Loss: 0.00013542953820433468, Validation Loss: 0.00015909438079688698\n",
      "Epoch [5/25], Train Loss: 0.0001759151928126812, Validation Loss: 0.00014949912995992538\n",
      "Epoch [5/25], Train Loss: 0.0001456709869671613, Validation Loss: 0.00015031347284093498\n",
      "Epoch [5/25], Train Loss: 0.0001329995138803497, Validation Loss: 0.0001565131671668496\n",
      "Epoch [5/25], Train Loss: 0.00019351970695424825, Validation Loss: 0.0001549547589092981\n",
      "Epoch [5/25], Train Loss: 0.0001484952081227675, Validation Loss: 0.00014881283835469123\n",
      "Epoch [5/25], Train Loss: 0.00013964387471787632, Validation Loss: 0.0001488570528939211\n",
      "Epoch [5/25], Train Loss: 0.00013678162940777838, Validation Loss: 0.0001530888660151201\n",
      "Epoch [5/25], Train Loss: 0.00017081490659620613, Validation Loss: 0.00015314381089410746\n",
      "Epoch [5/25], Train Loss: 0.00013038907491136342, Validation Loss: 0.000148687003578137\n",
      "Epoch [5/25], Train Loss: 0.0002069766487693414, Validation Loss: 0.0001479881847141466\n",
      "Epoch [5/25], Train Loss: 0.00021642004139721394, Validation Loss: 0.0001508405618854643\n",
      "Epoch [5/25], Train Loss: 0.0001912973530124873, Validation Loss: 0.00015118352481901335\n",
      "Epoch [5/25], Train Loss: 0.00015731387247797102, Validation Loss: 0.00014894707540709837\n",
      "Epoch [5/25], Train Loss: 0.00015029545465949923, Validation Loss: 0.00014783430718428764\n",
      "Epoch [5/25], Train Loss: 0.00018214978626929224, Validation Loss: 0.00014972554660441044\n",
      "Epoch [5/25], Train Loss: 0.00016753640375100076, Validation Loss: 0.00015034966127132067\n",
      "Epoch [5/25], Train Loss: 0.00015026396431494504, Validation Loss: 0.00014825862890575082\n",
      "Epoch [5/25], Train Loss: 0.00019967815023846924, Validation Loss: 0.0001479237304010894\n",
      "Epoch [5/25], Train Loss: 0.00018742898828350008, Validation Loss: 0.00014898641262940752\n",
      "Epoch [5/25], Train Loss: 0.00019122665980830789, Validation Loss: 0.00014898776377473646\n",
      "Epoch [5/25], Train Loss: 0.0001669834164204076, Validation Loss: 0.00014787124212792453\n",
      "Epoch [5/25], Train Loss: 0.00015787154552526772, Validation Loss: 0.00014752352144569157\n",
      "Epoch [5/25], Train Loss: 0.00014720890612807125, Validation Loss: 0.00014820171612276074\n",
      "Epoch [5/25], Train Loss: 0.00014605015167035162, Validation Loss: 0.00014863764833232077\n",
      "Epoch [5/25], Train Loss: 0.00016186616267077625, Validation Loss: 0.00014810887029549727\n",
      "Epoch [5/25], Train Loss: 0.00017918090452440083, Validation Loss: 0.00014756293555061954\n",
      "Epoch [5/25], Train Loss: 0.0001622751005925238, Validation Loss: 0.00014744208140958411\n",
      "Epoch [5/25], Train Loss: 0.00014846732665318996, Validation Loss: 0.00014784276208956726\n",
      "Epoch [5/25], Train Loss: 0.00012237837654538453, Validation Loss: 0.0001480545955322062\n",
      "Epoch [5/25], Train Loss: 0.0001349701196886599, Validation Loss: 0.00014773655154082615\n",
      "Epoch [5/25], Train Loss: 0.00020287721417844296, Validation Loss: 0.00014738983251542473\n",
      "Epoch [5/25], Train Loss: 0.00013543208478949964, Validation Loss: 0.00014742984567419625\n",
      "Epoch [5/25], Train Loss: 0.00017377060430590063, Validation Loss: 0.0001476118566642981\n",
      "Epoch [5/25], Train Loss: 0.00016563766985200346, Validation Loss: 0.000147722895538512\n",
      "Epoch [5/25], Train Loss: 0.0001430076517863199, Validation Loss: 0.00014760026776154214\n",
      "Epoch [5/25], Train Loss: 0.00012479451834224164, Validation Loss: 0.00014743951590692934\n",
      "Epoch [5/25], Train Loss: 0.00017418476636521518, Validation Loss: 0.0001473069962230511\n",
      "Epoch [5/25], Train Loss: 0.00014845485566183925, Validation Loss: 0.0001473197691666428\n",
      "Epoch [5/25], Train Loss: 0.00015850347699597478, Validation Loss: 0.00014750660508677053\n",
      "Epoch [5/25], Train Loss: 0.00012302104732953012, Validation Loss: 0.00014764394751788738\n",
      "Epoch [5/25], Train Loss: 9.90141779766418e-05, Validation Loss: 0.00014754029131533268\n",
      "Epoch [5/25], Train Loss: 0.00019115926988888532, Validation Loss: 0.00014749849894239256\n",
      "Epoch [5/25], Train Loss: 0.0001523402170278132, Validation Loss: 0.0001473732763164056\n",
      "Epoch [5/25], Train Loss: 0.00016491289716213942, Validation Loss: 0.000147254871505235\n",
      "Epoch [5/25], Train Loss: 0.0001410798286087811, Validation Loss: 0.00014729494408432703\n",
      "Epoch [5/25], Train Loss: 0.00013920544006396085, Validation Loss: 0.0001473394212856268\n",
      "Epoch [5/25], Train Loss: 0.00015677792544011027, Validation Loss: 0.00014732642860811515\n",
      "Epoch [5/25], Train Loss: 0.00018523169273976237, Validation Loss: 0.00014727577120841792\n",
      "Epoch [5/25], Train Loss: 0.00015738251386210322, Validation Loss: 0.00014726918840703244\n",
      "Epoch [5/25], Train Loss: 0.00015022784646134824, Validation Loss: 0.00014724554154478634\n",
      "Epoch [5/25], Train Loss: 0.00019850973330903798, Validation Loss: 0.00014720962669040699\n",
      "Epoch [5/25], Train Loss: 0.00012648693518713117, Validation Loss: 0.0001472118485253304\n",
      "Epoch [5/25], Train Loss: 0.00015389861073344946, Validation Loss: 0.0001472278920118697\n",
      "Epoch [5/25], Train Loss: 0.00011525136505952105, Validation Loss: 0.00014722778651048428\n",
      "Epoch [5/25], Train Loss: 0.00014749715046491474, Validation Loss: 0.00014720673473978725\n",
      "Epoch [5/25], Train Loss: 0.0001701309665804729, Validation Loss: 0.0001472076357458718\n",
      "Epoch [5/25], Train Loss: 0.00010702270810725167, Validation Loss: 0.0001471899345536561\n",
      "Epoch [5/25], Train Loss: 0.00020528692402876914, Validation Loss: 0.00014715333212128218\n",
      "Epoch [5/25], Train Loss: 0.00011752487625926733, Validation Loss: 0.00014716390263250407\n",
      "Epoch [5/25], Train Loss: 0.0001103833201341331, Validation Loss: 0.00014721548577654176\n",
      "Epoch [5/25], Train Loss: 0.00017666202620603144, Validation Loss: 0.0001472366076389638\n",
      "Epoch [5/25], Train Loss: 0.00024972038227133453, Validation Loss: 0.00014720179387950338\n",
      "Epoch [5/25], Train Loss: 0.00016542214143555611, Validation Loss: 0.00014762269541582402\n",
      "Epoch [5/25], Train Loss: 0.00014905315765645355, Validation Loss: 0.00014779796183574944\n",
      "Epoch [5/25], Train Loss: 0.00019030863768421113, Validation Loss: 0.0001475409679793908\n",
      "Epoch [5/25], Train Loss: 0.00018215918680652976, Validation Loss: 0.00014771133622465033\n",
      "Epoch [5/25], Train Loss: 0.0001564536796649918, Validation Loss: 0.0001479190449269178\n",
      "Epoch [5/25], Train Loss: 0.000186372286407277, Validation Loss: 0.00014804173794497427\n",
      "Epoch [5/25], Train Loss: 0.00012436941324267536, Validation Loss: 0.00014827931251299258\n",
      "Epoch [5/25], Train Loss: 0.0001303474564338103, Validation Loss: 0.00014831185423342202\n",
      "Epoch [5/25], Train Loss: 0.00021842450951226056, Validation Loss: 0.00014800818001579804\n",
      "Epoch [5/25], Train Loss: 0.00014552869834005833, Validation Loss: 0.00014811552149088432\n",
      "Epoch [5/25], Train Loss: 0.00011849420116050169, Validation Loss: 0.00014862495348400747\n",
      "Epoch [5/25], Train Loss: 9.3138383817859e-05, Validation Loss: 0.00014935802149314745\n",
      "Epoch [5/25], Train Loss: 0.00016923225484788418, Validation Loss: 0.00014951045571554762\n",
      "Epoch [5/25], Train Loss: 0.00012028042692691088, Validation Loss: 0.00014922561652686757\n",
      "Epoch [5/25], Train Loss: 0.0001421438209945336, Validation Loss: 0.00014869818575486232\n",
      "Epoch [5/25], Train Loss: 0.00016645963478367776, Validation Loss: 0.00014802594814682378\n",
      "Epoch [5/25], Train Loss: 0.0001457303442293778, Validation Loss: 0.00014764340473144937\n",
      "Epoch [5/25], Train Loss: 0.00015040888683870435, Validation Loss: 0.00014775603728291268\n",
      "Epoch [5/25], Train Loss: 0.00015340163372457027, Validation Loss: 0.0001481277562561445\n",
      "Epoch [5/25], Train Loss: 0.00016778781719040126, Validation Loss: 0.0001485082121992794\n",
      "Epoch [5/25], Train Loss: 0.0001349739613942802, Validation Loss: 0.00014888451211542513\n",
      "Epoch [5/25], Train Loss: 0.000154786161147058, Validation Loss: 0.00014912232994295966\n",
      "Epoch [5/25], Train Loss: 0.0001897346373880282, Validation Loss: 0.00014919838819575185\n",
      "Epoch [5/25], Train Loss: 0.0001575171045260504, Validation Loss: 0.00014898005295738888\n",
      "Epoch [5/25], Train Loss: 0.00015575646830257028, Validation Loss: 0.00014895578933646903\n",
      "Epoch [5/25], Train Loss: 0.00022029747196938843, Validation Loss: 0.00014884022190623605\n",
      "Epoch [5/25], Train Loss: 0.00010391672549303621, Validation Loss: 0.00014869848722203944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], Train Loss: 0.00018632219871506095, Validation Loss: 0.000148399486594523\n",
      "Epoch [5/25], Train Loss: 0.0001258885022252798, Validation Loss: 0.00014812315712333656\n",
      "Epoch [5/25], Train Loss: 0.00016701828280929476, Validation Loss: 0.00014795319051093733\n",
      "Epoch [5/25], Train Loss: 0.00014535609807353467, Validation Loss: 0.00014775113328748072\n",
      "Epoch [5/25], Train Loss: 0.00015880362479947507, Validation Loss: 0.00014758001828643803\n",
      "Epoch [5/25], Train Loss: 0.00011380184878362343, Validation Loss: 0.0001473948078152413\n",
      "Epoch [5/25], Train Loss: 0.00020307851082179695, Validation Loss: 0.00014725607300836903\n",
      "Epoch [5/25], Train Loss: 0.00011798898049164563, Validation Loss: 0.00014713937271153553\n",
      "Epoch [5/25], Train Loss: 0.0001493001909693703, Validation Loss: 0.00014711384792462923\n",
      "Epoch [5/25], Train Loss: 0.00017817771004047245, Validation Loss: 0.00014713297011136698\n",
      "Epoch [5/25], Train Loss: 0.00012004675227217376, Validation Loss: 0.00014716457759883877\n",
      "Epoch [5/25], Train Loss: 0.00016732719086576253, Validation Loss: 0.00014719230263532762\n",
      "Epoch [5/25], Train Loss: 0.0001507374836364761, Validation Loss: 0.00014727805561657684\n",
      "Epoch [5/25], Train Loss: 0.0001534338662168011, Validation Loss: 0.00014733880840746375\n",
      "Epoch [5/25], Train Loss: 0.0001343642798019573, Validation Loss: 0.00014731406190549023\n",
      "Epoch [5/25], Train Loss: 0.00010633211059030145, Validation Loss: 0.00014727014325520335\n",
      "Epoch [5/25], Train Loss: 0.00015638419426977634, Validation Loss: 0.0001471191356055594\n",
      "Epoch [5/25], Train Loss: 0.00014724575157742947, Validation Loss: 0.00014704096781012292\n",
      "Epoch [5/25], Train Loss: 0.00017012289026752114, Validation Loss: 0.00014702883709105663\n",
      "Epoch [5/25], Train Loss: 0.0001590267347637564, Validation Loss: 0.00014705714711453765\n",
      "Epoch [5/25], Train Loss: 0.0001734088727971539, Validation Loss: 0.00014714865586332355\n",
      "Epoch [5/25], Train Loss: 0.0001654900552239269, Validation Loss: 0.00014723603259578037\n",
      "Epoch [5/25], Train Loss: 0.00014321949856821448, Validation Loss: 0.00014731273840880023\n",
      "Epoch [5/25], Train Loss: 0.00016085054085124284, Validation Loss: 0.00014728734968230128\n",
      "Epoch [5/25], Train Loss: 0.00016878926544450223, Validation Loss: 0.00014726516092196108\n",
      "Epoch [5/25], Train Loss: 8.750077540753409e-05, Validation Loss: 0.0001472896018337148\n",
      "Epoch [5/25], Train Loss: 0.00014952073979657143, Validation Loss: 0.00014723207471737017\n",
      "Epoch [5/25], Train Loss: 0.0001776509016053751, Validation Loss: 0.0001471158330483983\n",
      "Epoch [5/25], Train Loss: 0.00021130642562638968, Validation Loss: 0.0001470677918405272\n",
      "Epoch [5/25], Train Loss: 0.0001256998220924288, Validation Loss: 0.00014706032743561082\n",
      "Epoch [5/25], Train Loss: 0.00014299366739578545, Validation Loss: 0.00014706647998536938\n",
      "Epoch [5/25], Train Loss: 0.0001415033038938418, Validation Loss: 0.00014705477879033423\n",
      "Epoch [5/25], Train Loss: 0.0001461568899685517, Validation Loss: 0.00014709298726908553\n",
      "Epoch [5/25], Train Loss: 0.00018045057367999107, Validation Loss: 0.00014729456840238223\n",
      "Epoch [5/25], Train Loss: 0.00016469867841806263, Validation Loss: 0.00014796887286744703\n",
      "Epoch [5/25], Train Loss: 8.454353519482538e-05, Validation Loss: 0.00014893534801861582\n",
      "Epoch [5/25], Train Loss: 0.0001987540745176375, Validation Loss: 0.0001503355240856763\n",
      "Epoch [5/25], Train Loss: 0.00018393782374914736, Validation Loss: 0.00015291650658279347\n",
      "Epoch [5/25], Train Loss: 0.00020923331612721086, Validation Loss: 0.00015582667838316412\n",
      "Epoch [5/25], Train Loss: 0.00016444572247564793, Validation Loss: 0.00016005620467088495\n",
      "Epoch [5/25], Train Loss: 0.00011563593579921871, Validation Loss: 0.00016045032267963203\n",
      "Epoch [5/25], Train Loss: 0.00017930376634467393, Validation Loss: 0.00015793990314705297\n",
      "Epoch [5/25], Train Loss: 0.00011926684965146706, Validation Loss: 0.00015094034679350444\n",
      "Epoch [5/25], Train Loss: 0.0001909194397740066, Validation Loss: 0.00014729869023237066\n",
      "Epoch [5/25], Train Loss: 0.00016502001381013542, Validation Loss: 0.00014997239365281226\n",
      "Epoch [5/25], Train Loss: 0.0001918026973726228, Validation Loss: 0.00015312257358649124\n",
      "Epoch [5/25], Train Loss: 0.00016229716129601002, Validation Loss: 0.00015189128025667743\n",
      "Epoch [5/25], Train Loss: 0.0001756358687998727, Validation Loss: 0.000147927610911817\n",
      "Epoch [5/25], Train Loss: 0.00019880267791450024, Validation Loss: 0.00014734276822612932\n",
      "Epoch [5/25], Train Loss: 0.00013954190944787115, Validation Loss: 0.0001497800284899616\n",
      "Epoch [5/25], Train Loss: 0.00016158536891452968, Validation Loss: 0.00015126587701767372\n",
      "Epoch [5/25], Train Loss: 0.00016562918608542532, Validation Loss: 0.00015344186782992135\n",
      "Epoch [6/25], Train Loss: 0.00014530243061017245, Validation Loss: 0.0001502277419300905\n",
      "Epoch [6/25], Train Loss: 0.00015071048983372748, Validation Loss: 0.00014733849166077563\n",
      "Epoch [6/25], Train Loss: 0.00013688272156286985, Validation Loss: 0.00014913533353440775\n",
      "Epoch [6/25], Train Loss: 0.00016015615256037563, Validation Loss: 0.00015098374691054536\n",
      "Epoch [6/25], Train Loss: 0.00016592195606790483, Validation Loss: 0.0001491943881167875\n",
      "Epoch [6/25], Train Loss: 0.0001365642383461818, Validation Loss: 0.00014728543634798068\n",
      "Epoch [6/25], Train Loss: 7.627755257999524e-05, Validation Loss: 0.00014844700514610548\n",
      "Epoch [6/25], Train Loss: 0.00015916448319330812, Validation Loss: 0.00014976047144349042\n",
      "Epoch [6/25], Train Loss: 0.00011148107296321541, Validation Loss: 0.0001485986974633609\n",
      "Epoch [6/25], Train Loss: 0.00018552712572272867, Validation Loss: 0.000147207318029056\n",
      "Epoch [6/25], Train Loss: 0.00018180912593379617, Validation Loss: 0.00014750120317330584\n",
      "Epoch [6/25], Train Loss: 0.00019811424135696143, Validation Loss: 0.00014852449069924963\n",
      "Epoch [6/25], Train Loss: 0.00014462524268310517, Validation Loss: 0.00014872362201761764\n",
      "Epoch [6/25], Train Loss: 0.00019942526705563068, Validation Loss: 0.00014775565408247832\n",
      "Epoch [6/25], Train Loss: 0.00016841499018482864, Validation Loss: 0.00014697780012890387\n",
      "Epoch [6/25], Train Loss: 0.00015060701116453856, Validation Loss: 0.00014700139824223393\n",
      "Epoch [6/25], Train Loss: 0.0001751345698721707, Validation Loss: 0.00014732626611172842\n",
      "Epoch [6/25], Train Loss: 0.00012379165855236351, Validation Loss: 0.00014746630695299244\n",
      "Epoch [6/25], Train Loss: 0.00017800004570744932, Validation Loss: 0.0001473456963140052\n",
      "Epoch [6/25], Train Loss: 0.00013229547766968608, Validation Loss: 0.00014703904768490853\n",
      "Epoch [6/25], Train Loss: 0.00017451783060096204, Validation Loss: 0.00014691220324796934\n",
      "Epoch [6/25], Train Loss: 0.00021246013056952506, Validation Loss: 0.00014704368319750453\n",
      "Epoch [6/25], Train Loss: 0.00018976214050780982, Validation Loss: 0.0001473318538046442\n",
      "Epoch [6/25], Train Loss: 0.00016875834262464195, Validation Loss: 0.00014745612473537523\n",
      "Epoch [6/25], Train Loss: 0.00011187302152393386, Validation Loss: 0.00014736485876104172\n",
      "Epoch [6/25], Train Loss: 0.000150268868310377, Validation Loss: 0.00014723243342208055\n",
      "Epoch [6/25], Train Loss: 0.00014565972378477454, Validation Loss: 0.00014717176284951468\n",
      "Epoch [6/25], Train Loss: 0.00016722429427318275, Validation Loss: 0.00014699470654401617\n",
      "Epoch [6/25], Train Loss: 0.00014590923092328012, Validation Loss: 0.00014692533077322877\n",
      "Epoch [6/25], Train Loss: 0.00017921728431247175, Validation Loss: 0.00014698538094914208\n",
      "Epoch [6/25], Train Loss: 0.00012179753684904426, Validation Loss: 0.0001471010747385056\n",
      "Epoch [6/25], Train Loss: 0.00014550158812198788, Validation Loss: 0.00014715547901384223\n",
      "Epoch [6/25], Train Loss: 0.00020229595247656107, Validation Loss: 0.000147206345233523\n",
      "Epoch [6/25], Train Loss: 0.00015931339294184, Validation Loss: 0.00014725298460689374\n",
      "Epoch [6/25], Train Loss: 0.00012257049093022943, Validation Loss: 0.00014721748035905572\n",
      "Epoch [6/25], Train Loss: 0.00012908787175547332, Validation Loss: 0.00014710417284125772\n",
      "Epoch [6/25], Train Loss: 0.00012038883869536221, Validation Loss: 0.00014700315077789127\n",
      "Epoch [6/25], Train Loss: 0.0001406470692018047, Validation Loss: 0.00014694628092305114\n",
      "Epoch [6/25], Train Loss: 0.0001672716171015054, Validation Loss: 0.00014688221732891786\n",
      "Epoch [6/25], Train Loss: 0.00019978937052655965, Validation Loss: 0.00014678859515697694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], Train Loss: 0.00011963211727561429, Validation Loss: 0.0001467827445594594\n",
      "Epoch [6/25], Train Loss: 0.00014622810704167932, Validation Loss: 0.0001467936694098171\n",
      "Epoch [6/25], Train Loss: 0.00016507258987985551, Validation Loss: 0.00014683607223560103\n",
      "Epoch [6/25], Train Loss: 0.00021979628945700824, Validation Loss: 0.00014684912772888007\n",
      "Epoch [6/25], Train Loss: 0.0001042683215928264, Validation Loss: 0.00014682428906477677\n",
      "Epoch [6/25], Train Loss: 7.56436784286052e-05, Validation Loss: 0.00014678117222501896\n",
      "Epoch [6/25], Train Loss: 0.00016270454216282815, Validation Loss: 0.00014673770153118918\n",
      "Epoch [6/25], Train Loss: 0.00010520521755097434, Validation Loss: 0.000146735109835087\n",
      "Epoch [6/25], Train Loss: 0.00015579340106341988, Validation Loss: 0.00014673716092753844\n",
      "Epoch [6/25], Train Loss: 0.00017747269885148853, Validation Loss: 0.00014675729131946962\n",
      "Epoch [6/25], Train Loss: 0.00012451349175535142, Validation Loss: 0.00014677009409448752\n",
      "Epoch [6/25], Train Loss: 0.00017419212963432074, Validation Loss: 0.00014680042004329153\n",
      "Epoch [6/25], Train Loss: 0.00022093644656706601, Validation Loss: 0.0001468737900722772\n",
      "Epoch [6/25], Train Loss: 0.0001369310193695128, Validation Loss: 0.00014694445053464733\n",
      "Epoch [6/25], Train Loss: 0.00016348256031051278, Validation Loss: 0.00014688158480566927\n",
      "Epoch [6/25], Train Loss: 0.0001922367955558002, Validation Loss: 0.00014678080163624447\n",
      "Epoch [6/25], Train Loss: 0.00016077960026450455, Validation Loss: 0.00014673158924172943\n",
      "Epoch [6/25], Train Loss: 0.00017005292465910316, Validation Loss: 0.00014672301234289382\n",
      "Epoch [6/25], Train Loss: 0.00013245758600533009, Validation Loss: 0.0001466908332076855\n",
      "Epoch [6/25], Train Loss: 0.00017084127466659993, Validation Loss: 0.00014662614121334626\n",
      "Epoch [6/25], Train Loss: 0.0001453004515497014, Validation Loss: 0.00014658421932836063\n",
      "Epoch [6/25], Train Loss: 0.00012238834460731596, Validation Loss: 0.00014660253630912243\n",
      "Epoch [6/25], Train Loss: 0.00015339076344389468, Validation Loss: 0.00014672459268088762\n",
      "Epoch [6/25], Train Loss: 0.00013499606575351208, Validation Loss: 0.00014692273131610515\n",
      "Epoch [6/25], Train Loss: 0.0001776116550900042, Validation Loss: 0.00014700799098742816\n",
      "Epoch [6/25], Train Loss: 0.00012439730926416814, Validation Loss: 0.0001470748759553923\n",
      "Epoch [6/25], Train Loss: 0.00013542475062422454, Validation Loss: 0.00014732129930052906\n",
      "Epoch [6/25], Train Loss: 0.00021106869098730385, Validation Loss: 0.00014763893341296352\n",
      "Epoch [6/25], Train Loss: 0.00020664793555624783, Validation Loss: 0.00014852774669028198\n",
      "Epoch [6/25], Train Loss: 0.00018599194299895316, Validation Loss: 0.00014992100356418328\n",
      "Epoch [6/25], Train Loss: 0.00018310394079890102, Validation Loss: 0.00015155309932500435\n",
      "Epoch [6/25], Train Loss: 0.00017483223928138614, Validation Loss: 0.00015273328851132342\n",
      "Epoch [6/25], Train Loss: 0.00018578646995592862, Validation Loss: 0.0001517119659789993\n",
      "Epoch [6/25], Train Loss: 0.0001748271897668019, Validation Loss: 0.00014980228297645226\n",
      "Epoch [6/25], Train Loss: 0.00011619672295637429, Validation Loss: 0.00014796728913400633\n",
      "Epoch [6/25], Train Loss: 0.00012845675519201905, Validation Loss: 0.00014675829418896077\n",
      "Epoch [6/25], Train Loss: 0.0001752936514094472, Validation Loss: 0.00014720702238264495\n",
      "Epoch [6/25], Train Loss: 0.00019876960141118616, Validation Loss: 0.00014877479067460325\n",
      "Epoch [6/25], Train Loss: 9.950405365088955e-05, Validation Loss: 0.00015086560864195538\n",
      "Epoch [6/25], Train Loss: 0.0001186113731819205, Validation Loss: 0.00015313455805880948\n",
      "Epoch [6/25], Train Loss: 0.00012880501162726432, Validation Loss: 0.00015300813271702888\n",
      "Epoch [6/25], Train Loss: 0.00015971144603099674, Validation Loss: 0.00015007630766679844\n",
      "Epoch [6/25], Train Loss: 0.0001521059311926365, Validation Loss: 0.00014714053007386003\n",
      "Epoch [6/25], Train Loss: 0.00013304056483320892, Validation Loss: 0.00014738910904270596\n",
      "Epoch [6/25], Train Loss: 0.00017352560826111585, Validation Loss: 0.00014953675296662064\n",
      "Epoch [6/25], Train Loss: 0.0002310963609488681, Validation Loss: 0.00014959415784687735\n",
      "Epoch [6/25], Train Loss: 0.00014896118955221027, Validation Loss: 0.000147968324017711\n",
      "Epoch [6/25], Train Loss: 0.00012345991854090244, Validation Loss: 0.00014676748032798058\n",
      "Epoch [6/25], Train Loss: 0.00013628479791805148, Validation Loss: 0.00014726248433968674\n",
      "Epoch [6/25], Train Loss: 0.00011589181667659432, Validation Loss: 0.00014853819035730946\n",
      "Epoch [6/25], Train Loss: 0.00011555072705959901, Validation Loss: 0.00014905835608563695\n",
      "Epoch [6/25], Train Loss: 0.00019464037904981524, Validation Loss: 0.00014987988640010979\n",
      "Epoch [6/25], Train Loss: 0.00013467241660691798, Validation Loss: 0.00014879690255232466\n",
      "Epoch [6/25], Train Loss: 0.00011186368647031486, Validation Loss: 0.00014728985576463552\n",
      "Epoch [6/25], Train Loss: 0.00018148562230635434, Validation Loss: 0.00014687789929060575\n",
      "Epoch [6/25], Train Loss: 0.00015583695494569838, Validation Loss: 0.0001478570663797048\n",
      "Epoch [6/25], Train Loss: 0.00012988221715204418, Validation Loss: 0.0001486192804198557\n",
      "Epoch [6/25], Train Loss: 0.0001462333311792463, Validation Loss: 0.0001480079685279634\n",
      "Epoch [6/25], Train Loss: 0.00012150719703640789, Validation Loss: 0.00014703530541737564\n",
      "Epoch [6/25], Train Loss: 0.00015118792362045497, Validation Loss: 0.00014659830509723785\n",
      "Epoch [6/25], Train Loss: 0.00016849020903464407, Validation Loss: 0.00014705239082104526\n",
      "Epoch [6/25], Train Loss: 0.00013255930389277637, Validation Loss: 0.0001480097280970464\n",
      "Epoch [6/25], Train Loss: 0.00014014155021868646, Validation Loss: 0.0001487824786939503\n",
      "Epoch [6/25], Train Loss: 0.00013470118574332446, Validation Loss: 0.0001490473000255103\n",
      "Epoch [6/25], Train Loss: 0.0001475698663853109, Validation Loss: 0.00014848052718055744\n",
      "Epoch [6/25], Train Loss: 0.00016624869022052735, Validation Loss: 0.00014782244979869575\n",
      "Epoch [6/25], Train Loss: 9.215397585649043e-05, Validation Loss: 0.0001469689062408482\n",
      "Epoch [6/25], Train Loss: 0.00018806528532877564, Validation Loss: 0.0001465856369274358\n",
      "Epoch [6/25], Train Loss: 8.834643813315779e-05, Validation Loss: 0.00014686683304413842\n",
      "Epoch [6/25], Train Loss: 0.00014452412142418325, Validation Loss: 0.00014743393500490736\n",
      "Epoch [6/25], Train Loss: 0.00015786071890033782, Validation Loss: 0.00014759064385240587\n",
      "Epoch [6/25], Train Loss: 0.0001514723408035934, Validation Loss: 0.00014720644079109965\n",
      "Epoch [6/25], Train Loss: 0.00015514029655605555, Validation Loss: 0.00014672529529586125\n",
      "Epoch [6/25], Train Loss: 0.00012626239913515747, Validation Loss: 0.00014648981693123156\n",
      "Epoch [6/25], Train Loss: 0.0001824120554374531, Validation Loss: 0.00014651754681835883\n",
      "Epoch [6/25], Train Loss: 0.00016242238052655011, Validation Loss: 0.00014663772575052765\n",
      "Epoch [6/25], Train Loss: 0.00018757655925583094, Validation Loss: 0.000146657703104817\n",
      "Epoch [6/25], Train Loss: 0.00016497141041327268, Validation Loss: 0.00014678343674556042\n",
      "Epoch [6/25], Train Loss: 0.00019823828188236803, Validation Loss: 0.00014678554313528973\n",
      "Epoch [6/25], Train Loss: 0.00019886337395291775, Validation Loss: 0.00014682832843391225\n",
      "Epoch [6/25], Train Loss: 0.0001248957560164854, Validation Loss: 0.00014677676639015166\n",
      "Epoch [6/25], Train Loss: 0.00020656299602705985, Validation Loss: 0.00014658287497392545\n",
      "Epoch [6/25], Train Loss: 0.0002292062563356012, Validation Loss: 0.00014646146276694102\n",
      "Epoch [6/25], Train Loss: 0.00018626516975928098, Validation Loss: 0.00014641397040880596\n",
      "Epoch [6/25], Train Loss: 0.00016661389963701367, Validation Loss: 0.0001464977462698395\n",
      "Epoch [6/25], Train Loss: 0.00022418636945076287, Validation Loss: 0.00014654595627992725\n",
      "Epoch [6/25], Train Loss: 0.00015279607032425702, Validation Loss: 0.00014668123961503928\n",
      "Epoch [6/25], Train Loss: 0.00014994993398431689, Validation Loss: 0.0001467733799169461\n",
      "Epoch [6/25], Train Loss: 0.0002222539042122662, Validation Loss: 0.00014734044210248007\n",
      "Epoch [6/25], Train Loss: 0.00019735287060029805, Validation Loss: 0.00014801502329646609\n",
      "Epoch [6/25], Train Loss: 0.00014456881035584956, Validation Loss: 0.0001486130635991382\n",
      "Epoch [6/25], Train Loss: 0.00012681426596827805, Validation Loss: 0.00014863077934326914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], Train Loss: 0.00016642706759739667, Validation Loss: 0.0001481305439180384\n",
      "Epoch [6/25], Train Loss: 0.00019235926447436213, Validation Loss: 0.00014725252985954286\n",
      "Epoch [6/25], Train Loss: 0.00019060360500589013, Validation Loss: 0.00014654517581220717\n",
      "Epoch [6/25], Train Loss: 0.0001507176784798503, Validation Loss: 0.00014644876719103195\n",
      "Epoch [6/25], Train Loss: 0.00016527606931049377, Validation Loss: 0.00014693628254462964\n",
      "Epoch [6/25], Train Loss: 0.0001321214804193005, Validation Loss: 0.00014763526705792175\n",
      "Epoch [6/25], Train Loss: 0.0001320999872405082, Validation Loss: 0.00014862637690384872\n",
      "Epoch [6/25], Train Loss: 0.0001672440703259781, Validation Loss: 0.00015179199472186155\n",
      "Epoch [6/25], Train Loss: 0.00017088970344047993, Validation Loss: 0.00015459507606768358\n",
      "Epoch [6/25], Train Loss: 0.00013229186879470944, Validation Loss: 0.0001566116960020736\n",
      "Epoch [6/25], Train Loss: 0.00021206292149145156, Validation Loss: 0.00015331385196380628\n",
      "Epoch [6/25], Train Loss: 0.00021695707982871681, Validation Loss: 0.00014754473862315837\n",
      "Epoch [6/25], Train Loss: 0.0001970243756659329, Validation Loss: 0.00014780697383685038\n",
      "Epoch [6/25], Train Loss: 0.00020439195213839412, Validation Loss: 0.00015190552706674984\n",
      "Epoch [6/25], Train Loss: 0.00011516368249431252, Validation Loss: 0.00015333873549631485\n",
      "Epoch [6/25], Train Loss: 0.00016309472266584635, Validation Loss: 0.00014997043399489485\n",
      "Epoch [6/25], Train Loss: 0.00016834349662531167, Validation Loss: 0.00014734395420722043\n",
      "Epoch [6/25], Train Loss: 0.00014844516408629715, Validation Loss: 0.00014663272765271055\n",
      "Epoch [6/25], Train Loss: 0.00018309116421733052, Validation Loss: 0.00014817679596793216\n",
      "Epoch [6/25], Train Loss: 0.00020283785124775022, Validation Loss: 0.00014989745929293956\n",
      "Epoch [6/25], Train Loss: 0.00012488341599237174, Validation Loss: 0.00014871417151880452\n",
      "Epoch [6/25], Train Loss: 0.00017760608170647174, Validation Loss: 0.00014677034341730176\n",
      "Epoch [6/25], Train Loss: 0.00016260439588222653, Validation Loss: 0.00014704738423461096\n",
      "Epoch [6/25], Train Loss: 0.00012674095341935754, Validation Loss: 0.00014839684057127062\n",
      "Epoch [6/25], Train Loss: 0.00017522257985547185, Validation Loss: 0.00014819632827614744\n",
      "Epoch [6/25], Train Loss: 0.00019289422198198736, Validation Loss: 0.00014673930393958775\n",
      "Epoch [6/25], Train Loss: 0.00015426904428750277, Validation Loss: 0.00014661236903824222\n",
      "Epoch [6/25], Train Loss: 0.00019657867960631847, Validation Loss: 0.00014784584442774454\n",
      "Epoch [6/25], Train Loss: 0.00016376862186007202, Validation Loss: 0.0001484295210199586\n",
      "Epoch [6/25], Train Loss: 0.00014157884288579226, Validation Loss: 0.00014775402426797274\n",
      "Epoch [6/25], Train Loss: 0.00014483377162832767, Validation Loss: 0.0001466139052354265\n",
      "Epoch [6/25], Train Loss: 0.00014500721590593457, Validation Loss: 0.00014639128227524149\n",
      "Epoch [6/25], Train Loss: 0.00016978356870822608, Validation Loss: 0.00014687782483330619\n",
      "Epoch [6/25], Train Loss: 0.00015281871310435236, Validation Loss: 0.00014713461180993666\n",
      "Epoch [6/25], Train Loss: 0.00012996488658245653, Validation Loss: 0.00014677259290086416\n",
      "Epoch [6/25], Train Loss: 0.00014141637075226754, Validation Loss: 0.0001463211873973099\n",
      "Epoch [6/25], Train Loss: 0.00013933045556768775, Validation Loss: 0.00014635952878355358\n",
      "Epoch [6/25], Train Loss: 0.0002110860514221713, Validation Loss: 0.00014685851929243653\n",
      "Epoch [6/25], Train Loss: 0.00014864974946249276, Validation Loss: 0.00014723026858215842\n",
      "Epoch [6/25], Train Loss: 0.00016305927420035005, Validation Loss: 0.00014718622599806016\n",
      "Epoch [6/25], Train Loss: 0.0001357587898382917, Validation Loss: 0.0001468554920090052\n",
      "Epoch [6/25], Train Loss: 9.649255662225187e-05, Validation Loss: 0.00014642002521820057\n",
      "Epoch [6/25], Train Loss: 0.00015479933063033968, Validation Loss: 0.00014624732721131296\n",
      "Epoch [6/25], Train Loss: 0.00014568759070243686, Validation Loss: 0.00014625114563386886\n",
      "Epoch [6/25], Train Loss: 0.0001773122203303501, Validation Loss: 0.00014649446457042358\n",
      "Epoch [6/25], Train Loss: 0.00015452629304490983, Validation Loss: 0.00014681623870274051\n",
      "Epoch [6/25], Train Loss: 0.00013730389764532447, Validation Loss: 0.00014692538849582585\n",
      "Epoch [6/25], Train Loss: 0.00016283780860248953, Validation Loss: 0.00014669335262927538\n",
      "Epoch [6/25], Train Loss: 0.00015837409591767937, Validation Loss: 0.00014636194512907726\n",
      "Epoch [6/25], Train Loss: 0.0001504469255451113, Validation Loss: 0.00014621706044029754\n",
      "Epoch [6/25], Train Loss: 0.00014642736641690135, Validation Loss: 0.00014625559269916267\n",
      "Epoch [6/25], Train Loss: 0.00021726566774304956, Validation Loss: 0.0001463271900623416\n",
      "Epoch [6/25], Train Loss: 9.113681153394282e-05, Validation Loss: 0.0001465503009967506\n",
      "Epoch [6/25], Train Loss: 0.00011436829663580284, Validation Loss: 0.0001468656160189615\n",
      "Epoch [6/25], Train Loss: 0.00019268413598183542, Validation Loss: 0.00014715512152179143\n",
      "Epoch [6/25], Train Loss: 0.00017666180792730302, Validation Loss: 0.00014746173086071696\n",
      "Epoch [6/25], Train Loss: 0.00016439273895230144, Validation Loss: 0.0001475528226971316\n",
      "Epoch [6/25], Train Loss: 0.00019635552598629147, Validation Loss: 0.0001473029258098298\n",
      "Epoch [6/25], Train Loss: 0.00013664115977007896, Validation Loss: 0.00014681600102145847\n",
      "Epoch [6/25], Train Loss: 0.00017465048586018384, Validation Loss: 0.00014637022565390604\n",
      "Epoch [6/25], Train Loss: 0.0001720207219477743, Validation Loss: 0.00014623128275464598\n",
      "Epoch [6/25], Train Loss: 0.00014359276974573731, Validation Loss: 0.0001463424045747767\n",
      "Epoch [6/25], Train Loss: 0.00016860970936249942, Validation Loss: 0.00014659736431591833\n",
      "Epoch [6/25], Train Loss: 0.00018217775505036116, Validation Loss: 0.00014682595719932577\n",
      "Epoch [6/25], Train Loss: 0.00010028601536760107, Validation Loss: 0.00014682247953411813\n",
      "Epoch [6/25], Train Loss: 0.0001483006781199947, Validation Loss: 0.00014682824694318698\n",
      "Epoch [6/25], Train Loss: 0.0002133831731043756, Validation Loss: 0.00014716081180570956\n",
      "Epoch [6/25], Train Loss: 0.00019741078722290695, Validation Loss: 0.00014746049006741184\n",
      "Epoch [6/25], Train Loss: 0.00017360434867441654, Validation Loss: 0.00014778509381964492\n",
      "Epoch [6/25], Train Loss: 0.00012520291784312576, Validation Loss: 0.00014778794648009352\n",
      "Epoch [6/25], Train Loss: 0.0001601335097802803, Validation Loss: 0.0001474691942955057\n",
      "Epoch [6/25], Train Loss: 0.00013862857304047793, Validation Loss: 0.0001468814538384322\n",
      "Epoch [6/25], Train Loss: 0.0001263931771973148, Validation Loss: 0.00014639730919346523\n",
      "Epoch [6/25], Train Loss: 0.00012610999692697078, Validation Loss: 0.00014619434732594528\n",
      "Epoch [6/25], Train Loss: 0.000155235335114412, Validation Loss: 0.00014631096782977693\n",
      "Epoch [6/25], Train Loss: 0.00018081192683894187, Validation Loss: 0.00014665296863919746\n",
      "Epoch [6/25], Train Loss: 0.00011588416964514181, Validation Loss: 0.00014698103404953144\n",
      "Epoch [6/25], Train Loss: 0.00014054546772968024, Validation Loss: 0.00014731859458455194\n",
      "Epoch [6/25], Train Loss: 0.0001055064785759896, Validation Loss: 0.0001474117624942058\n",
      "Epoch [6/25], Train Loss: 0.0001455229357816279, Validation Loss: 0.00014731664317271982\n",
      "Epoch [6/25], Train Loss: 0.00025114769232459366, Validation Loss: 0.0001470847360906191\n",
      "Epoch [6/25], Train Loss: 9.38341036089696e-05, Validation Loss: 0.00014668719377368687\n",
      "Epoch [6/25], Train Loss: 0.00016945571405813098, Validation Loss: 0.00014631869780714623\n",
      "Epoch [6/25], Train Loss: 0.00017806085816118866, Validation Loss: 0.00014612836409166145\n",
      "Epoch [6/25], Train Loss: 0.00019319723651278764, Validation Loss: 0.00014629827904476164\n",
      "Epoch [6/25], Train Loss: 0.00014855984773021191, Validation Loss: 0.00014650312271745255\n",
      "Epoch [6/25], Train Loss: 0.00021077792916912585, Validation Loss: 0.0001469355784744645\n",
      "Epoch [6/25], Train Loss: 0.00016000161122065037, Validation Loss: 0.00014804215607000515\n",
      "Epoch [6/25], Train Loss: 0.00019474064174573869, Validation Loss: 0.00014891104049941835\n",
      "Epoch [6/25], Train Loss: 0.00016494504234287888, Validation Loss: 0.00014909857054590248\n",
      "Epoch [6/25], Train Loss: 0.0001284257014049217, Validation Loss: 0.00014881954945546264\n",
      "Epoch [6/25], Train Loss: 0.0001816186704672873, Validation Loss: 0.00014811136643402278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], Train Loss: 0.000189326805411838, Validation Loss: 0.00014683285238182482\n",
      "Epoch [6/25], Train Loss: 0.00012023326416965574, Validation Loss: 0.00014622393039947685\n",
      "Epoch [6/25], Train Loss: 0.0001458666374674067, Validation Loss: 0.0001465496449479057\n",
      "Epoch [6/25], Train Loss: 0.00022958182671573013, Validation Loss: 0.00014762406499357894\n",
      "Epoch [6/25], Train Loss: 0.00010248930630041286, Validation Loss: 0.00014905226222860317\n",
      "Epoch [6/25], Train Loss: 0.00021955804550088942, Validation Loss: 0.0001504412010641924\n",
      "Epoch [6/25], Train Loss: 0.00010976908379234374, Validation Loss: 0.00015054317846079356\n",
      "Epoch [6/25], Train Loss: 0.00013531252625398338, Validation Loss: 0.00014950591648812405\n",
      "Epoch [6/25], Train Loss: 0.00016427574155386537, Validation Loss: 0.0001477399569315215\n",
      "Epoch [6/25], Train Loss: 0.00015898223500698805, Validation Loss: 0.00014645513680685933\n",
      "Epoch [6/25], Train Loss: 0.00014468308654613793, Validation Loss: 0.00014623742924110654\n",
      "Epoch [6/25], Train Loss: 0.00019218289526179433, Validation Loss: 0.00014716597506776453\n",
      "Epoch [6/25], Train Loss: 0.00010518364433664829, Validation Loss: 0.00014826537517365068\n",
      "Epoch [6/25], Train Loss: 0.0002069992187898606, Validation Loss: 0.00014847155883520222\n",
      "Epoch [6/25], Train Loss: 0.00010866797674680129, Validation Loss: 0.0001478225819785924\n",
      "Epoch [6/25], Train Loss: 0.00013435592700261623, Validation Loss: 0.00014717871114650432\n",
      "Epoch [6/25], Train Loss: 0.0001197488236357458, Validation Loss: 0.0001465064570462952\n",
      "Epoch [6/25], Train Loss: 0.00016165564011316746, Validation Loss: 0.00014617509392943854\n",
      "Epoch [6/25], Train Loss: 0.00016394173144362867, Validation Loss: 0.00014633595225556443\n",
      "Epoch [6/25], Train Loss: 0.0001817681040847674, Validation Loss: 0.0001466833947536846\n",
      "Epoch [6/25], Train Loss: 9.07593421288766e-05, Validation Loss: 0.00014675126682656508\n",
      "Epoch [6/25], Train Loss: 0.0001661685382714495, Validation Loss: 0.00014650946686742828\n",
      "Epoch [6/25], Train Loss: 0.00017144979210570455, Validation Loss: 0.00014622086455347018\n",
      "Epoch [6/25], Train Loss: 0.00010703330190153793, Validation Loss: 0.00014609343973764528\n",
      "Epoch [6/25], Train Loss: 0.00011194939725100994, Validation Loss: 0.0001461415486119222\n",
      "Epoch [6/25], Train Loss: 0.00019190082093700767, Validation Loss: 0.00014631535741500558\n",
      "Epoch [6/25], Train Loss: 0.00019227623124606907, Validation Loss: 0.00014650074493450423\n",
      "Epoch [6/25], Train Loss: 0.0001522006350569427, Validation Loss: 0.00014650050118992415\n",
      "Epoch [6/25], Train Loss: 0.00016045317170210183, Validation Loss: 0.0001465755546329698\n",
      "Epoch [6/25], Train Loss: 0.00012926201452501118, Validation Loss: 0.00014666156760843781\n",
      "Epoch [6/25], Train Loss: 0.00018533377442508936, Validation Loss: 0.00014668024644682494\n",
      "Epoch [6/25], Train Loss: 9.566616790834814e-05, Validation Loss: 0.00014661686776283508\n",
      "Epoch [6/25], Train Loss: 0.00013174604100640863, Validation Loss: 0.00014647582599233526\n",
      "Epoch [6/25], Train Loss: 0.00012281966337468475, Validation Loss: 0.0001461973115510773\n",
      "Epoch [6/25], Train Loss: 0.0002151112275896594, Validation Loss: 0.00014595625058670217\n",
      "Epoch [6/25], Train Loss: 0.00014162131992634386, Validation Loss: 0.00014595214694660777\n",
      "Epoch [6/25], Train Loss: 0.00016341262380592525, Validation Loss: 0.00014610913446328294\n",
      "Epoch [6/25], Train Loss: 0.0001637527166167274, Validation Loss: 0.00014639622434818497\n",
      "Epoch [6/25], Train Loss: 0.00016057676111813635, Validation Loss: 0.0001469181600744681\n",
      "Epoch [6/25], Train Loss: 0.00018297112546861172, Validation Loss: 0.00014834035998016284\n",
      "Epoch [6/25], Train Loss: 0.00014728811220265925, Validation Loss: 0.00015073092411815498\n",
      "Epoch [6/25], Train Loss: 0.00019940828497055918, Validation Loss: 0.0001535711417091079\n",
      "Epoch [6/25], Train Loss: 0.0002049176109721884, Validation Loss: 0.00015547370445953372\n",
      "Epoch [6/25], Train Loss: 0.00029050593730062246, Validation Loss: 0.0001530423234119856\n",
      "Epoch [7/25], Train Loss: 0.00017057472723536193, Validation Loss: 0.00014894962126466756\n",
      "Epoch [7/25], Train Loss: 0.00018995892605744302, Validation Loss: 0.00014625925590128948\n",
      "Epoch [7/25], Train Loss: 8.927578164730221e-05, Validation Loss: 0.00014799181493193222\n",
      "Epoch [7/25], Train Loss: 0.00011113318760180846, Validation Loss: 0.00015105380007298663\n",
      "Epoch [7/25], Train Loss: 0.00014609488425776362, Validation Loss: 0.00015185592104292785\n",
      "Epoch [7/25], Train Loss: 0.0002452325134072453, Validation Loss: 0.00014894980971196976\n",
      "Epoch [7/25], Train Loss: 0.0001733884128043428, Validation Loss: 0.00014638890643254855\n",
      "Epoch [7/25], Train Loss: 0.00021994599956087768, Validation Loss: 0.0001470054705957106\n",
      "Epoch [7/25], Train Loss: 0.00013453811698127538, Validation Loss: 0.0001496837146987673\n",
      "Epoch [7/25], Train Loss: 0.00011776000610552728, Validation Loss: 0.00015091389456453423\n",
      "Epoch [7/25], Train Loss: 0.00016745309403631836, Validation Loss: 0.000149509634259933\n",
      "Epoch [7/25], Train Loss: 0.00012361719564069062, Validation Loss: 0.00014691256340787124\n",
      "Epoch [7/25], Train Loss: 0.00015388141036964953, Validation Loss: 0.00014613414847796473\n",
      "Epoch [7/25], Train Loss: 0.00019372958922758698, Validation Loss: 0.0001469490763459665\n",
      "Epoch [7/25], Train Loss: 0.00017335714073851705, Validation Loss: 0.00014763090318107666\n",
      "Epoch [7/25], Train Loss: 0.00020229969231877476, Validation Loss: 0.00014700581596116535\n",
      "Epoch [7/25], Train Loss: 0.00014486546569969505, Validation Loss: 0.00014616067928727717\n",
      "Epoch [7/25], Train Loss: 0.00010803611075971276, Validation Loss: 0.00014630124011697868\n",
      "Epoch [7/25], Train Loss: 0.00018452669610269368, Validation Loss: 0.0001468509843107313\n",
      "Epoch [7/25], Train Loss: 0.00021093952818773687, Validation Loss: 0.00014709070286092658\n",
      "Epoch [7/25], Train Loss: 9.749754826771095e-05, Validation Loss: 0.00014668673951139983\n",
      "Epoch [7/25], Train Loss: 0.00014784348604734987, Validation Loss: 0.00014615775774776314\n",
      "Epoch [7/25], Train Loss: 0.00013922606012783945, Validation Loss: 0.00014592465231544338\n",
      "Epoch [7/25], Train Loss: 0.00010652709170244634, Validation Loss: 0.00014620563694431137\n",
      "Epoch [7/25], Train Loss: 0.00017041692626662552, Validation Loss: 0.0001464135872083716\n",
      "Epoch [7/25], Train Loss: 0.00017860776279121637, Validation Loss: 0.00014639593840305072\n",
      "Epoch [7/25], Train Loss: 0.00018015479145105928, Validation Loss: 0.0001462392048172963\n",
      "Epoch [7/25], Train Loss: 0.00021206711244303733, Validation Loss: 0.00014597130357287824\n",
      "Epoch [7/25], Train Loss: 0.00018753379117697477, Validation Loss: 0.0001462216920723828\n",
      "Epoch [7/25], Train Loss: 0.00014657485007774085, Validation Loss: 0.00014641296826691056\n",
      "Epoch [7/25], Train Loss: 0.000191051178262569, Validation Loss: 0.00014642696987721138\n",
      "Epoch [7/25], Train Loss: 0.00015429792983923107, Validation Loss: 0.00014626837849694615\n",
      "Epoch [7/25], Train Loss: 0.00015840079868212342, Validation Loss: 0.00014618582160134488\n",
      "Epoch [7/25], Train Loss: 0.0001315675035584718, Validation Loss: 0.00014612015972185569\n",
      "Epoch [7/25], Train Loss: 0.00010257379472022876, Validation Loss: 0.000146037521820593\n",
      "Epoch [7/25], Train Loss: 8.082461863523349e-05, Validation Loss: 0.00014595595688054647\n",
      "Epoch [7/25], Train Loss: 0.00018094606639351696, Validation Loss: 0.00014597122086949335\n",
      "Epoch [7/25], Train Loss: 0.000148276420077309, Validation Loss: 0.00014615934560424648\n",
      "Epoch [7/25], Train Loss: 8.502593118464574e-05, Validation Loss: 0.00014649473838896181\n",
      "Epoch [7/25], Train Loss: 0.0001382278569508344, Validation Loss: 0.00014677136180883584\n",
      "Epoch [7/25], Train Loss: 0.0001540788944112137, Validation Loss: 0.0001468147813284304\n",
      "Epoch [7/25], Train Loss: 0.00015262373199220747, Validation Loss: 0.00014669770850256708\n",
      "Epoch [7/25], Train Loss: 0.00013640850374940783, Validation Loss: 0.00014666023271274752\n",
      "Epoch [7/25], Train Loss: 0.00014325010124593973, Validation Loss: 0.0001467168908372211\n",
      "Epoch [7/25], Train Loss: 0.00017390705761499703, Validation Loss: 0.0001471156991707782\n",
      "Epoch [7/25], Train Loss: 0.00012743304250761867, Validation Loss: 0.00014760716245897735\n",
      "Epoch [7/25], Train Loss: 0.00016557093476876616, Validation Loss: 0.00014817205228609965\n",
      "Epoch [7/25], Train Loss: 0.00015601131599396467, Validation Loss: 0.00014814246994016383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Train Loss: 0.00011388091661501676, Validation Loss: 0.00014775747258681803\n",
      "Epoch [7/25], Train Loss: 0.0001219494515680708, Validation Loss: 0.00014697583270996498\n",
      "Epoch [7/25], Train Loss: 0.00021231644495856017, Validation Loss: 0.0001461393985664472\n",
      "Epoch [7/25], Train Loss: 9.85173464869149e-05, Validation Loss: 0.0001458412705687806\n",
      "Epoch [7/25], Train Loss: 0.00010893085709540173, Validation Loss: 0.00014606800565767722\n",
      "Epoch [7/25], Train Loss: 0.00015561399050056934, Validation Loss: 0.00014657936456690852\n",
      "Epoch [7/25], Train Loss: 0.00012210244312882423, Validation Loss: 0.00014700152605655602\n",
      "Epoch [7/25], Train Loss: 0.00021426260354928672, Validation Loss: 0.00014756656237295828\n",
      "Epoch [7/25], Train Loss: 0.00013511189899872988, Validation Loss: 0.0001475693286920432\n",
      "Epoch [7/25], Train Loss: 0.00012771676119882613, Validation Loss: 0.00014704912052062962\n",
      "Epoch [7/25], Train Loss: 0.00013625177962239832, Validation Loss: 0.00014630703832760144\n",
      "Epoch [7/25], Train Loss: 0.0001588078448548913, Validation Loss: 0.00014581468106674342\n",
      "Epoch [7/25], Train Loss: 0.00011855409684358165, Validation Loss: 0.00014586242080743734\n",
      "Epoch [7/25], Train Loss: 0.00017813108570408076, Validation Loss: 0.00014622617527493276\n",
      "Epoch [7/25], Train Loss: 0.00016353696992155164, Validation Loss: 0.000146757762801523\n",
      "Epoch [7/25], Train Loss: 0.00012122430052841082, Validation Loss: 0.00014734230620282082\n",
      "Epoch [7/25], Train Loss: 0.00019244715804234147, Validation Loss: 0.00014810370460812314\n",
      "Epoch [7/25], Train Loss: 0.00018848640320356935, Validation Loss: 0.00014834443839693753\n",
      "Epoch [7/25], Train Loss: 0.00015499925939366221, Validation Loss: 0.00014796457204890127\n",
      "Epoch [7/25], Train Loss: 0.00013852468691766262, Validation Loss: 0.00014693555737418744\n",
      "Epoch [7/25], Train Loss: 0.0001743157918099314, Validation Loss: 0.00014599588272782663\n",
      "Epoch [7/25], Train Loss: 0.00019948188855778426, Validation Loss: 0.00014588408788161663\n",
      "Epoch [7/25], Train Loss: 0.00013791702804155648, Validation Loss: 0.00014679439603545083\n",
      "Epoch [7/25], Train Loss: 0.0002082436840282753, Validation Loss: 0.00014827179450852175\n",
      "Epoch [7/25], Train Loss: 0.0001900445349747315, Validation Loss: 0.00014942222987883723\n",
      "Epoch [7/25], Train Loss: 0.00021167198428884149, Validation Loss: 0.00015071898354411436\n",
      "Epoch [7/25], Train Loss: 0.00013462701463140547, Validation Loss: 0.00015054798592852118\n",
      "Epoch [7/25], Train Loss: 0.0001172303018392995, Validation Loss: 0.00014902027081310128\n",
      "Epoch [7/25], Train Loss: 0.0001865788217401132, Validation Loss: 0.00014697324659209698\n",
      "Epoch [7/25], Train Loss: 0.00015011083451099694, Validation Loss: 0.00014585147194641954\n",
      "Epoch [7/25], Train Loss: 0.00016440209583379328, Validation Loss: 0.00014614505586602416\n",
      "Epoch [7/25], Train Loss: 0.0001371076941723004, Validation Loss: 0.00014743865128063287\n",
      "Epoch [7/25], Train Loss: 0.00014028933946974576, Validation Loss: 0.00014884381574423362\n",
      "Epoch [7/25], Train Loss: 0.00018002628348767757, Validation Loss: 0.00014864937305295218\n",
      "Epoch [7/25], Train Loss: 0.0002065252629108727, Validation Loss: 0.00014810750362812542\n",
      "Epoch [7/25], Train Loss: 0.00017601803119760007, Validation Loss: 0.00014668083531432785\n",
      "Epoch [7/25], Train Loss: 0.0001618298119865358, Validation Loss: 0.00014590589998988434\n",
      "Epoch [7/25], Train Loss: 0.00019868269737344235, Validation Loss: 0.00014625533513026311\n",
      "Epoch [7/25], Train Loss: 0.00010719018609961495, Validation Loss: 0.00014690671475060905\n",
      "Epoch [7/25], Train Loss: 0.0001560649834573269, Validation Loss: 0.00014706136280437932\n",
      "Epoch [7/25], Train Loss: 0.0002164870238630101, Validation Loss: 0.0001465014376056691\n",
      "Epoch [7/25], Train Loss: 0.00017146801110357046, Validation Loss: 0.0001460086246273325\n",
      "Epoch [7/25], Train Loss: 0.00011373124289093539, Validation Loss: 0.00014579522636874268\n",
      "Epoch [7/25], Train Loss: 0.00013708308688364923, Validation Loss: 0.0001457405072869733\n",
      "Epoch [7/25], Train Loss: 0.0001163097404059954, Validation Loss: 0.00014577648738243928\n",
      "Epoch [7/25], Train Loss: 0.00015555978461634368, Validation Loss: 0.00014584966168816512\n",
      "Epoch [7/25], Train Loss: 0.0001872128777904436, Validation Loss: 0.00014592108952153163\n",
      "Epoch [7/25], Train Loss: 0.00021042293519712985, Validation Loss: 0.00014586520919692702\n",
      "Epoch [7/25], Train Loss: 0.000191748738870956, Validation Loss: 0.00014575790846720337\n",
      "Epoch [7/25], Train Loss: 0.00011281810293439776, Validation Loss: 0.00014565855308319443\n",
      "Epoch [7/25], Train Loss: 0.00015291110321413726, Validation Loss: 0.00014561960706487297\n",
      "Epoch [7/25], Train Loss: 0.00015576915757264942, Validation Loss: 0.00014562745054718106\n",
      "Epoch [7/25], Train Loss: 0.00015460094437003136, Validation Loss: 0.00014567812152866584\n",
      "Epoch [7/25], Train Loss: 0.00012664514360949397, Validation Loss: 0.00014579873216765313\n",
      "Epoch [7/25], Train Loss: 0.00015457856352441013, Validation Loss: 0.0001461351564406262\n",
      "Epoch [7/25], Train Loss: 0.0002300828491570428, Validation Loss: 0.00014724310143113446\n",
      "Epoch [7/25], Train Loss: 0.0001548989093862474, Validation Loss: 0.00014905272158406054\n",
      "Epoch [7/25], Train Loss: 0.00016171063180081546, Validation Loss: 0.00015113391855265946\n",
      "Epoch [7/25], Train Loss: 0.00012083669571438804, Validation Loss: 0.00015229918935801833\n",
      "Epoch [7/25], Train Loss: 0.00015319939120672643, Validation Loss: 0.00015194458052671204\n",
      "Epoch [7/25], Train Loss: 0.00014211225789040327, Validation Loss: 0.0001495530212802502\n",
      "Epoch [7/25], Train Loss: 0.00010449021647218615, Validation Loss: 0.00014713985413739768\n",
      "Epoch [7/25], Train Loss: 0.00017250096425414085, Validation Loss: 0.00014576882264615658\n",
      "Epoch [7/25], Train Loss: 0.00014058353553991765, Validation Loss: 0.00014622051991561118\n",
      "Epoch [7/25], Train Loss: 0.00011368044215487316, Validation Loss: 0.00014760519261471927\n",
      "Epoch [7/25], Train Loss: 0.00012517496361397207, Validation Loss: 0.00014866764007213836\n",
      "Epoch [7/25], Train Loss: 0.00013639307871926576, Validation Loss: 0.00014837441922281868\n",
      "Epoch [7/25], Train Loss: 9.104501805268228e-05, Validation Loss: 0.00014743395028441833\n",
      "Epoch [7/25], Train Loss: 9.509607480140403e-05, Validation Loss: 0.00014593996966141276\n",
      "Epoch [7/25], Train Loss: 0.00019783922471106052, Validation Loss: 0.00014588106472122794\n",
      "Epoch [7/25], Train Loss: 0.00011177173291798681, Validation Loss: 0.00014723061928331542\n",
      "Epoch [7/25], Train Loss: 0.00023090456670615822, Validation Loss: 0.00014818761022373413\n",
      "Epoch [7/25], Train Loss: 0.00018615640874486417, Validation Loss: 0.00014715993772066818\n",
      "Epoch [7/25], Train Loss: 0.00014274880231823772, Validation Loss: 0.000145866435195785\n",
      "Epoch [7/25], Train Loss: 0.00012674923345912248, Validation Loss: 0.0001458159424752618\n",
      "Epoch [7/25], Train Loss: 0.0001317392598139122, Validation Loss: 0.0001467243103737322\n",
      "Epoch [7/25], Train Loss: 0.00017425863188691437, Validation Loss: 0.00014743581632501446\n",
      "Epoch [7/25], Train Loss: 9.694525942904875e-05, Validation Loss: 0.0001473089663098411\n",
      "Epoch [7/25], Train Loss: 0.0002532268699724227, Validation Loss: 0.00014702320404467173\n",
      "Epoch [7/25], Train Loss: 0.00013902514183428138, Validation Loss: 0.00014612784386069204\n",
      "Epoch [7/25], Train Loss: 0.0001787748624337837, Validation Loss: 0.00014564729911702065\n",
      "Epoch [7/25], Train Loss: 0.0001752008101902902, Validation Loss: 0.00014607073972001673\n",
      "Epoch [7/25], Train Loss: 0.00017393127200193703, Validation Loss: 0.0001467783282957195\n",
      "Epoch [7/25], Train Loss: 0.00017797626787796617, Validation Loss: 0.00014708594535477458\n",
      "Epoch [7/25], Train Loss: 0.00016283737204503268, Validation Loss: 0.00014680022820054244\n",
      "Epoch [7/25], Train Loss: 0.00011872214236063883, Validation Loss: 0.00014641891757491977\n",
      "Epoch [7/25], Train Loss: 0.00015692428860347718, Validation Loss: 0.0001458844393103694\n",
      "Epoch [7/25], Train Loss: 0.00017855048645287752, Validation Loss: 0.0001455949796460724\n",
      "Epoch [7/25], Train Loss: 0.0001472323783673346, Validation Loss: 0.00014559262272086925\n",
      "Epoch [7/25], Train Loss: 9.940865129465237e-05, Validation Loss: 0.00014577003991386543\n",
      "Epoch [7/25], Train Loss: 0.00015507126227021217, Validation Loss: 0.00014602965845066744\n",
      "Epoch [7/25], Train Loss: 0.0001726192276692018, Validation Loss: 0.00014612890590797177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Train Loss: 0.00016862660413607955, Validation Loss: 0.00014600556848260265\n",
      "Epoch [7/25], Train Loss: 0.000193250467418693, Validation Loss: 0.00014582200698593322\n",
      "Epoch [7/25], Train Loss: 0.00017025017587002367, Validation Loss: 0.00014563439205327692\n",
      "Epoch [7/25], Train Loss: 0.0001730356743792072, Validation Loss: 0.0001455033120388786\n",
      "Epoch [7/25], Train Loss: 0.0001522352104075253, Validation Loss: 0.00014556555906892755\n",
      "Epoch [7/25], Train Loss: 0.00013740551366936415, Validation Loss: 0.00014570114168842944\n",
      "Epoch [7/25], Train Loss: 0.00013872564886696637, Validation Loss: 0.00014581227660528384\n",
      "Epoch [7/25], Train Loss: 0.00012994333519600332, Validation Loss: 0.00014590573991881683\n",
      "Epoch [7/25], Train Loss: 0.00019775786495301872, Validation Loss: 0.00014604743604043808\n",
      "Epoch [7/25], Train Loss: 0.00021604931680485606, Validation Loss: 0.00014620545456030717\n",
      "Epoch [7/25], Train Loss: 0.0001601229450898245, Validation Loss: 0.00014626550352356087\n",
      "Epoch [7/25], Train Loss: 9.887736814562231e-05, Validation Loss: 0.00014628391630443124\n",
      "Epoch [7/25], Train Loss: 0.0001664948940742761, Validation Loss: 0.0001461617139284499\n",
      "Epoch [7/25], Train Loss: 0.0001959888031706214, Validation Loss: 0.00014604177219249928\n",
      "Epoch [7/25], Train Loss: 0.00013449002290144563, Validation Loss: 0.0001458882208680734\n",
      "Epoch [7/25], Train Loss: 0.00017992906214203686, Validation Loss: 0.00014577471277637716\n",
      "Epoch [7/25], Train Loss: 0.0001733152603264898, Validation Loss: 0.00014558748274187867\n",
      "Epoch [7/25], Train Loss: 0.00011196603009011596, Validation Loss: 0.00014544446118331205\n",
      "Epoch [7/25], Train Loss: 0.0001966945274034515, Validation Loss: 0.0001455151978007052\n",
      "Epoch [7/25], Train Loss: 0.00012607562530320138, Validation Loss: 0.00014581565604506372\n",
      "Epoch [7/25], Train Loss: 0.00015337162767536938, Validation Loss: 0.00014620501096942461\n",
      "Epoch [7/25], Train Loss: 0.00012504885671660304, Validation Loss: 0.00014663848536050256\n",
      "Epoch [7/25], Train Loss: 0.00014809441927354783, Validation Loss: 0.0001468486564893586\n",
      "Epoch [7/25], Train Loss: 0.00020590717031154782, Validation Loss: 0.00014676954694247494\n",
      "Epoch [7/25], Train Loss: 0.00015678099589422345, Validation Loss: 0.0001465511527688553\n",
      "Epoch [7/25], Train Loss: 0.00019749626517295837, Validation Loss: 0.00014622436622933795\n",
      "Epoch [7/25], Train Loss: 0.00013638958625961095, Validation Loss: 0.00014588034149104107\n",
      "Epoch [7/25], Train Loss: 0.00016609879094175994, Validation Loss: 0.0001456283583441594\n",
      "Epoch [7/25], Train Loss: 0.00014428705617319793, Validation Loss: 0.00014548098867332251\n",
      "Epoch [7/25], Train Loss: 0.00013238174142315984, Validation Loss: 0.0001454518470078862\n",
      "Epoch [7/25], Train Loss: 0.00017777767789084464, Validation Loss: 0.0001455456932793216\n",
      "Epoch [7/25], Train Loss: 0.00014454126358032227, Validation Loss: 0.00014576442602750225\n",
      "Epoch [7/25], Train Loss: 0.0001339353621006012, Validation Loss: 0.0001460424757776006\n",
      "Epoch [7/25], Train Loss: 0.00018889318744186312, Validation Loss: 0.0001462727387358124\n",
      "Epoch [7/25], Train Loss: 0.00015659235941711813, Validation Loss: 0.00014631724091789994\n",
      "Epoch [7/25], Train Loss: 0.00011572500079637393, Validation Loss: 0.00014614942701882683\n",
      "Epoch [7/25], Train Loss: 0.00015287894348148257, Validation Loss: 0.00014591294093406758\n",
      "Epoch [7/25], Train Loss: 0.00016188643348868936, Validation Loss: 0.00014574432570952923\n",
      "Epoch [7/25], Train Loss: 0.0001348669029539451, Validation Loss: 0.0001456151883758139\n",
      "Epoch [7/25], Train Loss: 0.00015793302736710757, Validation Loss: 0.00014555256639141588\n",
      "Epoch [7/25], Train Loss: 0.0001754848490236327, Validation Loss: 0.00014545075658437174\n",
      "Epoch [7/25], Train Loss: 0.00013227411545813084, Validation Loss: 0.0001454202487366274\n",
      "Epoch [7/25], Train Loss: 0.00018919644935522228, Validation Loss: 0.0001456352469782966\n",
      "Epoch [7/25], Train Loss: 0.00011997030378552154, Validation Loss: 0.00014608366982429288\n",
      "Epoch [7/25], Train Loss: 0.00021625214139930904, Validation Loss: 0.00014649372654578958\n",
      "Epoch [7/25], Train Loss: 0.00019101149518974125, Validation Loss: 0.00014700987570298213\n",
      "Epoch [7/25], Train Loss: 0.00017080179532058537, Validation Loss: 0.00014769414774491453\n",
      "Epoch [7/25], Train Loss: 0.00016797901480458677, Validation Loss: 0.00014825938051217237\n",
      "Epoch [7/25], Train Loss: 0.00016627028526272625, Validation Loss: 0.00014954838504005846\n",
      "Epoch [7/25], Train Loss: 0.00020589318592101336, Validation Loss: 0.00015158723132723632\n",
      "Epoch [7/25], Train Loss: 0.00019413446716498584, Validation Loss: 0.00015473126962509316\n",
      "Epoch [7/25], Train Loss: 0.00010831144754774868, Validation Loss: 0.00015656203249818644\n",
      "Epoch [7/25], Train Loss: 0.0002383576356805861, Validation Loss: 0.0001546769441726307\n",
      "Epoch [7/25], Train Loss: 0.00013557147758547217, Validation Loss: 0.0001498027653724421\n",
      "Epoch [7/25], Train Loss: 0.00015774134953971952, Validation Loss: 0.0001462163512769621\n",
      "Epoch [7/25], Train Loss: 0.0001865572266979143, Validation Loss: 0.00014757775788893924\n",
      "Epoch [7/25], Train Loss: 0.00019393506227061152, Validation Loss: 0.0001507726228737738\n",
      "Epoch [7/25], Train Loss: 0.00016219739336520433, Validation Loss: 0.00015062114252941682\n",
      "Epoch [7/25], Train Loss: 0.0001817386073525995, Validation Loss: 0.00014772396146630248\n",
      "Epoch [7/25], Train Loss: 0.00014760058547835797, Validation Loss: 0.0001460753048983558\n",
      "Epoch [7/25], Train Loss: 0.0001657911780057475, Validation Loss: 0.00014723451871153279\n",
      "Epoch [7/25], Train Loss: 0.00013604383275378495, Validation Loss: 0.00014885371638229117\n",
      "Epoch [7/25], Train Loss: 0.00014953839126974344, Validation Loss: 0.00014826263262269397\n",
      "Epoch [7/25], Train Loss: 0.00015027401968836784, Validation Loss: 0.00014624882824136874\n",
      "Epoch [7/25], Train Loss: 0.00015710467414464802, Validation Loss: 0.0001461762602654441\n",
      "Epoch [7/25], Train Loss: 0.00013344614126253873, Validation Loss: 0.00014767591637792065\n",
      "Epoch [7/25], Train Loss: 0.000180329370778054, Validation Loss: 0.0001474214443684711\n",
      "Epoch [7/25], Train Loss: 0.00011692099360516295, Validation Loss: 0.0001459741179132834\n",
      "Epoch [7/25], Train Loss: 0.00011851348972413689, Validation Loss: 0.0001456871611784057\n",
      "Epoch [7/25], Train Loss: 0.00013408719678409398, Validation Loss: 0.00014662525233385775\n",
      "Epoch [7/25], Train Loss: 0.00016352195234503597, Validation Loss: 0.00014782940367391955\n",
      "Epoch [7/25], Train Loss: 0.00020519763347692788, Validation Loss: 0.0001474825548939407\n",
      "Epoch [7/25], Train Loss: 0.0001480861974414438, Validation Loss: 0.00014671559571676577\n",
      "Epoch [7/25], Train Loss: 0.0001331866078544408, Validation Loss: 0.0001457097382323506\n",
      "Epoch [7/25], Train Loss: 0.00017291202675551176, Validation Loss: 0.0001455371847744876\n",
      "Epoch [7/25], Train Loss: 0.00014063538401387632, Validation Loss: 0.0001460937715213125\n",
      "Epoch [7/25], Train Loss: 0.00021784666751045734, Validation Loss: 0.00014637652420788073\n",
      "Epoch [7/25], Train Loss: 0.000170369356055744, Validation Loss: 0.000146122564910911\n",
      "Epoch [7/25], Train Loss: 0.0002144820027751848, Validation Loss: 0.00014571888411107163\n",
      "Epoch [7/25], Train Loss: 0.00012850404891651124, Validation Loss: 0.00014544008420974325\n",
      "Epoch [7/25], Train Loss: 0.0002128722408087924, Validation Loss: 0.00014559410313571183\n",
      "Epoch [7/25], Train Loss: 0.000168932368978858, Validation Loss: 0.00014626886574357437\n",
      "Epoch [7/25], Train Loss: 0.00013619491073768586, Validation Loss: 0.00014727714975985387\n",
      "Epoch [7/25], Train Loss: 0.00016649930330459028, Validation Loss: 0.00014789265648384268\n",
      "Epoch [7/25], Train Loss: 0.00015929316577967256, Validation Loss: 0.00014798974419439522\n",
      "Epoch [7/25], Train Loss: 0.000183591473614797, Validation Loss: 0.00014755896190763452\n",
      "Epoch [7/25], Train Loss: 0.00017125866725109518, Validation Loss: 0.00014659539228887298\n",
      "Epoch [7/25], Train Loss: 0.00019402946054469794, Validation Loss: 0.00014577292046548488\n",
      "Epoch [7/25], Train Loss: 0.0001955530751729384, Validation Loss: 0.00014537177557940594\n",
      "Epoch [7/25], Train Loss: 0.00014681363245472312, Validation Loss: 0.00014570504160171064\n",
      "Epoch [7/25], Train Loss: 0.00014434168406296521, Validation Loss: 0.00014651012534159236\n",
      "Epoch [7/25], Train Loss: 0.00012391556811053306, Validation Loss: 0.00014738381335822244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Train Loss: 0.00010436372394906357, Validation Loss: 0.00014795667617969837\n",
      "Epoch [7/25], Train Loss: 0.0001592464977875352, Validation Loss: 0.00014846148502935345\n",
      "Epoch [7/25], Train Loss: 0.00014942655980121344, Validation Loss: 0.00014799493947066368\n",
      "Epoch [7/25], Train Loss: 0.00012852060899604112, Validation Loss: 0.0001471122850489337\n",
      "Epoch [7/25], Train Loss: 0.00017312630370724946, Validation Loss: 0.00014597386883300107\n",
      "Epoch [7/25], Train Loss: 0.00012934916594531387, Validation Loss: 0.00014539997282554397\n",
      "Epoch [7/25], Train Loss: 0.00016237188538070768, Validation Loss: 0.000145538772388439\n",
      "Epoch [7/25], Train Loss: 9.091412357520312e-05, Validation Loss: 0.0001460815791991384\n",
      "Epoch [7/25], Train Loss: 0.0001945667463587597, Validation Loss: 0.00014668304502265527\n",
      "Epoch [7/25], Train Loss: 0.0001703444286249578, Validation Loss: 0.00014653812662193861\n",
      "Epoch [7/25], Train Loss: 0.00020618281268980354, Validation Loss: 0.00014657498274270135\n",
      "Epoch [7/25], Train Loss: 0.0001842642668634653, Validation Loss: 0.00014612798986490816\n",
      "Epoch [7/25], Train Loss: 0.0002281179913552478, Validation Loss: 0.00014567590963755113\n",
      "Epoch [7/25], Train Loss: 0.00014678513980470598, Validation Loss: 0.00014530342426345063\n",
      "Epoch [7/25], Train Loss: 0.00018350138270761818, Validation Loss: 0.0001452889780921396\n",
      "Epoch [7/25], Train Loss: 0.00012884271563962102, Validation Loss: 0.00014547825024540845\n",
      "Epoch [7/25], Train Loss: 0.00013395727728493512, Validation Loss: 0.00014566506142728032\n",
      "Epoch [7/25], Train Loss: 0.00014286006626207381, Validation Loss: 0.00014589630930762117\n",
      "Epoch [7/25], Train Loss: 0.00018736471247393638, Validation Loss: 0.0001458380507150044\n",
      "Epoch [7/25], Train Loss: 0.00010637694504112005, Validation Loss: 0.00014550636672841695\n",
      "Epoch [7/25], Train Loss: 0.00011277214798610657, Validation Loss: 0.00014525671698114215\n",
      "Epoch [7/25], Train Loss: 0.00014782110520172864, Validation Loss: 0.00014524681804080805\n",
      "Epoch [7/25], Train Loss: 0.00017873334581963718, Validation Loss: 0.000145390369289089\n",
      "Epoch [7/25], Train Loss: 0.00011136463581351563, Validation Loss: 0.00014571601980909084\n",
      "Epoch [7/25], Train Loss: 0.00010486078099347651, Validation Loss: 0.00014600375046332678\n",
      "Epoch [7/25], Train Loss: 0.00014566532627213746, Validation Loss: 0.00014614527972298675\n",
      "Epoch [7/25], Train Loss: 0.00017925113206729293, Validation Loss: 0.0001464215825156619\n",
      "Epoch [7/25], Train Loss: 6.952302646823227e-05, Validation Loss: 0.00014670366290374658\n",
      "Epoch [7/25], Train Loss: 0.0001407379750162363, Validation Loss: 0.00014657477853082432\n",
      "Epoch [7/25], Train Loss: 0.00010464483057148755, Validation Loss: 0.00014600576056788365\n",
      "Epoch [7/25], Train Loss: 0.00021575039136223495, Validation Loss: 0.00014541739049794462\n",
      "Epoch [7/25], Train Loss: 0.00017671969544608146, Validation Loss: 0.00014520079275825992\n",
      "Epoch [7/25], Train Loss: 0.0001459228660678491, Validation Loss: 0.00014529847539961337\n",
      "Epoch [7/25], Train Loss: 0.000148916631587781, Validation Loss: 0.0001455615083007918\n",
      "Epoch [7/25], Train Loss: 0.0001906357065308839, Validation Loss: 0.0001458269599728131\n",
      "Epoch [7/25], Train Loss: 0.00018021653522737324, Validation Loss: 0.00014628283291434247\n",
      "Epoch [8/25], Train Loss: 0.00017815969476941973, Validation Loss: 0.0001469324769762655\n",
      "Epoch [8/25], Train Loss: 0.0001740026636980474, Validation Loss: 0.00014873343485911997\n",
      "Epoch [8/25], Train Loss: 0.00018084555631503463, Validation Loss: 0.00015057363658949424\n",
      "Epoch [8/25], Train Loss: 0.00010790335363708436, Validation Loss: 0.00015129140083445237\n",
      "Epoch [8/25], Train Loss: 0.00014553562505170703, Validation Loss: 0.00014966637463658116\n",
      "Epoch [8/25], Train Loss: 0.00013476637832354754, Validation Loss: 0.00014699708990519866\n",
      "Epoch [8/25], Train Loss: 0.0001452289434382692, Validation Loss: 0.0001455235816441321\n",
      "Epoch [8/25], Train Loss: 0.0001886702375486493, Validation Loss: 0.00014640056833741254\n",
      "Epoch [8/25], Train Loss: 0.00011381986405467615, Validation Loss: 0.0001483387743064668\n",
      "Epoch [8/25], Train Loss: 0.0001772260875441134, Validation Loss: 0.00014944458816898987\n",
      "Epoch [8/25], Train Loss: 0.00010712821676861495, Validation Loss: 0.0001487998398564135\n",
      "Epoch [8/25], Train Loss: 0.00024363720149267465, Validation Loss: 0.00014655780566196578\n",
      "Epoch [8/25], Train Loss: 0.00019624723063316196, Validation Loss: 0.00014557244600534128\n",
      "Epoch [8/25], Train Loss: 0.00013904507795814425, Validation Loss: 0.00014555756424670108\n",
      "Epoch [8/25], Train Loss: 0.00017515206127427518, Validation Loss: 0.00014614275860367343\n",
      "Epoch [8/25], Train Loss: 0.0001812843547668308, Validation Loss: 0.00014637811570234288\n",
      "Epoch [8/25], Train Loss: 0.00015865454042796046, Validation Loss: 0.0001459068686623747\n",
      "Epoch [8/25], Train Loss: 0.00022217621153686196, Validation Loss: 0.0001453801278936832\n",
      "Epoch [8/25], Train Loss: 0.00015956931747496128, Validation Loss: 0.0001454968674806878\n",
      "Epoch [8/25], Train Loss: 0.00017045775894075632, Validation Loss: 0.00014589373725660456\n",
      "Epoch [8/25], Train Loss: 0.00013864587526768446, Validation Loss: 0.00014594727059981475\n",
      "Epoch [8/25], Train Loss: 0.0001723431923892349, Validation Loss: 0.00014590502323699183\n",
      "Epoch [8/25], Train Loss: 0.00016216722724493593, Validation Loss: 0.00014606634819453273\n",
      "Epoch [8/25], Train Loss: 0.00016808579675853252, Validation Loss: 0.0001458762698651602\n",
      "Epoch [8/25], Train Loss: 0.0001567048457218334, Validation Loss: 0.00014573842029979762\n",
      "Epoch [8/25], Train Loss: 0.00017906703578773886, Validation Loss: 0.00014553404010560675\n",
      "Epoch [8/25], Train Loss: 0.00017232177197001874, Validation Loss: 0.00014546360810830568\n",
      "Epoch [8/25], Train Loss: 0.00016107909323181957, Validation Loss: 0.00014528321795902836\n",
      "Epoch [8/25], Train Loss: 0.00017168924387078732, Validation Loss: 0.0001451920131027388\n",
      "Epoch [8/25], Train Loss: 0.00012440179125405848, Validation Loss: 0.00014534314856670487\n",
      "Epoch [8/25], Train Loss: 0.00012671173317357898, Validation Loss: 0.00014561154287851726\n",
      "Epoch [8/25], Train Loss: 0.00017532969650346786, Validation Loss: 0.0001459493207221385\n",
      "Epoch [8/25], Train Loss: 0.0001682691799942404, Validation Loss: 0.0001464521606976632\n",
      "Epoch [8/25], Train Loss: 0.00012080261512892321, Validation Loss: 0.00014722232954227364\n",
      "Epoch [8/25], Train Loss: 0.0001908632693812251, Validation Loss: 0.00014787227604150152\n",
      "Epoch [8/25], Train Loss: 0.0001129081065300852, Validation Loss: 0.0001483765210044415\n",
      "Epoch [8/25], Train Loss: 0.00014137118705548346, Validation Loss: 0.00014827837197420497\n",
      "Epoch [8/25], Train Loss: 0.00012726953718811274, Validation Loss: 0.00014788349750839794\n",
      "Epoch [8/25], Train Loss: 0.00012024179159197956, Validation Loss: 0.00014680335370940157\n",
      "Epoch [8/25], Train Loss: 0.00013306099572218955, Validation Loss: 0.00014577378266646216\n",
      "Epoch [8/25], Train Loss: 0.00016327899356838316, Validation Loss: 0.00014518881265151624\n",
      "Epoch [8/25], Train Loss: 0.0001421140623278916, Validation Loss: 0.00014524628253032764\n",
      "Epoch [8/25], Train Loss: 0.00016248447354882956, Validation Loss: 0.0001457599724138466\n",
      "Epoch [8/25], Train Loss: 0.00017877349455375224, Validation Loss: 0.00014616730622947217\n",
      "Epoch [8/25], Train Loss: 0.0002017619990510866, Validation Loss: 0.0001464383569934095\n",
      "Epoch [8/25], Train Loss: 0.00016453774878755212, Validation Loss: 0.00014642012101830914\n",
      "Epoch [8/25], Train Loss: 0.0002352047013118863, Validation Loss: 0.00014718719345789091\n",
      "Epoch [8/25], Train Loss: 0.00012925582996103913, Validation Loss: 0.00014737887953136425\n",
      "Epoch [8/25], Train Loss: 0.00021139007003512233, Validation Loss: 0.00014683299232274294\n",
      "Epoch [8/25], Train Loss: 0.0001607109297765419, Validation Loss: 0.0001457040484334963\n",
      "Epoch [8/25], Train Loss: 0.00023113367205951363, Validation Loss: 0.0001452130238855413\n",
      "Epoch [8/25], Train Loss: 0.00018865738820750266, Validation Loss: 0.00014546318149465757\n",
      "Epoch [8/25], Train Loss: 0.00011279969476163387, Validation Loss: 0.00014598532264547732\n",
      "Epoch [8/25], Train Loss: 0.00015488092321902514, Validation Loss: 0.00014643962058471515\n",
      "Epoch [8/25], Train Loss: 0.00013754685642197728, Validation Loss: 0.00014664791597169825\n",
      "Epoch [8/25], Train Loss: 0.0001760916056809947, Validation Loss: 0.00014702404805575498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], Train Loss: 0.00013625338033307344, Validation Loss: 0.0001471210302649221\n",
      "Epoch [8/25], Train Loss: 0.00022224037093110383, Validation Loss: 0.00014680686557161\n",
      "Epoch [8/25], Train Loss: 0.0001426042872481048, Validation Loss: 0.0001460746158651697\n",
      "Epoch [8/25], Train Loss: 0.000169226826983504, Validation Loss: 0.00014541750182009612\n",
      "Epoch [8/25], Train Loss: 0.00017162220319733024, Validation Loss: 0.00014507949696659732\n",
      "Epoch [8/25], Train Loss: 0.0001416995655745268, Validation Loss: 0.00014521126455899018\n",
      "Epoch [8/25], Train Loss: 0.0001284765312448144, Validation Loss: 0.00014552764938950227\n",
      "Epoch [8/25], Train Loss: 0.0002015023783314973, Validation Loss: 0.00014574877714039757\n",
      "Epoch [8/25], Train Loss: 0.0001519390061730519, Validation Loss: 0.00014597466943087057\n",
      "Epoch [8/25], Train Loss: 0.00015734574117232114, Validation Loss: 0.00014613889482764837\n",
      "Epoch [8/25], Train Loss: 0.00021185696823522449, Validation Loss: 0.00014672746450135796\n",
      "Epoch [8/25], Train Loss: 8.391327719436958e-05, Validation Loss: 0.00014741078242271519\n",
      "Epoch [8/25], Train Loss: 0.00015499911387450993, Validation Loss: 0.0001478805211566699\n",
      "Epoch [8/25], Train Loss: 0.00014760221529286355, Validation Loss: 0.0001474871763396853\n",
      "Epoch [8/25], Train Loss: 0.00018690319848246872, Validation Loss: 0.0001462427680962719\n",
      "Epoch [8/25], Train Loss: 0.0001408573443768546, Validation Loss: 0.0001453017178088582\n",
      "Epoch [8/25], Train Loss: 0.00020092407066840678, Validation Loss: 0.0001452323956376252\n",
      "Epoch [8/25], Train Loss: 0.0001827530941227451, Validation Loss: 0.00014587479842399868\n",
      "Epoch [8/25], Train Loss: 0.0001579228410264477, Validation Loss: 0.00014658364258745375\n",
      "Epoch [8/25], Train Loss: 0.00013827868679072708, Validation Loss: 0.000147261254460318\n",
      "Epoch [8/25], Train Loss: 0.00011591884685913101, Validation Loss: 0.0001476444500440266\n",
      "Epoch [8/25], Train Loss: 0.0001371256512356922, Validation Loss: 0.00014784853022623186\n",
      "Epoch [8/25], Train Loss: 0.00018468023336026818, Validation Loss: 0.0001472210470334782\n",
      "Epoch [8/25], Train Loss: 0.00025025909417308867, Validation Loss: 0.00014656607017968782\n",
      "Epoch [8/25], Train Loss: 0.00015851453645154834, Validation Loss: 0.00014561080921945784\n",
      "Epoch [8/25], Train Loss: 0.00010365800699219108, Validation Loss: 0.0001450887667791297\n",
      "Epoch [8/25], Train Loss: 0.00012966321082785726, Validation Loss: 0.0001452292929267666\n",
      "Epoch [8/25], Train Loss: 0.00012035086547257379, Validation Loss: 0.00014568899180934143\n",
      "Epoch [8/25], Train Loss: 0.00010753484821179882, Validation Loss: 0.00014593017695005984\n",
      "Epoch [8/25], Train Loss: 0.00013609185407403857, Validation Loss: 0.00014579164853785188\n",
      "Epoch [8/25], Train Loss: 0.00015028021880425513, Validation Loss: 0.0001455307142653813\n",
      "Epoch [8/25], Train Loss: 0.0001480548526160419, Validation Loss: 0.00014522347943663288\n",
      "Epoch [8/25], Train Loss: 0.0002066684392048046, Validation Loss: 0.00014507036855017456\n",
      "Epoch [8/25], Train Loss: 0.00017726422811392695, Validation Loss: 0.00014508088594690586\n",
      "Epoch [8/25], Train Loss: 0.00011392252054065466, Validation Loss: 0.0001454056182410568\n",
      "Epoch [8/25], Train Loss: 0.00017863890388980508, Validation Loss: 0.0001460460215942779\n",
      "Epoch [8/25], Train Loss: 0.00010944789391942322, Validation Loss: 0.0001468972581884979\n",
      "Epoch [8/25], Train Loss: 0.00015004050510469824, Validation Loss: 0.00014784041656336436\n",
      "Epoch [8/25], Train Loss: 0.0001354632986476645, Validation Loss: 0.0001479916298800769\n",
      "Epoch [8/25], Train Loss: 0.00016588876314926893, Validation Loss: 0.00014704867911253435\n",
      "Epoch [8/25], Train Loss: 0.00011864909174619243, Validation Loss: 0.00014577089289862972\n",
      "Epoch [8/25], Train Loss: 0.00016070596757344902, Validation Loss: 0.00014513697339377056\n",
      "Epoch [8/25], Train Loss: 0.00023752200650051236, Validation Loss: 0.00014529754092412379\n",
      "Epoch [8/25], Train Loss: 0.00020233323448337615, Validation Loss: 0.00014594097325849967\n",
      "Epoch [8/25], Train Loss: 0.0001501039369031787, Validation Loss: 0.00014677544289346163\n",
      "Epoch [8/25], Train Loss: 0.00014135685341898352, Validation Loss: 0.00014763829046084235\n",
      "Epoch [8/25], Train Loss: 0.00016356144624296576, Validation Loss: 0.00014900994501658714\n",
      "Epoch [8/25], Train Loss: 0.0001266601320821792, Validation Loss: 0.0001498402268528783\n",
      "Epoch [8/25], Train Loss: 9.229803981725127e-05, Validation Loss: 0.00014908087420432517\n",
      "Epoch [8/25], Train Loss: 0.00016904619405977428, Validation Loss: 0.0001470203984354157\n",
      "Epoch [8/25], Train Loss: 0.0002230006648460403, Validation Loss: 0.00014536997623508795\n",
      "Epoch [8/25], Train Loss: 0.00014169476344250143, Validation Loss: 0.00014521457608983231\n",
      "Epoch [8/25], Train Loss: 0.00010874129657167941, Validation Loss: 0.00014616254047723488\n",
      "Epoch [8/25], Train Loss: 0.00014598804409615695, Validation Loss: 0.00014667369274926992\n",
      "Epoch [8/25], Train Loss: 0.00013539553037844598, Validation Loss: 0.00014618602144764737\n",
      "Epoch [8/25], Train Loss: 0.00014139950508251786, Validation Loss: 0.00014533751309500075\n",
      "Epoch [8/25], Train Loss: 0.00014536414528265595, Validation Loss: 0.00014497768715955318\n",
      "Epoch [8/25], Train Loss: 0.00015432760119438171, Validation Loss: 0.00014535901718772947\n",
      "Epoch [8/25], Train Loss: 0.00018986675422638655, Validation Loss: 0.00014691191730283512\n",
      "Epoch [8/25], Train Loss: 0.000178605318069458, Validation Loss: 0.00014841404214773016\n",
      "Epoch [8/25], Train Loss: 0.00016145799600053579, Validation Loss: 0.00015048277612853174\n",
      "Epoch [8/25], Train Loss: 0.0001326553028775379, Validation Loss: 0.00015015869721537455\n",
      "Epoch [8/25], Train Loss: 0.00011389688006602228, Validation Loss: 0.00014746712913620285\n",
      "Epoch [8/25], Train Loss: 0.00014259899035096169, Validation Loss: 0.00014529038356461872\n",
      "Epoch [8/25], Train Loss: 0.00015776423970237374, Validation Loss: 0.00014597882545785978\n",
      "Epoch [8/25], Train Loss: 0.00011613716196734458, Validation Loss: 0.0001479493696630622\n",
      "Epoch [8/25], Train Loss: 0.0001436607853975147, Validation Loss: 0.00014817226207621085\n",
      "Epoch [8/25], Train Loss: 0.00018189806723967195, Validation Loss: 0.00014676568753202446\n",
      "Epoch [8/25], Train Loss: 0.00020359682093840092, Validation Loss: 0.00014531818645385403\n",
      "Epoch [8/25], Train Loss: 0.00012665014946833253, Validation Loss: 0.00014528137495896468\n",
      "Epoch [8/25], Train Loss: 0.00015289139992091805, Validation Loss: 0.00014667325691940884\n",
      "Epoch [8/25], Train Loss: 0.0001383560011163354, Validation Loss: 0.00014768793237938856\n",
      "Epoch [8/25], Train Loss: 0.00023974764917511493, Validation Loss: 0.00014851054681154588\n",
      "Epoch [8/25], Train Loss: 0.00015280126535799354, Validation Loss: 0.0001473261198649804\n",
      "Epoch [8/25], Train Loss: 0.00018232676666229963, Validation Loss: 0.00014567500838893465\n",
      "Epoch [8/25], Train Loss: 0.000102871963463258, Validation Loss: 0.00014511405291462627\n",
      "Epoch [8/25], Train Loss: 0.00018799281679093838, Validation Loss: 0.00014606120118211644\n",
      "Epoch [8/25], Train Loss: 0.0001824283681344241, Validation Loss: 0.0001467572498465112\n",
      "Epoch [8/25], Train Loss: 0.00015029616770334542, Validation Loss: 0.00014645903575001286\n",
      "Epoch [8/25], Train Loss: 0.00015958519361447543, Validation Loss: 0.00014548691372813968\n",
      "Epoch [8/25], Train Loss: 0.0001930918951984495, Validation Loss: 0.0001449986203321411\n",
      "Epoch [8/25], Train Loss: 0.00012870431237388402, Validation Loss: 0.00014548522279559013\n",
      "Epoch [8/25], Train Loss: 0.00014192074013408273, Validation Loss: 0.00014655806735390796\n",
      "Epoch [8/25], Train Loss: 0.00013214966747909784, Validation Loss: 0.00014761329075554385\n",
      "Epoch [8/25], Train Loss: 0.00017704696801956743, Validation Loss: 0.0001491462377695522\n",
      "Epoch [8/25], Train Loss: 0.00016914219304453582, Validation Loss: 0.0001488699939121337\n",
      "Epoch [8/25], Train Loss: 0.00018569918756838888, Validation Loss: 0.0001467952999519184\n",
      "Epoch [8/25], Train Loss: 0.00016422700718976557, Validation Loss: 0.0001452310068998486\n",
      "Epoch [8/25], Train Loss: 0.0001314112014370039, Validation Loss: 0.00014560570465012765\n",
      "Epoch [8/25], Train Loss: 0.00011059914686484262, Validation Loss: 0.00014690754372471322\n",
      "Epoch [8/25], Train Loss: 0.000163770979270339, Validation Loss: 0.00014714601978387993\n",
      "Epoch [8/25], Train Loss: 0.00019217589579056948, Validation Loss: 0.00014615037774395507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], Train Loss: 0.00019426169455982745, Validation Loss: 0.0001451306168746669\n",
      "Epoch [8/25], Train Loss: 0.0001335269771516323, Validation Loss: 0.00014504873082235765\n",
      "Epoch [8/25], Train Loss: 0.00015928575885482132, Validation Loss: 0.0001457368775542515\n",
      "Epoch [8/25], Train Loss: 0.0001754953118506819, Validation Loss: 0.00014678301085950806\n",
      "Epoch [8/25], Train Loss: 0.0001379765017190948, Validation Loss: 0.00014882136795980235\n",
      "Epoch [8/25], Train Loss: 0.00013878273603040725, Validation Loss: 0.0001497809214924928\n",
      "Epoch [8/25], Train Loss: 0.0001212346032843925, Validation Loss: 0.00014832217032865932\n",
      "Epoch [8/25], Train Loss: 0.00017285908688791096, Validation Loss: 0.00014582021370491323\n",
      "Epoch [8/25], Train Loss: 0.00015170146070886403, Validation Loss: 0.00014539659944906209\n",
      "Epoch [8/25], Train Loss: 0.00015763762348797172, Validation Loss: 0.00014688799977496576\n",
      "Epoch [8/25], Train Loss: 0.00011500658001750708, Validation Loss: 0.0001476417989276039\n",
      "Epoch [8/25], Train Loss: 0.00015672224981244653, Validation Loss: 0.00014659573450141275\n",
      "Epoch [8/25], Train Loss: 0.0001869291445473209, Validation Loss: 0.00014525253233538631\n",
      "Epoch [8/25], Train Loss: 0.00015371653717011213, Validation Loss: 0.00014521774113139448\n",
      "Epoch [8/25], Train Loss: 0.00013244262663647532, Validation Loss: 0.00014617383979687777\n",
      "Epoch [8/25], Train Loss: 0.00010053306323243305, Validation Loss: 0.0001471563812325864\n",
      "Epoch [8/25], Train Loss: 0.00016905917436815798, Validation Loss: 0.000147739436943084\n",
      "Epoch [8/25], Train Loss: 0.00016528267588000745, Validation Loss: 0.0001467699871379106\n",
      "Epoch [8/25], Train Loss: 0.00013351179950404912, Validation Loss: 0.00014533369345978524\n",
      "Epoch [8/25], Train Loss: 0.00013988852151669562, Validation Loss: 0.0001450212449223424\n",
      "Epoch [8/25], Train Loss: 0.00015577154408674687, Validation Loss: 0.00014567405596608297\n",
      "Epoch [8/25], Train Loss: 0.00023826102551538497, Validation Loss: 0.00014615446222402775\n",
      "Epoch [8/25], Train Loss: 0.00013099174248054624, Validation Loss: 0.00014580520073650404\n",
      "Epoch [8/25], Train Loss: 0.00018308339349459857, Validation Loss: 0.00014507112524976643\n",
      "Epoch [8/25], Train Loss: 0.00011177548003615811, Validation Loss: 0.00014503151081347217\n",
      "Epoch [8/25], Train Loss: 0.00013358007709030062, Validation Loss: 0.00014563580019360717\n",
      "Epoch [8/25], Train Loss: 0.00016792585665825754, Validation Loss: 0.0001460973871871829\n",
      "Epoch [8/25], Train Loss: 0.00011368613195372745, Validation Loss: 0.0001460546661595193\n",
      "Epoch [8/25], Train Loss: 0.0001508249988546595, Validation Loss: 0.00014576656685676425\n",
      "Epoch [8/25], Train Loss: 0.0002163065946660936, Validation Loss: 0.00014522241520656582\n",
      "Epoch [8/25], Train Loss: 0.00013200372632127255, Validation Loss: 0.0001449455111772598\n",
      "Epoch [8/25], Train Loss: 0.0001531244779471308, Validation Loss: 0.00014511581272624123\n",
      "Epoch [8/25], Train Loss: 0.00016513155424036086, Validation Loss: 0.00014547181053785606\n",
      "Epoch [8/25], Train Loss: 0.00014357612235471606, Validation Loss: 0.00014572110061029283\n",
      "Epoch [8/25], Train Loss: 0.00014806442777626216, Validation Loss: 0.00014574363740393892\n",
      "Epoch [8/25], Train Loss: 0.00013855425640940666, Validation Loss: 0.0001456314678459118\n",
      "Epoch [8/25], Train Loss: 0.0001558400981593877, Validation Loss: 0.00014531003616866656\n",
      "Epoch [8/25], Train Loss: 0.0001881068601505831, Validation Loss: 0.00014506019094066383\n",
      "Epoch [8/25], Train Loss: 0.00012631798745132983, Validation Loss: 0.00014490448326493304\n",
      "Epoch [8/25], Train Loss: 0.00018056931730825454, Validation Loss: 0.0001448715411243029\n",
      "Epoch [8/25], Train Loss: 0.0001169785246020183, Validation Loss: 0.00014490848431402507\n",
      "Epoch [8/25], Train Loss: 0.00014749678666703403, Validation Loss: 0.00014496397658755693\n",
      "Epoch [8/25], Train Loss: 0.00018092950631398708, Validation Loss: 0.0001449514699440139\n",
      "Epoch [8/25], Train Loss: 0.00018074132094625384, Validation Loss: 0.00014492005357169546\n",
      "Epoch [8/25], Train Loss: 0.00013522474910132587, Validation Loss: 0.00014489695895463227\n",
      "Epoch [8/25], Train Loss: 0.00014310042024590075, Validation Loss: 0.00014488183248128432\n",
      "Epoch [8/25], Train Loss: 0.000213156221434474, Validation Loss: 0.0001448808608984109\n",
      "Epoch [8/25], Train Loss: 0.0001270933134946972, Validation Loss: 0.00014488296656054445\n",
      "Epoch [8/25], Train Loss: 0.00016036575834732503, Validation Loss: 0.00014491152639190357\n",
      "Epoch [8/25], Train Loss: 0.00013083159865345806, Validation Loss: 0.00014500889349922848\n",
      "Epoch [8/25], Train Loss: 0.0002088933251798153, Validation Loss: 0.00014513035372753317\n",
      "Epoch [8/25], Train Loss: 0.000186745950486511, Validation Loss: 0.00014512443716133323\n",
      "Epoch [8/25], Train Loss: 0.00016923861403483897, Validation Loss: 0.0001451468349841889\n",
      "Epoch [8/25], Train Loss: 0.00015693283057771623, Validation Loss: 0.00014518471750003906\n",
      "Epoch [8/25], Train Loss: 0.00012479667202569544, Validation Loss: 0.00014513421920128166\n",
      "Epoch [8/25], Train Loss: 0.0001733366952976212, Validation Loss: 0.00014500522859937822\n",
      "Epoch [8/25], Train Loss: 0.00016850717656780034, Validation Loss: 0.0001448473422594058\n",
      "Epoch [8/25], Train Loss: 0.00015798579261172563, Validation Loss: 0.0001447941142638835\n",
      "Epoch [8/25], Train Loss: 0.00016856982256285846, Validation Loss: 0.00014481248993737002\n",
      "Epoch [8/25], Train Loss: 0.00014530910993926227, Validation Loss: 0.0001448413512359063\n",
      "Epoch [8/25], Train Loss: 0.00011935771908611059, Validation Loss: 0.00014492752986067597\n",
      "Epoch [8/25], Train Loss: 0.00016879189934115857, Validation Loss: 0.0001450262747918411\n",
      "Epoch [8/25], Train Loss: 0.0001489432470407337, Validation Loss: 0.00014503818383673205\n",
      "Epoch [8/25], Train Loss: 0.00020006146223749965, Validation Loss: 0.0001450623179456064\n",
      "Epoch [8/25], Train Loss: 0.00013304798630997539, Validation Loss: 0.00014503227454648975\n",
      "Epoch [8/25], Train Loss: 0.00011561495193745941, Validation Loss: 0.00014494752783017855\n",
      "Epoch [8/25], Train Loss: 0.00015042611630633473, Validation Loss: 0.00014485631763818673\n",
      "Epoch [8/25], Train Loss: 0.00014452621689997613, Validation Loss: 0.0001447837062490483\n",
      "Epoch [8/25], Train Loss: 0.00021442219440359622, Validation Loss: 0.0001447619688406121\n",
      "Epoch [8/25], Train Loss: 0.00015869778872001916, Validation Loss: 0.00014485517458524556\n",
      "Epoch [8/25], Train Loss: 0.00012512189277913421, Validation Loss: 0.0001449515919375699\n",
      "Epoch [8/25], Train Loss: 0.0001582497643539682, Validation Loss: 0.0001449918796424754\n",
      "Epoch [8/25], Train Loss: 0.0001371816615574062, Validation Loss: 0.00014495746703081143\n",
      "Epoch [8/25], Train Loss: 0.000170854342286475, Validation Loss: 0.0001449342344130855\n",
      "Epoch [8/25], Train Loss: 0.0002503163705114275, Validation Loss: 0.00014503059646813198\n",
      "Epoch [8/25], Train Loss: 0.0001252991787623614, Validation Loss: 0.0001451647243811749\n",
      "Epoch [8/25], Train Loss: 0.0001502507657278329, Validation Loss: 0.00014533958722798463\n",
      "Epoch [8/25], Train Loss: 0.00013484287774190307, Validation Loss: 0.00014547092408368674\n",
      "Epoch [8/25], Train Loss: 0.00017134984955191612, Validation Loss: 0.00014570368415055176\n",
      "Epoch [8/25], Train Loss: 0.00022503842774312943, Validation Loss: 0.00014590837599826045\n",
      "Epoch [8/25], Train Loss: 0.00010145414125872776, Validation Loss: 0.00014617941463560176\n",
      "Epoch [8/25], Train Loss: 0.00014566699974238873, Validation Loss: 0.0001464652285600702\n",
      "Epoch [8/25], Train Loss: 0.00015681388322263956, Validation Loss: 0.0001467259901498134\n",
      "Epoch [8/25], Train Loss: 0.00012899604917038232, Validation Loss: 0.00014682411953496435\n",
      "Epoch [8/25], Train Loss: 0.0001427936222171411, Validation Loss: 0.00014684942337529113\n",
      "Epoch [8/25], Train Loss: 0.0002153719397028908, Validation Loss: 0.00014653625160766144\n",
      "Epoch [8/25], Train Loss: 0.00012428659829311073, Validation Loss: 0.00014597300129632156\n",
      "Epoch [8/25], Train Loss: 0.00013081662473268807, Validation Loss: 0.00014537910295378726\n",
      "Epoch [8/25], Train Loss: 0.0001535526243969798, Validation Loss: 0.00014501449331874027\n",
      "Epoch [8/25], Train Loss: 0.00010569379082880914, Validation Loss: 0.00014497392864238162\n",
      "Epoch [8/25], Train Loss: 0.00015006281319074333, Validation Loss: 0.00014521519527382527\n",
      "Epoch [8/25], Train Loss: 0.0001440887281205505, Validation Loss: 0.00014552797098682884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], Train Loss: 0.0002096552780130878, Validation Loss: 0.00014577289063405868\n",
      "Epoch [8/25], Train Loss: 0.00010386894427938387, Validation Loss: 0.00014606785989599303\n",
      "Epoch [8/25], Train Loss: 0.0001070046637323685, Validation Loss: 0.0001462334354679721\n",
      "Epoch [8/25], Train Loss: 0.00013325239706318825, Validation Loss: 0.0001461960404412821\n",
      "Epoch [8/25], Train Loss: 0.00015877473924774677, Validation Loss: 0.0001458802636382946\n",
      "Epoch [8/25], Train Loss: 0.00014086891314946115, Validation Loss: 0.00014546307817605946\n",
      "Epoch [8/25], Train Loss: 0.00020338456670287997, Validation Loss: 0.00014498834449720258\n",
      "Epoch [8/25], Train Loss: 0.0001424155052518472, Validation Loss: 0.00014474310810328462\n",
      "Epoch [8/25], Train Loss: 0.00013494418817572296, Validation Loss: 0.0001448376290985228\n",
      "Epoch [8/25], Train Loss: 0.00016767463239375502, Validation Loss: 0.00014509700510340434\n",
      "Epoch [8/25], Train Loss: 0.0001405613002134487, Validation Loss: 0.00014532974164467306\n",
      "Epoch [8/25], Train Loss: 0.000101375779195223, Validation Loss: 0.00014547470345860347\n",
      "Epoch [8/25], Train Loss: 0.0001562247343827039, Validation Loss: 0.00014554392279630217\n",
      "Epoch [8/25], Train Loss: 0.00014233576075639576, Validation Loss: 0.00014545287825361205\n",
      "Epoch [8/25], Train Loss: 0.00011630394874373451, Validation Loss: 0.00014522235578624533\n",
      "Epoch [8/25], Train Loss: 0.00019054289441555738, Validation Loss: 0.0001452510904831191\n",
      "Epoch [8/25], Train Loss: 0.0001578642550157383, Validation Loss: 0.00014545708036166615\n",
      "Epoch [8/25], Train Loss: 0.00022649526363238692, Validation Loss: 0.00014546161279819594\n",
      "Epoch [8/25], Train Loss: 0.00010736866533989087, Validation Loss: 0.00014522832837731887\n",
      "Epoch [8/25], Train Loss: 0.0001435584417777136, Validation Loss: 0.00014505789440590888\n",
      "Epoch [8/25], Train Loss: 0.00010676970850909129, Validation Loss: 0.0001449818608913726\n",
      "Epoch [8/25], Train Loss: 0.00013183339615352452, Validation Loss: 0.000144945120215804\n",
      "Epoch [8/25], Train Loss: 0.00014196652045939118, Validation Loss: 0.0001449118833988905\n",
      "Epoch [8/25], Train Loss: 0.00014848180580884218, Validation Loss: 0.0001448666036594659\n",
      "Epoch [8/25], Train Loss: 0.00015392676868941635, Validation Loss: 0.00014484153119459126\n",
      "Epoch [8/25], Train Loss: 0.0001861934579210356, Validation Loss: 0.00014482978852659774\n",
      "Epoch [8/25], Train Loss: 0.00016361077723558992, Validation Loss: 0.00014485394640360028\n",
      "Epoch [8/25], Train Loss: 9.043091267812997e-05, Validation Loss: 0.00014486497893813067\n",
      "Epoch [9/25], Train Loss: 0.00012057102139806375, Validation Loss: 0.0001449608821227836\n",
      "Epoch [9/25], Train Loss: 0.0001319189468631521, Validation Loss: 0.0001449716658195636\n",
      "Epoch [9/25], Train Loss: 0.0001742712629493326, Validation Loss: 0.0001449840313095289\n",
      "Epoch [9/25], Train Loss: 0.00019319032435305417, Validation Loss: 0.00014510433393297718\n",
      "Epoch [9/25], Train Loss: 0.00015967516810633242, Validation Loss: 0.00014533925665697703\n",
      "Epoch [9/25], Train Loss: 0.00016639128443785012, Validation Loss: 0.0001456211614519513\n",
      "Epoch [9/25], Train Loss: 0.00016882667841855437, Validation Loss: 0.00014601030561607332\n",
      "Epoch [9/25], Train Loss: 0.00010565069533186033, Validation Loss: 0.00014667395516880787\n",
      "Epoch [9/25], Train Loss: 0.00014636630658060312, Validation Loss: 0.00014754518609455164\n",
      "Epoch [9/25], Train Loss: 0.00018422554421704262, Validation Loss: 0.00014898355705857588\n",
      "Epoch [9/25], Train Loss: 9.000592399388552e-05, Validation Loss: 0.00015064926628838293\n",
      "Epoch [9/25], Train Loss: 0.0001766156347002834, Validation Loss: 0.00015194688530755228\n",
      "Epoch [9/25], Train Loss: 0.00012783802230842412, Validation Loss: 0.00015155165359222641\n",
      "Epoch [9/25], Train Loss: 0.00017193006351590157, Validation Loss: 0.00014883899517978232\n",
      "Epoch [9/25], Train Loss: 0.0001225715532200411, Validation Loss: 0.00014588937507748295\n",
      "Epoch [9/25], Train Loss: 0.00022660660033579916, Validation Loss: 0.00014491465359848613\n",
      "Epoch [9/25], Train Loss: 0.0001707578921923414, Validation Loss: 0.00014662568622346347\n",
      "Epoch [9/25], Train Loss: 0.00014097116945777088, Validation Loss: 0.00014941921375187425\n",
      "Epoch [9/25], Train Loss: 0.0002574302488937974, Validation Loss: 0.00015617341923643835\n",
      "Epoch [9/25], Train Loss: 0.000180166243808344, Validation Loss: 0.00015826012216469583\n",
      "Epoch [9/25], Train Loss: 0.00015577164595015347, Validation Loss: 0.0001530012285608488\n",
      "Epoch [9/25], Train Loss: 0.00017313475836999714, Validation Loss: 0.000146031437664836\n",
      "Epoch [9/25], Train Loss: 9.181872883345932e-05, Validation Loss: 0.00014647173641909225\n",
      "Epoch [9/25], Train Loss: 0.00012757876538671553, Validation Loss: 0.0001508652402359682\n",
      "Epoch [9/25], Train Loss: 0.00014836384798400104, Validation Loss: 0.0001500240830258311\n",
      "Epoch [9/25], Train Loss: 0.0001869239058578387, Validation Loss: 0.00014609332550511074\n",
      "Epoch [9/25], Train Loss: 0.00024523018510080874, Validation Loss: 0.00014569307459169067\n",
      "Epoch [9/25], Train Loss: 0.0001868709659902379, Validation Loss: 0.00014843074895907194\n",
      "Epoch [9/25], Train Loss: 0.0001953817845787853, Validation Loss: 0.00015185875866639739\n",
      "Epoch [9/25], Train Loss: 0.0001589774910826236, Validation Loss: 0.0001497042603053463\n",
      "Epoch [9/25], Train Loss: 0.0001699053100310266, Validation Loss: 0.00014630122289721232\n",
      "Epoch [9/25], Train Loss: 0.0001438216568203643, Validation Loss: 0.00014539064747320178\n",
      "Epoch [9/25], Train Loss: 9.894232789520174e-05, Validation Loss: 0.00014749347925923454\n",
      "Epoch [9/25], Train Loss: 0.00015450279170181602, Validation Loss: 0.00014817143407223435\n",
      "Epoch [9/25], Train Loss: 0.0002170606458093971, Validation Loss: 0.0001459341583540663\n",
      "Epoch [9/25], Train Loss: 0.00018195033771917224, Validation Loss: 0.00014531135141927128\n",
      "Epoch [9/25], Train Loss: 0.000162777112564072, Validation Loss: 0.0001468922731874045\n",
      "Epoch [9/25], Train Loss: 0.00016818885342217982, Validation Loss: 0.00014739642550315086\n",
      "Epoch [9/25], Train Loss: 0.00016050947306212038, Validation Loss: 0.00014607993750056873\n",
      "Epoch [9/25], Train Loss: 0.00016651273472234607, Validation Loss: 0.00014494054339593277\n",
      "Epoch [9/25], Train Loss: 0.00010050894343294203, Validation Loss: 0.0001455716216393436\n",
      "Epoch [9/25], Train Loss: 0.00015739274385850877, Validation Loss: 0.00014688101825110304\n",
      "Epoch [9/25], Train Loss: 0.00014398882922250777, Validation Loss: 0.00014716697429927687\n",
      "Epoch [9/25], Train Loss: 0.00013292150106281042, Validation Loss: 0.0001462599859223701\n",
      "Epoch [9/25], Train Loss: 0.00017636445409152657, Validation Loss: 0.0001449677423806861\n",
      "Epoch [9/25], Train Loss: 0.00017870291776489466, Validation Loss: 0.0001448625388244788\n",
      "Epoch [9/25], Train Loss: 0.00015835529484320432, Validation Loss: 0.00014555537563865073\n",
      "Epoch [9/25], Train Loss: 0.00017454609042033553, Validation Loss: 0.0001460115270068248\n",
      "Epoch [9/25], Train Loss: 0.00017240768647752702, Validation Loss: 0.0001457259963596395\n",
      "Epoch [9/25], Train Loss: 0.00015286073903553188, Validation Loss: 0.00014505550595155607\n",
      "Epoch [9/25], Train Loss: 0.0001833572896430269, Validation Loss: 0.00014474512063316068\n",
      "Epoch [9/25], Train Loss: 0.0001401630142936483, Validation Loss: 0.00014486440219722377\n",
      "Epoch [9/25], Train Loss: 0.00017487585137132555, Validation Loss: 0.00014504800540938352\n",
      "Epoch [9/25], Train Loss: 0.00016659512766636908, Validation Loss: 0.00014513090912563105\n",
      "Epoch [9/25], Train Loss: 0.00015048522618599236, Validation Loss: 0.00014503036873065868\n",
      "Epoch [9/25], Train Loss: 0.00013564000255428255, Validation Loss: 0.00014482715268968605\n",
      "Epoch [9/25], Train Loss: 0.0001388474483974278, Validation Loss: 0.00014468610606854782\n",
      "Epoch [9/25], Train Loss: 0.00011715012806234881, Validation Loss: 0.0001446840566738198\n",
      "Epoch [9/25], Train Loss: 0.00016518890333827585, Validation Loss: 0.00014475895004579798\n",
      "Epoch [9/25], Train Loss: 0.00020922299881931394, Validation Loss: 0.0001448107934265863\n",
      "Epoch [9/25], Train Loss: 0.00015833615907467902, Validation Loss: 0.00014482084904254104\n",
      "Epoch [9/25], Train Loss: 0.00012257532216608524, Validation Loss: 0.00014483009702720057\n",
      "Epoch [9/25], Train Loss: 0.0001384160714223981, Validation Loss: 0.00014481355077199017\n",
      "Epoch [9/25], Train Loss: 0.00015813260688446462, Validation Loss: 0.00014475413093653817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], Train Loss: 0.0001510817965026945, Validation Loss: 0.0001447188495755351\n",
      "Epoch [9/25], Train Loss: 0.00014258231385610998, Validation Loss: 0.0001446730425717154\n",
      "Epoch [9/25], Train Loss: 0.00018881174037232995, Validation Loss: 0.0001446453524598231\n",
      "Epoch [9/25], Train Loss: 0.00016146639245562255, Validation Loss: 0.00014465270141954533\n",
      "Epoch [9/25], Train Loss: 0.000185683136805892, Validation Loss: 0.00014466402814529525\n",
      "Epoch [9/25], Train Loss: 0.00015498531865887344, Validation Loss: 0.00014463909925931754\n",
      "Epoch [9/25], Train Loss: 0.0001861237979028374, Validation Loss: 0.00014463888510363176\n",
      "Epoch [9/25], Train Loss: 0.00015354380593635142, Validation Loss: 0.00014463945335592144\n",
      "Epoch [9/25], Train Loss: 0.00017114642832893878, Validation Loss: 0.0001446466686805555\n",
      "Epoch [9/25], Train Loss: 0.00013665558071807027, Validation Loss: 0.00014463226640752205\n",
      "Epoch [9/25], Train Loss: 0.00016349070938304067, Validation Loss: 0.00014459047937028421\n",
      "Epoch [9/25], Train Loss: 0.00016506327665410936, Validation Loss: 0.0001445763725011299\n",
      "Epoch [9/25], Train Loss: 0.0001341487222816795, Validation Loss: 0.00014461613294164028\n",
      "Epoch [9/25], Train Loss: 0.00022205391724128276, Validation Loss: 0.00014464832784142346\n",
      "Epoch [9/25], Train Loss: 0.00017684002523310483, Validation Loss: 0.00014461545943049713\n",
      "Epoch [9/25], Train Loss: 0.00011922189878532663, Validation Loss: 0.00014456282879109493\n",
      "Epoch [9/25], Train Loss: 0.00016213544586207718, Validation Loss: 0.00014455865918231817\n",
      "Epoch [9/25], Train Loss: 0.00016564459656365216, Validation Loss: 0.0001446081519437333\n",
      "Epoch [9/25], Train Loss: 9.701724047772586e-05, Validation Loss: 0.00014467110498420273\n",
      "Epoch [9/25], Train Loss: 0.00016960996435955167, Validation Loss: 0.00014479408079447846\n",
      "Epoch [9/25], Train Loss: 0.00016387234791181982, Validation Loss: 0.0001450003287269889\n",
      "Epoch [9/25], Train Loss: 0.0001840612239902839, Validation Loss: 0.0001453619932969256\n",
      "Epoch [9/25], Train Loss: 9.576845332048833e-05, Validation Loss: 0.00014589797938242556\n",
      "Epoch [9/25], Train Loss: 0.00016477589088026434, Validation Loss: 0.00014631863256605964\n",
      "Epoch [9/25], Train Loss: 0.0001521447120467201, Validation Loss: 0.0001464358171991383\n",
      "Epoch [9/25], Train Loss: 0.00018929642101284117, Validation Loss: 0.00014612991920633553\n",
      "Epoch [9/25], Train Loss: 0.00014112396456766874, Validation Loss: 0.0001455465506296605\n",
      "Epoch [9/25], Train Loss: 0.00016135098121594638, Validation Loss: 0.00014496538497041911\n",
      "Epoch [9/25], Train Loss: 0.00018026656471192837, Validation Loss: 0.0001446420532981089\n",
      "Epoch [9/25], Train Loss: 0.00016606991994194686, Validation Loss: 0.0001445499214848193\n",
      "Epoch [9/25], Train Loss: 0.00012543750926852226, Validation Loss: 0.00014461140259906338\n",
      "Epoch [9/25], Train Loss: 0.00015516146959271282, Validation Loss: 0.00014476874105942745\n",
      "Epoch [9/25], Train Loss: 0.00019444714416749775, Validation Loss: 0.0001452588772129578\n",
      "Epoch [9/25], Train Loss: 0.00017542224668432027, Validation Loss: 0.00014581972476056156\n",
      "Epoch [9/25], Train Loss: 0.00016955294995568693, Validation Loss: 0.00014667682407889515\n",
      "Epoch [9/25], Train Loss: 0.0001632266939850524, Validation Loss: 0.0001469584482644374\n",
      "Epoch [9/25], Train Loss: 0.0001483593077864498, Validation Loss: 0.0001465462599298917\n",
      "Epoch [9/25], Train Loss: 0.00013068922271486372, Validation Loss: 0.00014573611285110624\n",
      "Epoch [9/25], Train Loss: 0.00018867096514441073, Validation Loss: 0.00014491975719768864\n",
      "Epoch [9/25], Train Loss: 0.0001306673657381907, Validation Loss: 0.00014458203901691983\n",
      "Epoch [9/25], Train Loss: 0.00012421954306773841, Validation Loss: 0.00014479355935084943\n",
      "Epoch [9/25], Train Loss: 0.00015196869208011776, Validation Loss: 0.0001453239800563703\n",
      "Epoch [9/25], Train Loss: 0.0001398083259118721, Validation Loss: 0.00014589995626010933\n",
      "Epoch [9/25], Train Loss: 0.0001530931913293898, Validation Loss: 0.00014659251901321112\n",
      "Epoch [9/25], Train Loss: 0.00012323581904638559, Validation Loss: 0.00014674820898411174\n",
      "Epoch [9/25], Train Loss: 0.00018536597781348974, Validation Loss: 0.00014652310225452916\n",
      "Epoch [9/25], Train Loss: 0.00026255586999468505, Validation Loss: 0.0001456834327351923\n",
      "Epoch [9/25], Train Loss: 0.00016397731087636203, Validation Loss: 0.00014500087030076733\n",
      "Epoch [9/25], Train Loss: 0.00017909797315951437, Validation Loss: 0.00014464228515862488\n",
      "Epoch [9/25], Train Loss: 0.00012433640949893743, Validation Loss: 0.0001448669970462409\n",
      "Epoch [9/25], Train Loss: 0.00019846031500492245, Validation Loss: 0.00014547883038176223\n",
      "Epoch [9/25], Train Loss: 0.00017747101082932204, Validation Loss: 0.00014589328323684944\n",
      "Epoch [9/25], Train Loss: 0.00012915668776258826, Validation Loss: 0.00014627289080332655\n",
      "Epoch [9/25], Train Loss: 0.00013204675633460283, Validation Loss: 0.0001465643428673502\n",
      "Epoch [9/25], Train Loss: 0.00010486238898010924, Validation Loss: 0.00014656800364415783\n",
      "Epoch [9/25], Train Loss: 0.00012322107795625925, Validation Loss: 0.00014614431299075174\n",
      "Epoch [9/25], Train Loss: 0.0001325227931374684, Validation Loss: 0.00014556522025183465\n",
      "Epoch [9/25], Train Loss: 0.00020180897263344377, Validation Loss: 0.00014494403294520452\n",
      "Epoch [9/25], Train Loss: 0.0002126692997990176, Validation Loss: 0.00014460449917047907\n",
      "Epoch [9/25], Train Loss: 0.0001450382696930319, Validation Loss: 0.000144649137412974\n",
      "Epoch [9/25], Train Loss: 0.00013805192429572344, Validation Loss: 0.00014500956021947786\n",
      "Epoch [9/25], Train Loss: 0.00014986644964665174, Validation Loss: 0.00014566191724346329\n",
      "Epoch [9/25], Train Loss: 0.00014277301670517772, Validation Loss: 0.0001462061574178127\n",
      "Epoch [9/25], Train Loss: 0.0001731083175400272, Validation Loss: 0.0001463952202660342\n",
      "Epoch [9/25], Train Loss: 0.00016637668886687607, Validation Loss: 0.00014618571585742757\n",
      "Epoch [9/25], Train Loss: 0.00019308930495753884, Validation Loss: 0.00014589495525190916\n",
      "Epoch [9/25], Train Loss: 0.00012402108404785395, Validation Loss: 0.0001453273170530641\n",
      "Epoch [9/25], Train Loss: 0.00016426367801614106, Validation Loss: 0.0001448239418095909\n",
      "Epoch [9/25], Train Loss: 0.00016055017476901412, Validation Loss: 0.00014462382411390234\n",
      "Epoch [9/25], Train Loss: 0.00011115138477180153, Validation Loss: 0.00014473908789417085\n",
      "Epoch [9/25], Train Loss: 0.00011721440387191251, Validation Loss: 0.00014501247860607692\n",
      "Epoch [9/25], Train Loss: 0.00016418029554188251, Validation Loss: 0.00014560610264500914\n",
      "Epoch [9/25], Train Loss: 9.398999100085348e-05, Validation Loss: 0.00014584452874260022\n",
      "Epoch [9/25], Train Loss: 0.0001341529277851805, Validation Loss: 0.00014575924530314902\n",
      "Epoch [9/25], Train Loss: 0.0001381045876769349, Validation Loss: 0.00014548517816971677\n",
      "Epoch [9/25], Train Loss: 0.00017341201601084322, Validation Loss: 0.0001452329119880839\n",
      "Epoch [9/25], Train Loss: 0.0001186272784252651, Validation Loss: 0.00014491172163009954\n",
      "Epoch [9/25], Train Loss: 0.0001896368048619479, Validation Loss: 0.0001448497054904389\n",
      "Epoch [9/25], Train Loss: 0.00021406420273706317, Validation Loss: 0.0001450647762491523\n",
      "Epoch [9/25], Train Loss: 0.0001515135809313506, Validation Loss: 0.00014550369329905757\n",
      "Epoch [9/25], Train Loss: 0.00014237944560591131, Validation Loss: 0.00014569017851802832\n",
      "Epoch [9/25], Train Loss: 0.000127280960441567, Validation Loss: 0.00014573226969029444\n",
      "Epoch [9/25], Train Loss: 0.00015636251191608608, Validation Loss: 0.00014548882876018373\n",
      "Epoch [9/25], Train Loss: 0.00016607613360974938, Validation Loss: 0.00014512748748529702\n",
      "Epoch [9/25], Train Loss: 0.00013838907761964947, Validation Loss: 0.00014483928437888002\n",
      "Epoch [9/25], Train Loss: 0.0001685414172243327, Validation Loss: 0.00014469582408006924\n",
      "Epoch [9/25], Train Loss: 0.00012103476910851896, Validation Loss: 0.00014472188995569014\n",
      "Epoch [9/25], Train Loss: 0.00013276621757540852, Validation Loss: 0.00014493284931328768\n",
      "Epoch [9/25], Train Loss: 0.00012472114758566022, Validation Loss: 0.000145236418757122\n",
      "Epoch [9/25], Train Loss: 0.00011921168334083632, Validation Loss: 0.00014571231561906947\n",
      "Epoch [9/25], Train Loss: 0.00016466026136185974, Validation Loss: 0.00014669085261023913\n",
      "Epoch [9/25], Train Loss: 0.00016529799904674292, Validation Loss: 0.00014814798923907803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], Train Loss: 0.0001376399741275236, Validation Loss: 0.00015092691052511024\n",
      "Epoch [9/25], Train Loss: 0.00013210292672738433, Validation Loss: 0.0001541597138081367\n",
      "Epoch [9/25], Train Loss: 0.00021564614144153893, Validation Loss: 0.0001567924664414022\n",
      "Epoch [9/25], Train Loss: 0.0001549547305330634, Validation Loss: 0.0001559475424680083\n",
      "Epoch [9/25], Train Loss: 0.00015354683273471892, Validation Loss: 0.00015145861495208617\n",
      "Epoch [9/25], Train Loss: 0.00017459662922192365, Validation Loss: 0.0001459675870137289\n",
      "Epoch [9/25], Train Loss: 0.00013280172424856573, Validation Loss: 0.00014549479116491662\n",
      "Epoch [9/25], Train Loss: 0.00016320038412231952, Validation Loss: 0.00014862559061536255\n",
      "Epoch [9/25], Train Loss: 0.00017518196546006948, Validation Loss: 0.0001491064984293189\n",
      "Epoch [9/25], Train Loss: 0.00020038190996274352, Validation Loss: 0.00014700405663461424\n",
      "Epoch [9/25], Train Loss: 0.00010174747148994356, Validation Loss: 0.0001449866208228438\n",
      "Epoch [9/25], Train Loss: 0.00012857539695687592, Validation Loss: 0.00014535595643489312\n",
      "Epoch [9/25], Train Loss: 0.000156367605086416, Validation Loss: 0.00014718285092385486\n",
      "Epoch [9/25], Train Loss: 0.00017389634740538895, Validation Loss: 0.000147097801285175\n",
      "Epoch [9/25], Train Loss: 0.00017563896835781634, Validation Loss: 0.00014582432146805028\n",
      "Epoch [9/25], Train Loss: 0.00011230063682887703, Validation Loss: 0.00014482822007266805\n",
      "Epoch [9/25], Train Loss: 0.00020591857901308686, Validation Loss: 0.00014539128339189725\n",
      "Epoch [9/25], Train Loss: 0.0001278798736166209, Validation Loss: 0.00014591198341804557\n",
      "Epoch [9/25], Train Loss: 0.00014288077363744378, Validation Loss: 0.00014534019016233893\n",
      "Epoch [9/25], Train Loss: 0.00011637115676421672, Validation Loss: 0.0001447039467166178\n",
      "Epoch [9/25], Train Loss: 0.00012797879753634334, Validation Loss: 0.0001448694715994255\n",
      "Epoch [9/25], Train Loss: 0.00014365441165864468, Validation Loss: 0.000145318247571898\n",
      "Epoch [9/25], Train Loss: 0.0001461682259105146, Validation Loss: 0.00014540768146010426\n",
      "Epoch [9/25], Train Loss: 0.00016074803716037422, Validation Loss: 0.00014500287895013268\n",
      "Epoch [9/25], Train Loss: 0.00021948458743281662, Validation Loss: 0.00014469942834693938\n",
      "Epoch [9/25], Train Loss: 0.00013631653564516455, Validation Loss: 0.0001445584758281863\n",
      "Epoch [9/25], Train Loss: 0.00015945608902256936, Validation Loss: 0.00014459574595093727\n",
      "Epoch [9/25], Train Loss: 0.00011168835771968588, Validation Loss: 0.00014463163097389042\n",
      "Epoch [9/25], Train Loss: 0.00016442193009424955, Validation Loss: 0.0001446257641267342\n",
      "Epoch [9/25], Train Loss: 0.00022451751283369958, Validation Loss: 0.00014462112594628706\n",
      "Epoch [9/25], Train Loss: 0.00014144032320473343, Validation Loss: 0.00014455672110974167\n",
      "Epoch [9/25], Train Loss: 0.0001802117913030088, Validation Loss: 0.00014451914733702628\n",
      "Epoch [9/25], Train Loss: 0.0001559179654577747, Validation Loss: 0.00014447095915481138\n",
      "Epoch [9/25], Train Loss: 0.00012016543769277632, Validation Loss: 0.00014446362717232357\n",
      "Epoch [9/25], Train Loss: 9.915419650496915e-05, Validation Loss: 0.00014446627368063975\n",
      "Epoch [9/25], Train Loss: 0.0002026988659054041, Validation Loss: 0.00014444952321355232\n",
      "Epoch [9/25], Train Loss: 0.00015966531645972282, Validation Loss: 0.00014444595799432137\n",
      "Epoch [9/25], Train Loss: 0.0001795404386939481, Validation Loss: 0.00014447127299111647\n",
      "Epoch [9/25], Train Loss: 0.00014146305329632014, Validation Loss: 0.00014447384819504805\n",
      "Epoch [9/25], Train Loss: 0.00013289506023284048, Validation Loss: 0.0001444515442320456\n",
      "Epoch [9/25], Train Loss: 0.00015691880253143609, Validation Loss: 0.00014443134665877247\n",
      "Epoch [9/25], Train Loss: 0.00014320800255518407, Validation Loss: 0.0001444607383746188\n",
      "Epoch [9/25], Train Loss: 0.00014252662367653102, Validation Loss: 0.00014448233899505187\n",
      "Epoch [9/25], Train Loss: 0.00018358143279328942, Validation Loss: 0.00014450567396124826\n",
      "Epoch [9/25], Train Loss: 0.0001389152166666463, Validation Loss: 0.0001446108210075181\n",
      "Epoch [9/25], Train Loss: 0.0001969253207789734, Validation Loss: 0.00014488326535987047\n",
      "Epoch [9/25], Train Loss: 0.00013310053327586502, Validation Loss: 0.0001453097075379143\n",
      "Epoch [9/25], Train Loss: 0.00014367923722602427, Validation Loss: 0.00014605540345655755\n",
      "Epoch [9/25], Train Loss: 0.0001418288447894156, Validation Loss: 0.00014706094734719954\n",
      "Epoch [9/25], Train Loss: 0.00016894625150598586, Validation Loss: 0.00014795499616108523\n",
      "Epoch [9/25], Train Loss: 0.0001662153226789087, Validation Loss: 0.00014864468733624865\n",
      "Epoch [9/25], Train Loss: 0.00019834761042147875, Validation Loss: 0.00014826374147863436\n",
      "Epoch [9/25], Train Loss: 0.00017636900884099305, Validation Loss: 0.00014731571427546443\n",
      "Epoch [9/25], Train Loss: 0.00016293952648993582, Validation Loss: 0.0001459392132043528\n",
      "Epoch [9/25], Train Loss: 0.00013784467591904104, Validation Loss: 0.00014482106513848217\n",
      "Epoch [9/25], Train Loss: 0.00017991049389820546, Validation Loss: 0.0001445325983998676\n",
      "Epoch [9/25], Train Loss: 0.00015453582454938442, Validation Loss: 0.00014518963968536506\n",
      "Epoch [9/25], Train Loss: 0.00019229641475249082, Validation Loss: 0.0001461447961143373\n",
      "Epoch [9/25], Train Loss: 0.00015587828238494694, Validation Loss: 0.00014627896428767901\n",
      "Epoch [9/25], Train Loss: 0.00017513286729808897, Validation Loss: 0.00014611791460386787\n",
      "Epoch [9/25], Train Loss: 9.714635962154716e-05, Validation Loss: 0.0001454942551693724\n",
      "Epoch [9/25], Train Loss: 0.00016751387738622725, Validation Loss: 0.0001447784736228641\n",
      "Epoch [9/25], Train Loss: 0.00016492500435560942, Validation Loss: 0.00014456007823658487\n",
      "Epoch [9/25], Train Loss: 0.00014290159742813557, Validation Loss: 0.0001448769881487048\n",
      "Epoch [9/25], Train Loss: 0.00019380587036721408, Validation Loss: 0.0001453542160258318\n",
      "Epoch [9/25], Train Loss: 0.00010743238817667589, Validation Loss: 0.00014547100217896514\n",
      "Epoch [9/25], Train Loss: 0.000201442715479061, Validation Loss: 0.00014524489330748717\n",
      "Epoch [9/25], Train Loss: 0.00018023120355792344, Validation Loss: 0.00014472897940625746\n",
      "Epoch [9/25], Train Loss: 0.000152477907249704, Validation Loss: 0.0001444746815347268\n",
      "Epoch [9/25], Train Loss: 0.00014778976037632674, Validation Loss: 0.0001445266427860285\n",
      "Epoch [9/25], Train Loss: 0.0001625761651666835, Validation Loss: 0.00014475959663589794\n",
      "Epoch [9/25], Train Loss: 0.0001391997211612761, Validation Loss: 0.00014486642564103628\n",
      "Epoch [9/25], Train Loss: 0.0001499871286796406, Validation Loss: 0.000144885651631436\n",
      "Epoch [9/25], Train Loss: 9.444185707252473e-05, Validation Loss: 0.00014462692682476092\n",
      "Epoch [9/25], Train Loss: 0.0001491722505306825, Validation Loss: 0.00014445481865550392\n",
      "Epoch [9/25], Train Loss: 0.00010842668416444212, Validation Loss: 0.00014440062441281042\n",
      "Epoch [9/25], Train Loss: 0.00015204958617687225, Validation Loss: 0.00014450746845492783\n",
      "Epoch [9/25], Train Loss: 0.0001621925475774333, Validation Loss: 0.0001446718949106677\n",
      "Epoch [9/25], Train Loss: 0.0001370273093925789, Validation Loss: 0.00014480286117759533\n",
      "Epoch [9/25], Train Loss: 0.00012161146878497675, Validation Loss: 0.00014484592466033064\n",
      "Epoch [9/25], Train Loss: 0.00019348225032445043, Validation Loss: 0.00014489688668011997\n",
      "Epoch [9/25], Train Loss: 0.00016488217806909233, Validation Loss: 0.00014486725049209782\n",
      "Epoch [9/25], Train Loss: 0.0001521569793112576, Validation Loss: 0.00014479697735320468\n",
      "Epoch [9/25], Train Loss: 0.000187883764738217, Validation Loss: 0.00014464409711460272\n",
      "Epoch [9/25], Train Loss: 0.00017235157429240644, Validation Loss: 0.00014454825747331295\n",
      "Epoch [9/25], Train Loss: 0.0001338789879810065, Validation Loss: 0.00014447541664897774\n",
      "Epoch [9/25], Train Loss: 0.00013389457308221608, Validation Loss: 0.0001444209219092348\n",
      "Epoch [9/25], Train Loss: 0.0001581327960593626, Validation Loss: 0.00014440898739849218\n",
      "Epoch [9/25], Train Loss: 0.00016419155872426927, Validation Loss: 0.00014442467921374675\n",
      "Epoch [9/25], Train Loss: 0.0001700944994809106, Validation Loss: 0.00014438679427257738\n",
      "Epoch [9/25], Train Loss: 0.00010022613423643634, Validation Loss: 0.00014435580572656667\n",
      "Epoch [9/25], Train Loss: 0.00015124141646083444, Validation Loss: 0.00014436201454373076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], Train Loss: 8.015303319552913e-05, Validation Loss: 0.0001443933540334304\n",
      "Epoch [9/25], Train Loss: 0.00020224381296429783, Validation Loss: 0.0001444258062595812\n",
      "Epoch [9/25], Train Loss: 9.718082583276555e-05, Validation Loss: 0.00014445853303186597\n",
      "Epoch [9/25], Train Loss: 0.00016295048408210278, Validation Loss: 0.00014447006663734403\n",
      "Epoch [9/25], Train Loss: 0.0001620624534552917, Validation Loss: 0.00014445886481553316\n",
      "Epoch [9/25], Train Loss: 0.00017366712563671172, Validation Loss: 0.00014443476126568082\n",
      "Epoch [9/25], Train Loss: 0.00011472505138954148, Validation Loss: 0.00014443627296714112\n",
      "Epoch [9/25], Train Loss: 0.0001469528506277129, Validation Loss: 0.00014442434864273915\n",
      "Epoch [9/25], Train Loss: 0.00020270716049708426, Validation Loss: 0.00014441511205707988\n",
      "Epoch [9/25], Train Loss: 0.00015098026779014617, Validation Loss: 0.00014440528951430073\n",
      "Epoch [9/25], Train Loss: 0.00012611843703780323, Validation Loss: 0.00014444582532936086\n",
      "Epoch [9/25], Train Loss: 0.00014654397091362625, Validation Loss: 0.00014454430905364764\n",
      "Epoch [9/25], Train Loss: 0.00016765289183240384, Validation Loss: 0.00014460121310548856\n",
      "Epoch [9/25], Train Loss: 0.0001353430125163868, Validation Loss: 0.00014466963621089234\n",
      "Epoch [9/25], Train Loss: 0.00012287904974073172, Validation Loss: 0.00014475817127580133\n",
      "Epoch [9/25], Train Loss: 0.00012350312317721546, Validation Loss: 0.00014485967258224264\n",
      "Epoch [9/25], Train Loss: 0.00021434891095850617, Validation Loss: 0.0001449001957856429\n",
      "Epoch [9/25], Train Loss: 0.00015078973956406116, Validation Loss: 0.00014494036052686471\n",
      "Epoch [9/25], Train Loss: 0.0001329899241682142, Validation Loss: 0.000145059717760887\n",
      "Epoch [9/25], Train Loss: 9.630683052819222e-05, Validation Loss: 0.00014514647288403162\n",
      "Epoch [10/25], Train Loss: 0.00019894777506124228, Validation Loss: 0.0001452275787111527\n",
      "Epoch [10/25], Train Loss: 0.00017108088650275022, Validation Loss: 0.00014547468429858176\n",
      "Epoch [10/25], Train Loss: 0.00010459039185661823, Validation Loss: 0.00014585447182374384\n",
      "Epoch [10/25], Train Loss: 0.00017496522923465818, Validation Loss: 0.0001462956689162335\n",
      "Epoch [10/25], Train Loss: 0.0001349610392935574, Validation Loss: 0.00014658006257377564\n",
      "Epoch [10/25], Train Loss: 0.0001362988114124164, Validation Loss: 0.00014669424999738112\n",
      "Epoch [10/25], Train Loss: 0.00012474168033804744, Validation Loss: 0.00014634658606761756\n",
      "Epoch [10/25], Train Loss: 0.0002154228714061901, Validation Loss: 0.00014566735529418414\n",
      "Epoch [10/25], Train Loss: 0.0001970615703612566, Validation Loss: 0.00014502216775629979\n",
      "Epoch [10/25], Train Loss: 0.00010684420703910291, Validation Loss: 0.0001445249751365433\n",
      "Epoch [10/25], Train Loss: 0.00013172718172427267, Validation Loss: 0.00014437529389397242\n",
      "Epoch [10/25], Train Loss: 0.00019271773635409772, Validation Loss: 0.00014439863104295607\n",
      "Epoch [10/25], Train Loss: 0.00013338823919184506, Validation Loss: 0.00014447305099262546\n",
      "Epoch [10/25], Train Loss: 0.00013360466982703656, Validation Loss: 0.00014480166452509973\n",
      "Epoch [10/25], Train Loss: 0.00011236871796427295, Validation Loss: 0.00014516173590285082\n",
      "Epoch [10/25], Train Loss: 0.00018516503041610122, Validation Loss: 0.00014540246896406947\n",
      "Epoch [10/25], Train Loss: 0.00019021893967874348, Validation Loss: 0.00014555656962329523\n",
      "Epoch [10/25], Train Loss: 0.00015773905033711344, Validation Loss: 0.00014541090228400815\n",
      "Epoch [10/25], Train Loss: 0.00017679962911643088, Validation Loss: 0.0001451399342234557\n",
      "Epoch [10/25], Train Loss: 0.00010867891978705302, Validation Loss: 0.00014482275801128708\n",
      "Epoch [10/25], Train Loss: 0.0001757806894602254, Validation Loss: 0.00014452830898032215\n",
      "Epoch [10/25], Train Loss: 0.00014723736967425793, Validation Loss: 0.00014434312130712595\n",
      "Epoch [10/25], Train Loss: 0.0001356663415208459, Validation Loss: 0.00014431917588808574\n",
      "Epoch [10/25], Train Loss: 0.00015684153186157346, Validation Loss: 0.00014444930420722812\n",
      "Epoch [10/25], Train Loss: 0.00015385478036478162, Validation Loss: 0.00014467797397325435\n",
      "Epoch [10/25], Train Loss: 0.00012821140990126878, Validation Loss: 0.00014489428504020906\n",
      "Epoch [10/25], Train Loss: 0.0001225660089403391, Validation Loss: 0.00014526411493231232\n",
      "Epoch [10/25], Train Loss: 0.0001681692519923672, Validation Loss: 0.0001458059086871799\n",
      "Epoch [10/25], Train Loss: 0.00017201023001689464, Validation Loss: 0.0001462058884499129\n",
      "Epoch [10/25], Train Loss: 0.00013275237870402634, Validation Loss: 0.00014655054668158602\n",
      "Epoch [10/25], Train Loss: 0.00014263555931393057, Validation Loss: 0.0001464973279022767\n",
      "Epoch [10/25], Train Loss: 0.00013981344818603247, Validation Loss: 0.0001462629673672685\n",
      "Epoch [10/25], Train Loss: 0.00013523781672120094, Validation Loss: 0.00014583781206359465\n",
      "Epoch [10/25], Train Loss: 0.00016542281082365662, Validation Loss: 0.00014529237741953694\n",
      "Epoch [10/25], Train Loss: 0.00015609328693244606, Validation Loss: 0.0001447549084938752\n",
      "Epoch [10/25], Train Loss: 0.00017094671784434468, Validation Loss: 0.0001443959054692338\n",
      "Epoch [10/25], Train Loss: 0.00016371948004234582, Validation Loss: 0.00014545932305433477\n",
      "Epoch [10/25], Train Loss: 0.0001513459428679198, Validation Loss: 0.0001475282636723326\n",
      "Epoch [10/25], Train Loss: 0.00010625663708196953, Validation Loss: 0.00015431734379186917\n",
      "Epoch [10/25], Train Loss: 0.00013404584024101496, Validation Loss: 0.0001597819493326824\n",
      "Epoch [10/25], Train Loss: 0.00019592700118664652, Validation Loss: 0.0001583180499437731\n",
      "Epoch [10/25], Train Loss: 0.00021633667347487062, Validation Loss: 0.00014993021395639516\n",
      "Epoch [10/25], Train Loss: 0.00019589265866670758, Validation Loss: 0.0001452244522321659\n",
      "Epoch [10/25], Train Loss: 0.00022700763656757772, Validation Loss: 0.00014885715499985964\n",
      "Epoch [10/25], Train Loss: 0.0001313380489591509, Validation Loss: 0.00015179862020886503\n",
      "Epoch [10/25], Train Loss: 0.00016465235967189074, Validation Loss: 0.0001502691857846609\n",
      "Epoch [10/25], Train Loss: 0.00016384178888984025, Validation Loss: 0.00014550954535176667\n",
      "Epoch [10/25], Train Loss: 0.00016539836360607296, Validation Loss: 0.0001447839456280538\n",
      "Epoch [10/25], Train Loss: 0.00020846104598604143, Validation Loss: 0.00014790465841845918\n",
      "Epoch [10/25], Train Loss: 0.0001395883155055344, Validation Loss: 0.0001494435261217101\n",
      "Epoch [10/25], Train Loss: 0.00017825061513576657, Validation Loss: 0.0001477752147669283\n",
      "Epoch [10/25], Train Loss: 0.00019502965733408928, Validation Loss: 0.00014480195604846813\n",
      "Epoch [10/25], Train Loss: 0.00013473871513269842, Validation Loss: 0.00014604689761957463\n",
      "Epoch [10/25], Train Loss: 0.00013110277359373868, Validation Loss: 0.00014809486019657926\n",
      "Epoch [10/25], Train Loss: 0.00015977212751749903, Validation Loss: 0.0001463532741278565\n",
      "Epoch [10/25], Train Loss: 0.00013496592873707414, Validation Loss: 0.0001445813337340951\n",
      "Epoch [10/25], Train Loss: 9.771934855962172e-05, Validation Loss: 0.0001455406866928873\n",
      "Epoch [10/25], Train Loss: 0.00020897619833704084, Validation Loss: 0.00014618916660159205\n",
      "Epoch [10/25], Train Loss: 0.00017232331447303295, Validation Loss: 0.0001458447974679681\n",
      "Epoch [10/25], Train Loss: 0.00015135551802814007, Validation Loss: 0.00014475023926934226\n",
      "Epoch [10/25], Train Loss: 0.00020020171359647065, Validation Loss: 0.00014444162564662596\n",
      "Epoch [10/25], Train Loss: 0.0001411812991136685, Validation Loss: 0.00014476467501178074\n",
      "Epoch [10/25], Train Loss: 0.00016016539302654564, Validation Loss: 0.0001450695740156031\n",
      "Epoch [10/25], Train Loss: 0.0001522923121228814, Validation Loss: 0.00014500575101313493\n",
      "Epoch [10/25], Train Loss: 0.00012836621317546815, Validation Loss: 0.0001446539841708727\n",
      "Epoch [10/25], Train Loss: 0.0001717580307740718, Validation Loss: 0.00014437484423979186\n",
      "Epoch [10/25], Train Loss: 0.0001468905247747898, Validation Loss: 0.00014440985517770363\n",
      "Epoch [10/25], Train Loss: 0.00012472837988752872, Validation Loss: 0.00014464367559412493\n",
      "Epoch [10/25], Train Loss: 0.00010543786629568785, Validation Loss: 0.00014471886461251416\n",
      "Epoch [10/25], Train Loss: 0.0001326479105046019, Validation Loss: 0.0001447535743257807\n",
      "Epoch [10/25], Train Loss: 0.0001911135477712378, Validation Loss: 0.00014484849792400684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], Train Loss: 0.00017257535364478827, Validation Loss: 0.00014470035093836487\n",
      "Epoch [10/25], Train Loss: 9.831325587583706e-05, Validation Loss: 0.00014446143977693283\n",
      "Epoch [10/25], Train Loss: 9.581125777913257e-05, Validation Loss: 0.0001443486912952115\n",
      "Epoch [10/25], Train Loss: 0.00012378254905343056, Validation Loss: 0.0001443154069420416\n",
      "Epoch [10/25], Train Loss: 0.0001766304048942402, Validation Loss: 0.0001443343993742019\n",
      "Epoch [10/25], Train Loss: 0.00014246164937503636, Validation Loss: 0.00014442307462256092\n",
      "Epoch [10/25], Train Loss: 0.00017347281391266733, Validation Loss: 0.0001444980194113062\n",
      "Epoch [10/25], Train Loss: 0.0001843161298893392, Validation Loss: 0.00014448326959003074\n",
      "Epoch [10/25], Train Loss: 0.00018229849229101092, Validation Loss: 0.0001444866761933857\n",
      "Epoch [10/25], Train Loss: 0.00012436836550477892, Validation Loss: 0.00014449854352278636\n",
      "Epoch [10/25], Train Loss: 0.00015689799329265952, Validation Loss: 0.0001444763448186374\n",
      "Epoch [10/25], Train Loss: 0.00020974413200747222, Validation Loss: 0.0001443862245650962\n",
      "Epoch [10/25], Train Loss: 0.0001396303705405444, Validation Loss: 0.000144314329130187\n",
      "Epoch [10/25], Train Loss: 0.00012839461851399392, Validation Loss: 0.00014430194787564687\n",
      "Epoch [10/25], Train Loss: 0.00012255497858859599, Validation Loss: 0.00014431832169066184\n",
      "Epoch [10/25], Train Loss: 0.00019312086806166917, Validation Loss: 0.00014432751801602233\n",
      "Epoch [10/25], Train Loss: 0.00020954063802491874, Validation Loss: 0.00014429292059503496\n",
      "Epoch [10/25], Train Loss: 0.00019160214287694544, Validation Loss: 0.00014431253997220967\n",
      "Epoch [10/25], Train Loss: 0.0002200231101596728, Validation Loss: 0.000144371879529596\n",
      "Epoch [10/25], Train Loss: 0.00015004316810518503, Validation Loss: 0.00014445747025699046\n",
      "Epoch [10/25], Train Loss: 0.00015622070350218564, Validation Loss: 0.00014444291082327254\n",
      "Epoch [10/25], Train Loss: 0.00014637256390415132, Validation Loss: 0.00014435936876301033\n",
      "Epoch [10/25], Train Loss: 0.00013268434850033373, Validation Loss: 0.00014434848077750454\n",
      "Epoch [10/25], Train Loss: 0.00018171769625041634, Validation Loss: 0.00014438199262561586\n",
      "Epoch [10/25], Train Loss: 0.00019797032291535288, Validation Loss: 0.00014432974057854153\n",
      "Epoch [10/25], Train Loss: 0.00013746044714935124, Validation Loss: 0.0001442818422219716\n",
      "Epoch [10/25], Train Loss: 0.00015589696704410017, Validation Loss: 0.00014426895601597303\n",
      "Epoch [10/25], Train Loss: 0.00012613539001904428, Validation Loss: 0.00014433496301838507\n",
      "Epoch [10/25], Train Loss: 0.0001529274886706844, Validation Loss: 0.0001443733175013525\n",
      "Epoch [10/25], Train Loss: 0.00017139881674665958, Validation Loss: 0.0001443038010620512\n",
      "Epoch [10/25], Train Loss: 0.0001740083534969017, Validation Loss: 0.00014429855606673908\n",
      "Epoch [10/25], Train Loss: 0.00018400183762423694, Validation Loss: 0.0001443413440332127\n",
      "Epoch [10/25], Train Loss: 0.00012644751404877752, Validation Loss: 0.00014432198173987368\n",
      "Epoch [10/25], Train Loss: 0.00016354172839783132, Validation Loss: 0.00014426957665515753\n",
      "Epoch [10/25], Train Loss: 0.00014805322280153632, Validation Loss: 0.0001442595222518624\n",
      "Epoch [10/25], Train Loss: 0.00011769672710215673, Validation Loss: 0.00014428757300872047\n",
      "Epoch [10/25], Train Loss: 0.0001322428579442203, Validation Loss: 0.00014429734195194518\n",
      "Epoch [10/25], Train Loss: 0.00020249273802619427, Validation Loss: 0.0001443435973972858\n",
      "Epoch [10/25], Train Loss: 0.00014257583825383335, Validation Loss: 0.0001444258989067748\n",
      "Epoch [10/25], Train Loss: 0.00011877188080688938, Validation Loss: 0.00014451653138773204\n",
      "Epoch [10/25], Train Loss: 0.00016080310160759836, Validation Loss: 0.00014456110584433192\n",
      "Epoch [10/25], Train Loss: 0.0001161111649707891, Validation Loss: 0.00014464105867470305\n",
      "Epoch [10/25], Train Loss: 0.00016496170428581536, Validation Loss: 0.00014479977132092852\n",
      "Epoch [10/25], Train Loss: 0.00011546949826879427, Validation Loss: 0.0001450580794577642\n",
      "Epoch [10/25], Train Loss: 0.00014619302237406373, Validation Loss: 0.00014536864352218497\n",
      "Epoch [10/25], Train Loss: 0.00016258256800938398, Validation Loss: 0.00014587592401464158\n",
      "Epoch [10/25], Train Loss: 0.000137728828121908, Validation Loss: 0.00014647147472715005\n",
      "Epoch [10/25], Train Loss: 0.00011628736683633178, Validation Loss: 0.00014707363079651258\n",
      "Epoch [10/25], Train Loss: 0.00019630925089586526, Validation Loss: 0.0001473374179719637\n",
      "Epoch [10/25], Train Loss: 0.00016804759798105806, Validation Loss: 0.0001473714534464913\n",
      "Epoch [10/25], Train Loss: 0.00014543381985276937, Validation Loss: 0.0001469050677163371\n",
      "Epoch [10/25], Train Loss: 0.00021114127594046295, Validation Loss: 0.00014640710651292466\n",
      "Epoch [10/25], Train Loss: 0.00019212975166738033, Validation Loss: 0.00014540629514764685\n",
      "Epoch [10/25], Train Loss: 0.00012035511463182047, Validation Loss: 0.00014454030533670448\n",
      "Epoch [10/25], Train Loss: 0.00017345324158668518, Validation Loss: 0.00014426766598868806\n",
      "Epoch [10/25], Train Loss: 0.00016464958025608212, Validation Loss: 0.00014448080182773992\n",
      "Epoch [10/25], Train Loss: 0.00016758349374867976, Validation Loss: 0.00014496250053828893\n",
      "Epoch [10/25], Train Loss: 0.00017792310973163694, Validation Loss: 0.00014537586199973399\n",
      "Epoch [10/25], Train Loss: 0.00016483500075992197, Validation Loss: 0.00014573787460297655\n",
      "Epoch [10/25], Train Loss: 0.00017223189934156835, Validation Loss: 0.00014538812150325005\n",
      "Epoch [10/25], Train Loss: 0.00012977853475604206, Validation Loss: 0.00014475647864552836\n",
      "Epoch [10/25], Train Loss: 0.00013547985872719437, Validation Loss: 0.00014432963119664539\n",
      "Epoch [10/25], Train Loss: 0.0001300697331316769, Validation Loss: 0.00014439244914683514\n",
      "Epoch [10/25], Train Loss: 0.00016412189870607108, Validation Loss: 0.00014477321528829634\n",
      "Epoch [10/25], Train Loss: 0.00021155837748665363, Validation Loss: 0.00014495386688698393\n",
      "Epoch [10/25], Train Loss: 0.00016420123574789613, Validation Loss: 0.00014503049751510844\n",
      "Epoch [10/25], Train Loss: 0.0001505221298430115, Validation Loss: 0.00014485857075972793\n",
      "Epoch [10/25], Train Loss: 0.00015336312935687602, Validation Loss: 0.00014466947056159066\n",
      "Epoch [10/25], Train Loss: 0.0001869826955953613, Validation Loss: 0.00014436681958613918\n",
      "Epoch [10/25], Train Loss: 0.00015005323803052306, Validation Loss: 0.00014425010861790117\n",
      "Epoch [10/25], Train Loss: 0.00011564083979465067, Validation Loss: 0.00014423304431450865\n",
      "Epoch [10/25], Train Loss: 0.00014378789637703449, Validation Loss: 0.00014428878954883355\n",
      "Epoch [10/25], Train Loss: 0.00018436457321513444, Validation Loss: 0.00014440084366166653\n",
      "Epoch [10/25], Train Loss: 0.0001602027623448521, Validation Loss: 0.00014443838784548765\n",
      "Epoch [10/25], Train Loss: 0.00019649940077215433, Validation Loss: 0.00014444208621474294\n",
      "Epoch [10/25], Train Loss: 0.0001820794423110783, Validation Loss: 0.00014442630830065659\n",
      "Epoch [10/25], Train Loss: 0.0002206652716267854, Validation Loss: 0.00014457640257508803\n",
      "Epoch [10/25], Train Loss: 0.00018795323558151722, Validation Loss: 0.00014462485244924513\n",
      "Epoch [10/25], Train Loss: 0.0001759951701387763, Validation Loss: 0.0001447854665457271\n",
      "Epoch [10/25], Train Loss: 0.00015691161388531327, Validation Loss: 0.00014489021583964737\n",
      "Epoch [10/25], Train Loss: 0.00012280252121854573, Validation Loss: 0.00014493497656076215\n",
      "Epoch [10/25], Train Loss: 0.00013933880836702883, Validation Loss: 0.00014508755581725077\n",
      "Epoch [10/25], Train Loss: 0.0001456418540328741, Validation Loss: 0.00014536886228597723\n",
      "Epoch [10/25], Train Loss: 0.00015537947183474898, Validation Loss: 0.00014580707696344082\n",
      "Epoch [10/25], Train Loss: 0.00017880530504044145, Validation Loss: 0.00014603383557793375\n",
      "Epoch [10/25], Train Loss: 0.00015302585961762816, Validation Loss: 0.0001460972972078404\n",
      "Epoch [10/25], Train Loss: 0.00017922508413903415, Validation Loss: 0.00014592608179858264\n",
      "Epoch [10/25], Train Loss: 0.00018110786913894117, Validation Loss: 0.00014573752753979836\n",
      "Epoch [10/25], Train Loss: 6.798539106966928e-05, Validation Loss: 0.00014568931389173181\n",
      "Epoch [10/25], Train Loss: 0.00011412629100959748, Validation Loss: 0.00014597637903837797\n",
      "Epoch [10/25], Train Loss: 9.041441808221862e-05, Validation Loss: 0.00014591977354333114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], Train Loss: 0.00014094298239797354, Validation Loss: 0.00014596982412816336\n",
      "Epoch [10/25], Train Loss: 0.00010949368879664689, Validation Loss: 0.00014591197832487525\n",
      "Epoch [10/25], Train Loss: 0.00018220313359051943, Validation Loss: 0.00014579655495860304\n",
      "Epoch [10/25], Train Loss: 0.0002065880544250831, Validation Loss: 0.00014615856271120719\n",
      "Epoch [10/25], Train Loss: 0.00010583629045868292, Validation Loss: 0.00014602916441314545\n",
      "Epoch [10/25], Train Loss: 0.00012734142364934087, Validation Loss: 0.00014538543206678393\n",
      "Epoch [10/25], Train Loss: 0.00018507275672163814, Validation Loss: 0.00014467257302991737\n",
      "Epoch [10/25], Train Loss: 0.00014522994752041996, Validation Loss: 0.00014442912603650863\n",
      "Epoch [10/25], Train Loss: 0.00019177691137883812, Validation Loss: 0.0001447026637227585\n",
      "Epoch [10/25], Train Loss: 0.00012308471195865422, Validation Loss: 0.00014522259662044234\n",
      "Epoch [10/25], Train Loss: 0.00018311063467990607, Validation Loss: 0.00014585253350863543\n",
      "Epoch [10/25], Train Loss: 0.00013066157407592982, Validation Loss: 0.0001461598423096196\n",
      "Epoch [10/25], Train Loss: 0.00013770708756055683, Validation Loss: 0.00014637274677321935\n",
      "Epoch [10/25], Train Loss: 0.0002184250915888697, Validation Loss: 0.00014578696257861641\n",
      "Epoch [10/25], Train Loss: 0.00013842704356648028, Validation Loss: 0.0001451198356032061\n",
      "Epoch [10/25], Train Loss: 0.0001113473335863091, Validation Loss: 0.0001445932869197956\n",
      "Epoch [10/25], Train Loss: 0.00014501839177682996, Validation Loss: 0.0001443343307376684\n",
      "Epoch [10/25], Train Loss: 0.00016855326248332858, Validation Loss: 0.00014429505899897775\n",
      "Epoch [10/25], Train Loss: 0.00015084010374266654, Validation Loss: 0.00014447240197720628\n",
      "Epoch [10/25], Train Loss: 9.477259300183505e-05, Validation Loss: 0.00014464294969608696\n",
      "Epoch [10/25], Train Loss: 0.00014955854567233473, Validation Loss: 0.0001447477043257095\n",
      "Epoch [10/25], Train Loss: 0.0001852974819485098, Validation Loss: 0.00014482114565907977\n",
      "Epoch [10/25], Train Loss: 0.00015203333168756217, Validation Loss: 0.0001448082167674632\n",
      "Epoch [10/25], Train Loss: 0.0001463615772081539, Validation Loss: 0.00014477580577173892\n",
      "Epoch [10/25], Train Loss: 0.000201065864530392, Validation Loss: 0.00014459045827000712\n",
      "Epoch [10/25], Train Loss: 0.00016805532504804432, Validation Loss: 0.0001446813553532896\n",
      "Epoch [10/25], Train Loss: 0.00014402347733266652, Validation Loss: 0.0001448497127663965\n",
      "Epoch [10/25], Train Loss: 0.00014587205077987164, Validation Loss: 0.00014505178817974713\n",
      "Epoch [10/25], Train Loss: 0.00015945562336128205, Validation Loss: 0.00014530654977230976\n",
      "Epoch [10/25], Train Loss: 0.00017965849838219583, Validation Loss: 0.0001458145564053363\n",
      "Epoch [10/25], Train Loss: 0.00013312204100657254, Validation Loss: 0.0001464111182334212\n",
      "Epoch [10/25], Train Loss: 0.00016038178000599146, Validation Loss: 0.000147144680765147\n",
      "Epoch [10/25], Train Loss: 0.0001411678531439975, Validation Loss: 0.000147428220952861\n",
      "Epoch [10/25], Train Loss: 0.00019392379908822477, Validation Loss: 0.0001470433511713054\n",
      "Epoch [10/25], Train Loss: 0.00014497109805233777, Validation Loss: 0.00014611340933091317\n",
      "Epoch [10/25], Train Loss: 0.00019312833319418132, Validation Loss: 0.00014520419984667872\n",
      "Epoch [10/25], Train Loss: 0.00012066178169334307, Validation Loss: 0.00014453650340631914\n",
      "Epoch [10/25], Train Loss: 0.00014234360423870385, Validation Loss: 0.00014420177079349134\n",
      "Epoch [10/25], Train Loss: 0.0001732442615320906, Validation Loss: 0.00014416938938666135\n",
      "Epoch [10/25], Train Loss: 0.00017271492106374353, Validation Loss: 0.0001443812737610036\n",
      "Epoch [10/25], Train Loss: 0.0001520840305602178, Validation Loss: 0.00014484404139996815\n",
      "Epoch [10/25], Train Loss: 0.00014413008466362953, Validation Loss: 0.00014527589276743433\n",
      "Epoch [10/25], Train Loss: 0.0001519424404250458, Validation Loss: 0.00014572324604766133\n",
      "Epoch [10/25], Train Loss: 0.0001383334893034771, Validation Loss: 0.00014595306978056519\n",
      "Epoch [10/25], Train Loss: 0.00016945734387263656, Validation Loss: 0.00014600958359854607\n",
      "Epoch [10/25], Train Loss: 0.00011982450087089092, Validation Loss: 0.0001457234163050695\n",
      "Epoch [10/25], Train Loss: 0.0001886877726064995, Validation Loss: 0.00014512801038411757\n",
      "Epoch [10/25], Train Loss: 0.00014380746870301664, Validation Loss: 0.00014449730224441737\n",
      "Epoch [10/25], Train Loss: 0.00017550903430674225, Validation Loss: 0.00014422708773054183\n",
      "Epoch [10/25], Train Loss: 0.00013160183152649552, Validation Loss: 0.00014433898007458387\n",
      "Epoch [10/25], Train Loss: 0.00016839345335029066, Validation Loss: 0.0001446255310535586\n",
      "Epoch [10/25], Train Loss: 0.00016077084001153708, Validation Loss: 0.0001450963723376238\n",
      "Epoch [10/25], Train Loss: 0.00018461165018379688, Validation Loss: 0.00014535009783382216\n",
      "Epoch [10/25], Train Loss: 0.00012219842756167054, Validation Loss: 0.00014551583299180493\n",
      "Epoch [10/25], Train Loss: 0.0001621119154151529, Validation Loss: 0.00014543223999983942\n",
      "Epoch [10/25], Train Loss: 0.00012389254698064178, Validation Loss: 0.0001452382037920567\n",
      "Epoch [10/25], Train Loss: 0.00015761460235808045, Validation Loss: 0.00014515483829503258\n",
      "Epoch [10/25], Train Loss: 0.000133806504891254, Validation Loss: 0.00014490969988401048\n",
      "Epoch [10/25], Train Loss: 0.00014632008969783783, Validation Loss: 0.00014457790918337803\n",
      "Epoch [10/25], Train Loss: 0.00013888903777115047, Validation Loss: 0.00014429448225807088\n",
      "Epoch [10/25], Train Loss: 0.00017372905858792365, Validation Loss: 0.0001441576110664755\n",
      "Epoch [10/25], Train Loss: 0.00016049295663833618, Validation Loss: 0.00014421432133531197\n",
      "Epoch [10/25], Train Loss: 0.0001396442239638418, Validation Loss: 0.00014436906421906316\n",
      "Epoch [10/25], Train Loss: 0.00011861954408232123, Validation Loss: 0.00014462287799688056\n",
      "Epoch [10/25], Train Loss: 0.00017065797874238342, Validation Loss: 0.000144938377343351\n",
      "Epoch [10/25], Train Loss: 0.00019401618919800967, Validation Loss: 0.0001450946047649874\n",
      "Epoch [10/25], Train Loss: 0.00019122316734865308, Validation Loss: 0.00014528717583743854\n",
      "Epoch [10/25], Train Loss: 0.00014046831347513944, Validation Loss: 0.0001453201441715161\n",
      "Epoch [10/25], Train Loss: 9.3045127869118e-05, Validation Loss: 0.00014525269434670918\n",
      "Epoch [10/25], Train Loss: 0.00013729043712373823, Validation Loss: 0.00014512290338946816\n",
      "Epoch [10/25], Train Loss: 0.00014505181752610952, Validation Loss: 0.00014503858061895396\n",
      "Epoch [10/25], Train Loss: 0.00020602229051291943, Validation Loss: 0.0001448808608984109\n",
      "Epoch [10/25], Train Loss: 0.00015625891683157533, Validation Loss: 0.00014466163847828283\n",
      "Epoch [10/25], Train Loss: 0.00010666278103599325, Validation Loss: 0.0001444484674721025\n",
      "Epoch [10/25], Train Loss: 0.0001794779091142118, Validation Loss: 0.00014429323079336124\n",
      "Epoch [10/25], Train Loss: 0.00024134972773026675, Validation Loss: 0.00014427606438403018\n",
      "Epoch [10/25], Train Loss: 0.00012632329890038818, Validation Loss: 0.0001442960807859587\n",
      "Epoch [10/25], Train Loss: 0.0001308321370743215, Validation Loss: 0.00014425805954185005\n",
      "Epoch [10/25], Train Loss: 0.00024715953622944653, Validation Loss: 0.0001443512781406753\n",
      "Epoch [10/25], Train Loss: 0.00018283365352544934, Validation Loss: 0.00014464224417073032\n",
      "Epoch [10/25], Train Loss: 0.00014728917449247092, Validation Loss: 0.0001450066217027294\n",
      "Epoch [10/25], Train Loss: 0.00018207493121735752, Validation Loss: 0.00014571777234474818\n",
      "Epoch [10/25], Train Loss: 0.00012121906911488622, Validation Loss: 0.00014727579497654612\n",
      "Epoch [10/25], Train Loss: 0.0001408100506523624, Validation Loss: 0.0001498891152247476\n",
      "Epoch [10/25], Train Loss: 0.0001640580885577947, Validation Loss: 0.00015333752962760628\n",
      "Epoch [10/25], Train Loss: 0.00012952440010849386, Validation Loss: 0.00015744893268371623\n",
      "Epoch [10/25], Train Loss: 0.00019241236441303045, Validation Loss: 0.00015782911602097254\n",
      "Epoch [10/25], Train Loss: 0.0001376041618641466, Validation Loss: 0.0001525397223304026\n",
      "Epoch [10/25], Train Loss: 0.00016079703345894814, Validation Loss: 0.0001456414118971831\n",
      "Epoch [10/25], Train Loss: 0.0001282093144254759, Validation Loss: 0.0001455638632857396\n",
      "Epoch [10/25], Train Loss: 0.00013761961599811912, Validation Loss: 0.0001500467585477357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], Train Loss: 0.0001769584050634876, Validation Loss: 0.00014971435545400405\n",
      "Epoch [10/25], Train Loss: 0.00012665924441535026, Validation Loss: 0.00014556507885572502\n",
      "Epoch [10/25], Train Loss: 0.00016557076014578342, Validation Loss: 0.00014515080668691855\n",
      "Epoch [10/25], Train Loss: 7.05365528119728e-05, Validation Loss: 0.00014935040502071691\n",
      "Epoch [10/25], Train Loss: 0.00014598306734114885, Validation Loss: 0.00015039988356875257\n",
      "Epoch [10/25], Train Loss: 0.0002448901068419218, Validation Loss: 0.0001454488214221783\n",
      "Epoch [10/25], Train Loss: 0.00016952157602645457, Validation Loss: 0.000145367844379507\n",
      "Epoch [10/25], Train Loss: 0.00020569510525092483, Validation Loss: 0.00014958568426663986\n",
      "Epoch [10/25], Train Loss: 0.00011306766828056425, Validation Loss: 0.00014878482931332353\n",
      "Epoch [10/25], Train Loss: 0.00017773699073586613, Validation Loss: 0.000145358774655809\n",
      "Epoch [10/25], Train Loss: 0.00012324411363806576, Validation Loss: 0.00014511903718812392\n",
      "Epoch [10/25], Train Loss: 7.063354860292748e-05, Validation Loss: 0.00014795091580405522\n",
      "Epoch [10/25], Train Loss: 0.00012018804409308359, Validation Loss: 0.00014775488089071586\n",
      "Epoch [10/25], Train Loss: 0.0001743161992635578, Validation Loss: 0.00014481221054059765\n",
      "Epoch [10/25], Train Loss: 0.00014505202125292271, Validation Loss: 0.00014511487024719826\n",
      "Epoch [11/25], Train Loss: 0.00016292996588163078, Validation Loss: 0.00014721240077051335\n",
      "Epoch [11/25], Train Loss: 0.00017424662655685097, Validation Loss: 0.00014682104641300004\n",
      "Epoch [11/25], Train Loss: 9.973288979381323e-05, Validation Loss: 0.0001449083235153618\n",
      "Epoch [11/25], Train Loss: 0.00014524432481266558, Validation Loss: 0.00014439478448669736\n",
      "Epoch [11/25], Train Loss: 0.00011661149619612843, Validation Loss: 0.00014578043410438113\n",
      "Epoch [11/25], Train Loss: 0.00013506632240023464, Validation Loss: 0.0001470223091018852\n",
      "Epoch [11/25], Train Loss: 0.0001437892933608964, Validation Loss: 0.00014635794480758098\n",
      "Epoch [11/25], Train Loss: 0.0001224933803314343, Validation Loss: 0.0001447545386326965\n",
      "Epoch [11/25], Train Loss: 0.00011832050222437829, Validation Loss: 0.00014431039526243694\n",
      "Epoch [11/25], Train Loss: 0.00018760483362711966, Validation Loss: 0.0001451896603005783\n",
      "Epoch [11/25], Train Loss: 0.00014233625552151352, Validation Loss: 0.0001456591145445903\n",
      "Epoch [11/25], Train Loss: 0.00020377206965349615, Validation Loss: 0.00014477869966261398\n",
      "Epoch [11/25], Train Loss: 0.00010716333781601861, Validation Loss: 0.0001441543201508466\n",
      "Epoch [11/25], Train Loss: 0.0001551988534629345, Validation Loss: 0.00014460192057110058\n",
      "Epoch [11/25], Train Loss: 0.00018299288058187813, Validation Loss: 0.00014511961489915848\n",
      "Epoch [11/25], Train Loss: 0.0001750040682964027, Validation Loss: 0.0001458259978486846\n",
      "Epoch [11/25], Train Loss: 0.00012874635285697877, Validation Loss: 0.00014592112396106434\n",
      "Epoch [11/25], Train Loss: 0.00017640086298342794, Validation Loss: 0.0001457677213087057\n",
      "Epoch [11/25], Train Loss: 0.0001455476594856009, Validation Loss: 0.00014487988179704794\n",
      "Epoch [11/25], Train Loss: 0.0001566534920129925, Validation Loss: 0.00014427429996430874\n",
      "Epoch [11/25], Train Loss: 0.00015572554548271, Validation Loss: 0.00014454813814760806\n",
      "Epoch [11/25], Train Loss: 0.0001367182267131284, Validation Loss: 0.0001450947597428846\n",
      "Epoch [11/25], Train Loss: 0.00014504673890769482, Validation Loss: 0.0001453510730546744\n",
      "Epoch [11/25], Train Loss: 0.00013077897892799228, Validation Loss: 0.00014510808953976568\n",
      "Epoch [11/25], Train Loss: 0.00013695460802409798, Validation Loss: 0.0001446582449716516\n",
      "Epoch [11/25], Train Loss: 0.0001841646444518119, Validation Loss: 0.00014426402170405103\n",
      "Epoch [11/25], Train Loss: 0.00017231340461876243, Validation Loss: 0.0001441395225507828\n",
      "Epoch [11/25], Train Loss: 0.00014217071293387562, Validation Loss: 0.00014419136156599657\n",
      "Epoch [11/25], Train Loss: 0.0001263835874851793, Validation Loss: 0.0001443950301715328\n",
      "Epoch [11/25], Train Loss: 0.00016229796165134758, Validation Loss: 0.00014468411876199146\n",
      "Epoch [11/25], Train Loss: 0.00015916604024823755, Validation Loss: 0.00014481130589653426\n",
      "Epoch [11/25], Train Loss: 0.00017196718545164913, Validation Loss: 0.00014480664273529935\n",
      "Epoch [11/25], Train Loss: 0.0001243443985003978, Validation Loss: 0.00014459642067474003\n",
      "Epoch [11/25], Train Loss: 0.0001454376324545592, Validation Loss: 0.00014428288171378274\n",
      "Epoch [11/25], Train Loss: 0.00015829212497919798, Validation Loss: 0.00014410606842526856\n",
      "Epoch [11/25], Train Loss: 0.0001647988974582404, Validation Loss: 0.00014412287418963388\n",
      "Epoch [11/25], Train Loss: 0.00019326839537825435, Validation Loss: 0.0001443004247751863\n",
      "Epoch [11/25], Train Loss: 0.0001362973707728088, Validation Loss: 0.00014446428467635998\n",
      "Epoch [11/25], Train Loss: 0.0001150837997556664, Validation Loss: 0.0001444043307856191\n",
      "Epoch [11/25], Train Loss: 0.00018025901226792485, Validation Loss: 0.00014426913209414732\n",
      "Epoch [11/25], Train Loss: 0.0001919310016091913, Validation Loss: 0.0001442043680678277\n",
      "Epoch [11/25], Train Loss: 9.939125447999686e-05, Validation Loss: 0.00014414105438239252\n",
      "Epoch [11/25], Train Loss: 0.00016536860493943095, Validation Loss: 0.0001440940815276311\n",
      "Epoch [11/25], Train Loss: 0.0001361048489343375, Validation Loss: 0.00014407266353373417\n",
      "Epoch [11/25], Train Loss: 0.00013900686462875456, Validation Loss: 0.00014411239802332906\n",
      "Epoch [11/25], Train Loss: 0.00016116038023028523, Validation Loss: 0.00014416889486407552\n",
      "Epoch [11/25], Train Loss: 0.00015354971401393414, Validation Loss: 0.0001441995217949928\n",
      "Epoch [11/25], Train Loss: 0.00020608263730537146, Validation Loss: 0.00014422781217338828\n",
      "Epoch [11/25], Train Loss: 0.0001268996566068381, Validation Loss: 0.00014427303807072652\n",
      "Epoch [11/25], Train Loss: 0.0001366213837172836, Validation Loss: 0.00014428141027262125\n",
      "Epoch [11/25], Train Loss: 8.295095176436007e-05, Validation Loss: 0.00014422571378721236\n",
      "Epoch [11/25], Train Loss: 8.825177792459726e-05, Validation Loss: 0.00014410369694815018\n",
      "Epoch [11/25], Train Loss: 0.00015510740922763944, Validation Loss: 0.0001440706796226247\n",
      "Epoch [11/25], Train Loss: 0.0002005138376262039, Validation Loss: 0.00014417551841082362\n",
      "Epoch [11/25], Train Loss: 0.00017629378999117762, Validation Loss: 0.0001442320026399102\n",
      "Epoch [11/25], Train Loss: 0.00011966498277615756, Validation Loss: 0.00014418728872745608\n",
      "Epoch [11/25], Train Loss: 0.00014338413893710822, Validation Loss: 0.00014415997623776395\n",
      "Epoch [11/25], Train Loss: 0.00014706264482811093, Validation Loss: 0.00014422115612736282\n",
      "Epoch [11/25], Train Loss: 0.00016497696924488991, Validation Loss: 0.00014423423829915312\n",
      "Epoch [11/25], Train Loss: 0.00012968530063517392, Validation Loss: 0.00014419065773836338\n",
      "Epoch [11/25], Train Loss: 9.582007623976097e-05, Validation Loss: 0.00014409840247632625\n",
      "Epoch [11/25], Train Loss: 0.0001254050002899021, Validation Loss: 0.00014409780584780189\n",
      "Epoch [11/25], Train Loss: 0.00015125049685593694, Validation Loss: 0.00014413190559328843\n",
      "Epoch [11/25], Train Loss: 0.00013642621343024075, Validation Loss: 0.00014412985983653926\n",
      "Epoch [11/25], Train Loss: 0.00013486968236975372, Validation Loss: 0.00014409967819422793\n",
      "Epoch [11/25], Train Loss: 0.00016184485866688192, Validation Loss: 0.00014407092166948132\n",
      "Epoch [11/25], Train Loss: 0.0002524683077353984, Validation Loss: 0.00014417490941317132\n",
      "Epoch [11/25], Train Loss: 0.00012876634718850255, Validation Loss: 0.00014423681423068047\n",
      "Epoch [11/25], Train Loss: 0.00012031177175231278, Validation Loss: 0.00014418689922119182\n",
      "Epoch [11/25], Train Loss: 0.0001894335582619533, Validation Loss: 0.00014411351730814204\n",
      "Epoch [11/25], Train Loss: 0.00017860282969195396, Validation Loss: 0.00014410179889334055\n",
      "Epoch [11/25], Train Loss: 0.00012897196575067937, Validation Loss: 0.0001441013562725857\n",
      "Epoch [11/25], Train Loss: 0.00019426853395998478, Validation Loss: 0.0001440964105616634\n",
      "Epoch [11/25], Train Loss: 0.0001741052110446617, Validation Loss: 0.00014411102407999957\n",
      "Epoch [11/25], Train Loss: 0.00011456571519374847, Validation Loss: 0.00014411490386313138\n",
      "Epoch [11/25], Train Loss: 0.00015998944581951946, Validation Loss: 0.00014411905067390762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], Train Loss: 0.0002325487439520657, Validation Loss: 0.00014407967052344853\n",
      "Epoch [11/25], Train Loss: 9.926372877089307e-05, Validation Loss: 0.0001440748771225723\n",
      "Epoch [11/25], Train Loss: 0.00011365874524926767, Validation Loss: 0.00014410185225036305\n",
      "Epoch [11/25], Train Loss: 0.00011742489732569084, Validation Loss: 0.00014415721088880673\n",
      "Epoch [11/25], Train Loss: 0.00015456948312930763, Validation Loss: 0.000144211597944377\n",
      "Epoch [11/25], Train Loss: 9.466538176639006e-05, Validation Loss: 0.00014426173826601976\n",
      "Epoch [11/25], Train Loss: 0.00012479760334827006, Validation Loss: 0.00014428110443986952\n",
      "Epoch [11/25], Train Loss: 0.00013792907702736557, Validation Loss: 0.00014430087564202646\n",
      "Epoch [11/25], Train Loss: 0.00011396044283173978, Validation Loss: 0.00014432814180812177\n",
      "Epoch [11/25], Train Loss: 0.00011759407789213583, Validation Loss: 0.00014440238956012764\n",
      "Epoch [11/25], Train Loss: 0.00015917005657684058, Validation Loss: 0.00014443734520076153\n",
      "Epoch [11/25], Train Loss: 0.0001847322564572096, Validation Loss: 0.0001444487543873644\n",
      "Epoch [11/25], Train Loss: 0.00018271309090778232, Validation Loss: 0.00014462235412793234\n",
      "Epoch [11/25], Train Loss: 0.0001155448189820163, Validation Loss: 0.0001447701773334605\n",
      "Epoch [11/25], Train Loss: 0.00017704830679576844, Validation Loss: 0.0001447574970370624\n",
      "Epoch [11/25], Train Loss: 0.0001682284928392619, Validation Loss: 0.00014484060084214435\n",
      "Epoch [11/25], Train Loss: 0.0001825902727432549, Validation Loss: 0.00014501743159295682\n",
      "Epoch [11/25], Train Loss: 0.00021341149113141, Validation Loss: 0.00014536025604077924\n",
      "Epoch [11/25], Train Loss: 0.00013995508197695017, Validation Loss: 0.00014584658517075393\n",
      "Epoch [11/25], Train Loss: 0.00015055507537908852, Validation Loss: 0.00014647458228864708\n",
      "Epoch [11/25], Train Loss: 0.00012923778558615595, Validation Loss: 0.0001472068761358969\n",
      "Epoch [11/25], Train Loss: 0.00016158042126335204, Validation Loss: 0.00014782552728623461\n",
      "Epoch [11/25], Train Loss: 0.00017649540677666664, Validation Loss: 0.0001480685947171878\n",
      "Epoch [11/25], Train Loss: 0.00024059100542217493, Validation Loss: 0.00014731705232406966\n",
      "Epoch [11/25], Train Loss: 9.348772437078878e-05, Validation Loss: 0.00014612677162707162\n",
      "Epoch [11/25], Train Loss: 0.00013046528329141438, Validation Loss: 0.0001450519026548136\n",
      "Epoch [11/25], Train Loss: 0.00018080396694131196, Validation Loss: 0.0001444752737976766\n",
      "Epoch [11/25], Train Loss: 0.00018740993982646614, Validation Loss: 0.00014431130791005368\n",
      "Epoch [11/25], Train Loss: 0.00017290504183620214, Validation Loss: 0.00014444040740878942\n",
      "Epoch [11/25], Train Loss: 0.0001553585461806506, Validation Loss: 0.00014469164646773908\n",
      "Epoch [11/25], Train Loss: 0.00015070120571181178, Validation Loss: 0.00014479898430484658\n",
      "Epoch [11/25], Train Loss: 0.00015556628932245076, Validation Loss: 0.00014477083192711385\n",
      "Epoch [11/25], Train Loss: 0.00019345020700711757, Validation Loss: 0.00014446811449791614\n",
      "Epoch [11/25], Train Loss: 9.104670607484877e-05, Validation Loss: 0.00014419779715050632\n",
      "Epoch [11/25], Train Loss: 9.880014113150537e-05, Validation Loss: 0.00014428274807869456\n",
      "Epoch [11/25], Train Loss: 0.0001947840937646106, Validation Loss: 0.0001442712574013664\n",
      "Epoch [11/25], Train Loss: 0.00017310050316154957, Validation Loss: 0.00014419190944560493\n",
      "Epoch [11/25], Train Loss: 0.0002451661857776344, Validation Loss: 0.0001441680513380561\n",
      "Epoch [11/25], Train Loss: 0.00015126136713661253, Validation Loss: 0.00014429254224523902\n",
      "Epoch [11/25], Train Loss: 0.00010169145389227197, Validation Loss: 0.00014429212096729317\n",
      "Epoch [11/25], Train Loss: 0.0001854793372331187, Validation Loss: 0.0001441772854983962\n",
      "Epoch [11/25], Train Loss: 0.0001338870933977887, Validation Loss: 0.00014409601183918615\n",
      "Epoch [11/25], Train Loss: 0.00017050224414560944, Validation Loss: 0.00014407017806661315\n",
      "Epoch [11/25], Train Loss: 0.000109852735477034, Validation Loss: 0.00014422161596788403\n",
      "Epoch [11/25], Train Loss: 0.0001494877360528335, Validation Loss: 0.00014451119107737517\n",
      "Epoch [11/25], Train Loss: 0.00017594436940271407, Validation Loss: 0.0001447218540609659\n",
      "Epoch [11/25], Train Loss: 0.0001830749970395118, Validation Loss: 0.00014499084500130266\n",
      "Epoch [11/25], Train Loss: 0.0001443532237317413, Validation Loss: 0.00014518317936259943\n",
      "Epoch [11/25], Train Loss: 0.00011297071614535525, Validation Loss: 0.00014538269897457212\n",
      "Epoch [11/25], Train Loss: 0.0001399850007146597, Validation Loss: 0.00014550574827201974\n",
      "Epoch [11/25], Train Loss: 0.00017333410505671054, Validation Loss: 0.00014569471095455812\n",
      "Epoch [11/25], Train Loss: 0.00017206261691171676, Validation Loss: 0.00014581895860222479\n",
      "Epoch [11/25], Train Loss: 0.00016368305659852922, Validation Loss: 0.00014610201881926817\n",
      "Epoch [11/25], Train Loss: 0.00014564278535544872, Validation Loss: 0.00014641798964779202\n",
      "Epoch [11/25], Train Loss: 9.863788000075147e-05, Validation Loss: 0.0001467338906271228\n",
      "Epoch [11/25], Train Loss: 0.00012859582784585655, Validation Loss: 0.0001467636871287444\n",
      "Epoch [11/25], Train Loss: 0.0001819354947656393, Validation Loss: 0.00014663989495602436\n",
      "Epoch [11/25], Train Loss: 0.0001738229184411466, Validation Loss: 0.00014599090112218013\n",
      "Epoch [11/25], Train Loss: 0.00015151248953770846, Validation Loss: 0.00014511951546107107\n",
      "Epoch [11/25], Train Loss: 0.00021626001398544759, Validation Loss: 0.00014431096060434355\n",
      "Epoch [11/25], Train Loss: 0.00016342267917934805, Validation Loss: 0.00014402478797516475\n",
      "Epoch [11/25], Train Loss: 0.00019196745415683836, Validation Loss: 0.00014444118132814766\n",
      "Epoch [11/25], Train Loss: 0.00010634529462549835, Validation Loss: 0.00014544039295287802\n",
      "Epoch [11/25], Train Loss: 0.00012303014227654785, Validation Loss: 0.00014641907740345534\n",
      "Epoch [11/25], Train Loss: 0.00016391926328651607, Validation Loss: 0.00014701552912204836\n",
      "Epoch [11/25], Train Loss: 0.00012375455116853118, Validation Loss: 0.00014731987175764516\n",
      "Epoch [11/25], Train Loss: 0.0001227096508955583, Validation Loss: 0.00014712554232877058\n",
      "Epoch [11/25], Train Loss: 0.0001369382080156356, Validation Loss: 0.0001461740391581164\n",
      "Epoch [11/25], Train Loss: 0.0001471566647524014, Validation Loss: 0.00014483880828872013\n",
      "Epoch [11/25], Train Loss: 0.00015973311383277178, Validation Loss: 0.00014426991886769733\n",
      "Epoch [11/25], Train Loss: 9.329113527201116e-05, Validation Loss: 0.00014500054094241933\n",
      "Epoch [11/25], Train Loss: 0.00011656955757644027, Validation Loss: 0.00014591917618721103\n",
      "Epoch [11/25], Train Loss: 0.0001308578357566148, Validation Loss: 0.00014586227165030625\n",
      "Epoch [11/25], Train Loss: 0.0001638140092836693, Validation Loss: 0.00014485631594046328\n",
      "Epoch [11/25], Train Loss: 0.00018756440840661526, Validation Loss: 0.00014424701269793633\n",
      "Epoch [11/25], Train Loss: 0.00016982016677502543, Validation Loss: 0.0001442753690450142\n",
      "Epoch [11/25], Train Loss: 0.0001468730333726853, Validation Loss: 0.00014497000665869564\n",
      "Epoch [11/25], Train Loss: 0.00018333291518501937, Validation Loss: 0.00014605534379370512\n",
      "Epoch [11/25], Train Loss: 0.00014883140102028847, Validation Loss: 0.000146336078614695\n",
      "Epoch [11/25], Train Loss: 0.0001114881961257197, Validation Loss: 0.00014541909476974979\n",
      "Epoch [11/25], Train Loss: 0.000168041413417086, Validation Loss: 0.00014456507293895507\n",
      "Epoch [11/25], Train Loss: 0.00012121492181904614, Validation Loss: 0.00014418224042553145\n",
      "Epoch [11/25], Train Loss: 0.00013988172577228397, Validation Loss: 0.0001446119439303099\n",
      "Epoch [11/25], Train Loss: 0.00023725592473056167, Validation Loss: 0.00014513757438786948\n",
      "Epoch [11/25], Train Loss: 0.00016541866352781653, Validation Loss: 0.00014549807892763055\n",
      "Epoch [11/25], Train Loss: 0.00012814707588404417, Validation Loss: 0.00014520590242076044\n",
      "Epoch [11/25], Train Loss: 0.00015536638966295868, Validation Loss: 0.00014466137557368104\n",
      "Epoch [11/25], Train Loss: 0.0001292476081289351, Validation Loss: 0.00014423069515032694\n",
      "Epoch [11/25], Train Loss: 0.00017309089889749885, Validation Loss: 0.0001441638897328327\n",
      "Epoch [11/25], Train Loss: 0.00011597602861002088, Validation Loss: 0.00014438568565916892\n",
      "Epoch [11/25], Train Loss: 0.00011834480392280966, Validation Loss: 0.00014474139946590487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], Train Loss: 0.0002079707628581673, Validation Loss: 0.00014522605609575598\n",
      "Epoch [11/25], Train Loss: 0.00014408089919015765, Validation Loss: 0.00014537885581376032\n",
      "Epoch [11/25], Train Loss: 0.0001576208887854591, Validation Loss: 0.00014527527237078174\n",
      "Epoch [11/25], Train Loss: 0.00018130794342141598, Validation Loss: 0.00014492108675767668\n",
      "Epoch [11/25], Train Loss: 0.0001555818016640842, Validation Loss: 0.00014453486025255795\n",
      "Epoch [11/25], Train Loss: 0.0001506309927208349, Validation Loss: 0.00014418655894890737\n",
      "Epoch [11/25], Train Loss: 0.00017614364332985133, Validation Loss: 0.00014406691892266584\n",
      "Epoch [11/25], Train Loss: 0.00021735165501013398, Validation Loss: 0.00014411925997895498\n",
      "Epoch [11/25], Train Loss: 0.00016582572425249964, Validation Loss: 0.00014418238157910915\n",
      "Epoch [11/25], Train Loss: 0.0001444906956749037, Validation Loss: 0.00014423300575193329\n",
      "Epoch [11/25], Train Loss: 0.00012393163342494518, Validation Loss: 0.00014430314210282327\n",
      "Epoch [11/25], Train Loss: 0.000147095721331425, Validation Loss: 0.000144438148951546\n",
      "Epoch [11/25], Train Loss: 0.00014854557230137289, Validation Loss: 0.00014447920960568202\n",
      "Epoch [11/25], Train Loss: 0.00016398122534155846, Validation Loss: 0.0001444652671731698\n",
      "Epoch [11/25], Train Loss: 0.00011176088446518406, Validation Loss: 0.00014442256942857057\n",
      "Epoch [11/25], Train Loss: 9.145127114607021e-05, Validation Loss: 0.00014438665966736152\n",
      "Epoch [11/25], Train Loss: 0.0001682120782788843, Validation Loss: 0.0001443527047134315\n",
      "Epoch [11/25], Train Loss: 0.00011333545990055427, Validation Loss: 0.0001443534771775982\n",
      "Epoch [11/25], Train Loss: 0.00010260887211188674, Validation Loss: 0.00014435975487382772\n",
      "Epoch [11/25], Train Loss: 0.0002068230096483603, Validation Loss: 0.0001443212582671549\n",
      "Epoch [11/25], Train Loss: 0.00016737265104893595, Validation Loss: 0.00014429828806896693\n",
      "Epoch [11/25], Train Loss: 0.00015143041673582047, Validation Loss: 0.00014436982940727223\n",
      "Epoch [11/25], Train Loss: 0.00012302788672968745, Validation Loss: 0.00014440139711950905\n",
      "Epoch [11/25], Train Loss: 0.00012234266614541411, Validation Loss: 0.00014437776480917818\n",
      "Epoch [11/25], Train Loss: 0.00015787643496878445, Validation Loss: 0.00014433120110576663\n",
      "Epoch [11/25], Train Loss: 0.0001638022076804191, Validation Loss: 0.00014429158763960004\n",
      "Epoch [11/25], Train Loss: 0.0001763518957886845, Validation Loss: 0.00014425029803533107\n",
      "Epoch [11/25], Train Loss: 0.0001694923994364217, Validation Loss: 0.00014418851424125023\n",
      "Epoch [11/25], Train Loss: 0.0001866720849648118, Validation Loss: 0.00014412054588319734\n",
      "Epoch [11/25], Train Loss: 0.0001295745896641165, Validation Loss: 0.00014403654907558424\n",
      "Epoch [11/25], Train Loss: 0.00020419262000359595, Validation Loss: 0.00014398820203496144\n",
      "Epoch [11/25], Train Loss: 0.0001899302878882736, Validation Loss: 0.00014400604535088254\n",
      "Epoch [11/25], Train Loss: 0.00018571280816104263, Validation Loss: 0.00014401534960294763\n",
      "Epoch [11/25], Train Loss: 0.00012908020289614797, Validation Loss: 0.00014404356431138392\n",
      "Epoch [11/25], Train Loss: 0.00012901202717330307, Validation Loss: 0.00014409121407273537\n",
      "Epoch [11/25], Train Loss: 0.00012738264922518283, Validation Loss: 0.00014417289081999722\n",
      "Epoch [11/25], Train Loss: 0.00017404102254658937, Validation Loss: 0.0001443305540306028\n",
      "Epoch [11/25], Train Loss: 0.00018831035413313657, Validation Loss: 0.0001444568927884878\n",
      "Epoch [11/25], Train Loss: 0.0001907516416395083, Validation Loss: 0.00014459902255718285\n",
      "Epoch [11/25], Train Loss: 0.0001262049045180902, Validation Loss: 0.00014470907699433156\n",
      "Epoch [11/25], Train Loss: 0.0001535821647848934, Validation Loss: 0.00014481152199247543\n",
      "Epoch [11/25], Train Loss: 0.00017615320393815637, Validation Loss: 0.00014496408839477227\n",
      "Epoch [11/25], Train Loss: 0.00012610788689926267, Validation Loss: 0.00014520859913318417\n",
      "Epoch [11/25], Train Loss: 0.0001871306449174881, Validation Loss: 0.00014543923001231936\n",
      "Epoch [11/25], Train Loss: 0.00014682981418445706, Validation Loss: 0.0001459040298262456\n",
      "Epoch [11/25], Train Loss: 0.00015428397455252707, Validation Loss: 0.00014641531476324113\n",
      "Epoch [11/25], Train Loss: 9.806994785321876e-05, Validation Loss: 0.0001470350655533063\n",
      "Epoch [11/25], Train Loss: 0.00020376114116515964, Validation Loss: 0.00014723354615853168\n",
      "Epoch [11/25], Train Loss: 0.00020274092094041407, Validation Loss: 0.0001478839966390903\n",
      "Epoch [11/25], Train Loss: 0.00019870477262884378, Validation Loss: 0.0001477318420559944\n",
      "Epoch [11/25], Train Loss: 0.00017458965885452926, Validation Loss: 0.0001468423979531508\n",
      "Epoch [11/25], Train Loss: 0.0001814931893022731, Validation Loss: 0.0001452885412921508\n",
      "Epoch [11/25], Train Loss: 0.00014735746663063765, Validation Loss: 0.00014414958908067396\n",
      "Epoch [11/25], Train Loss: 0.00014586355246137828, Validation Loss: 0.0001441813966569801\n",
      "Epoch [11/25], Train Loss: 0.0001475021126680076, Validation Loss: 0.00014507353116641753\n",
      "Epoch [11/25], Train Loss: 0.00012262027303222567, Validation Loss: 0.0001459448581348018\n",
      "Epoch [11/25], Train Loss: 0.00012112334661651403, Validation Loss: 0.00014612148855424795\n",
      "Epoch [11/25], Train Loss: 0.00018014614761341363, Validation Loss: 0.00014608079218305647\n",
      "Epoch [11/25], Train Loss: 0.00013308491907082498, Validation Loss: 0.00014519929100060837\n",
      "Epoch [11/25], Train Loss: 0.00015130391693674028, Validation Loss: 0.0001443390928519269\n",
      "Epoch [11/25], Train Loss: 0.00017062792903743684, Validation Loss: 0.00014398350734457684\n",
      "Epoch [11/25], Train Loss: 0.00015785628056619316, Validation Loss: 0.00014431704160718559\n",
      "Epoch [11/25], Train Loss: 0.0001458235055906698, Validation Loss: 0.00014492191548924892\n",
      "Epoch [11/25], Train Loss: 0.0001459103514207527, Validation Loss: 0.00014511394159247477\n",
      "Epoch [11/25], Train Loss: 0.00018337224901188165, Validation Loss: 0.00014492263726424427\n",
      "Epoch [11/25], Train Loss: 0.00020194111857563257, Validation Loss: 0.00014440382728935218\n",
      "Epoch [11/25], Train Loss: 0.00024278924684040248, Validation Loss: 0.00014407623408866735\n",
      "Epoch [11/25], Train Loss: 0.00017441845557186753, Validation Loss: 0.00014400378883389447\n",
      "Epoch [11/25], Train Loss: 0.00019105525279883295, Validation Loss: 0.00014414621715938363\n",
      "Epoch [11/25], Train Loss: 0.00010099368955707178, Validation Loss: 0.00014435151945993614\n",
      "Epoch [11/25], Train Loss: 0.00017761759227141738, Validation Loss: 0.00014455385656522897\n",
      "Epoch [11/25], Train Loss: 0.00016516080358996987, Validation Loss: 0.00014481464119550462\n",
      "Epoch [11/25], Train Loss: 0.0001740651932777837, Validation Loss: 0.00014490065683882373\n",
      "Epoch [11/25], Train Loss: 0.00012042622256558388, Validation Loss: 0.00014501052428386174\n",
      "Epoch [11/25], Train Loss: 0.00012225184764247388, Validation Loss: 0.0001450213288383869\n",
      "Epoch [11/25], Train Loss: 0.00020437325292732567, Validation Loss: 0.0001449516285598899\n",
      "Epoch [11/25], Train Loss: 0.00018053279200103134, Validation Loss: 0.00014478774319286457\n",
      "Epoch [11/25], Train Loss: 0.00018301737145520747, Validation Loss: 0.00014450367283037243\n",
      "Epoch [11/25], Train Loss: 0.00011426617857068777, Validation Loss: 0.00014421642626984976\n",
      "Epoch [11/25], Train Loss: 0.00016391593089792877, Validation Loss: 0.00014398433680374486\n",
      "Epoch [11/25], Train Loss: 0.0002239208115497604, Validation Loss: 0.00014390079959412106\n",
      "Epoch [11/25], Train Loss: 0.0001708543859422207, Validation Loss: 0.0001439342381975924\n",
      "Epoch [11/25], Train Loss: 9.749230230227113e-05, Validation Loss: 0.0001440143624980313\n",
      "Epoch [11/25], Train Loss: 0.00020522922568488866, Validation Loss: 0.00014409314292909887\n",
      "Epoch [11/25], Train Loss: 0.00014261493925005198, Validation Loss: 0.00014417592780470537\n",
      "Epoch [11/25], Train Loss: 0.00016892279381863773, Validation Loss: 0.00014426399939111435\n",
      "Epoch [11/25], Train Loss: 0.00017394319002050906, Validation Loss: 0.0001445265777874738\n",
      "Epoch [11/25], Train Loss: 0.0001592200278537348, Validation Loss: 0.00014493746154281933\n",
      "Epoch [11/25], Train Loss: 0.00020320022304076701, Validation Loss: 0.0001460235653212294\n",
      "Epoch [11/25], Train Loss: 0.0001368884404655546, Validation Loss: 0.00014752932426442082\n",
      "Epoch [11/25], Train Loss: 0.00017091679910663515, Validation Loss: 0.00014916064004258562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], Train Loss: 0.00014015907072462142, Validation Loss: 0.00014988343294438286\n",
      "Epoch [11/25], Train Loss: 0.0002415185736026615, Validation Loss: 0.00014847895290586165\n",
      "Epoch [11/25], Train Loss: 0.00017432386812288314, Validation Loss: 0.0001461533526404916\n",
      "Epoch [11/25], Train Loss: 0.00015606019587721676, Validation Loss: 0.0001443613315738427\n",
      "Epoch [11/25], Train Loss: 0.0001423422509105876, Validation Loss: 0.00014425121601865007\n",
      "Epoch [11/25], Train Loss: 0.00017568515613675117, Validation Loss: 0.0001455179175536614\n",
      "Epoch [11/25], Train Loss: 0.00014764838851988316, Validation Loss: 0.0001472420034891305\n",
      "Epoch [11/25], Train Loss: 0.00021491856023203582, Validation Loss: 0.00014843691266529883\n",
      "Epoch [11/25], Train Loss: 0.00012908471398986876, Validation Loss: 0.0001489694196303996\n",
      "Epoch [11/25], Train Loss: 9.476118430029601e-05, Validation Loss: 0.0001481816238083411\n",
      "Epoch [12/25], Train Loss: 9.948848310159519e-05, Validation Loss: 0.0001465406033579105\n",
      "Epoch [12/25], Train Loss: 0.00010843082418432459, Validation Loss: 0.00014478926823358051\n",
      "Epoch [12/25], Train Loss: 0.00014393489982467145, Validation Loss: 0.0001440835342994736\n",
      "Epoch [12/25], Train Loss: 0.00014641822781413794, Validation Loss: 0.00014479608507826923\n",
      "Epoch [12/25], Train Loss: 0.0001807285734685138, Validation Loss: 0.0001457869067962747\n",
      "Epoch [12/25], Train Loss: 0.00017742505588103086, Validation Loss: 0.00014607913083940124\n",
      "Epoch [12/25], Train Loss: 8.894461643649265e-05, Validation Loss: 0.00014518947233833993\n",
      "Epoch [12/25], Train Loss: 0.00017605486209504306, Validation Loss: 0.0001441829318840367\n",
      "Epoch [12/25], Train Loss: 0.00014569681661669165, Validation Loss: 0.00014419246363104322\n",
      "Epoch [12/25], Train Loss: 0.0001288313651457429, Validation Loss: 0.0001448784617726536\n",
      "Epoch [12/25], Train Loss: 0.00010052120342152193, Validation Loss: 0.00014506316098656196\n",
      "Epoch [12/25], Train Loss: 0.0001830594555940479, Validation Loss: 0.00014447549571438382\n",
      "Epoch [12/25], Train Loss: 0.0001648128527449444, Validation Loss: 0.00014405727852135898\n",
      "Epoch [12/25], Train Loss: 0.0001702574227238074, Validation Loss: 0.00014409621677865896\n",
      "Epoch [12/25], Train Loss: 0.0001949184515979141, Validation Loss: 0.0001443710871778118\n",
      "Epoch [12/25], Train Loss: 0.00014134508091956377, Validation Loss: 0.00014446515148544374\n",
      "Epoch [12/25], Train Loss: 0.00010249477054458112, Validation Loss: 0.00014449456405903524\n",
      "Epoch [12/25], Train Loss: 0.00011823867680504918, Validation Loss: 0.00014426395694802825\n",
      "Epoch [12/25], Train Loss: 0.00013463258801493794, Validation Loss: 0.0001440906790473188\n",
      "Epoch [12/25], Train Loss: 0.0001677867112448439, Validation Loss: 0.00014396676391091509\n",
      "Epoch [12/25], Train Loss: 0.00014313313295133412, Validation Loss: 0.00014393974852282553\n",
      "Epoch [12/25], Train Loss: 0.0001740217994665727, Validation Loss: 0.0001440006984921638\n",
      "Epoch [12/25], Train Loss: 0.000159344999701716, Validation Loss: 0.000144131851993734\n",
      "Epoch [12/25], Train Loss: 0.00013469273108057678, Validation Loss: 0.0001441573549527675\n",
      "Epoch [12/25], Train Loss: 0.00016920176858548075, Validation Loss: 0.0001440994892618619\n",
      "Epoch [12/25], Train Loss: 0.00011841909872600809, Validation Loss: 0.00014401196070442288\n",
      "Epoch [12/25], Train Loss: 0.00012728862930089235, Validation Loss: 0.00014396407301925745\n",
      "Epoch [12/25], Train Loss: 0.00013872490671928972, Validation Loss: 0.0001439359322830569\n",
      "Epoch [12/25], Train Loss: 0.00018996138533111662, Validation Loss: 0.0001439087742861981\n",
      "Epoch [12/25], Train Loss: 0.0001726499613141641, Validation Loss: 0.00014387529081432148\n",
      "Epoch [12/25], Train Loss: 0.0001603537966730073, Validation Loss: 0.00014390050710062497\n",
      "Epoch [12/25], Train Loss: 0.00020553336071316153, Validation Loss: 0.00014393555490338866\n",
      "Epoch [12/25], Train Loss: 0.00014713028213009238, Validation Loss: 0.00014390932216580646\n",
      "Epoch [12/25], Train Loss: 0.0001603471755515784, Validation Loss: 0.00014385818091492789\n",
      "Epoch [12/25], Train Loss: 0.00021471426589414477, Validation Loss: 0.00014387033152161167\n",
      "Epoch [12/25], Train Loss: 0.00017213374667335302, Validation Loss: 0.00014394267078993533\n",
      "Epoch [12/25], Train Loss: 0.00015470660582650453, Validation Loss: 0.00014405808833544141\n",
      "Epoch [12/25], Train Loss: 0.0001741913874866441, Validation Loss: 0.00014402296462018665\n",
      "Epoch [12/25], Train Loss: 0.0001401734334649518, Validation Loss: 0.00014398134881048462\n",
      "Epoch [12/25], Train Loss: 0.0001601272524567321, Validation Loss: 0.00014409058931050823\n",
      "Epoch [12/25], Train Loss: 0.00012462478480301797, Validation Loss: 0.00014431114032049664\n",
      "Epoch [12/25], Train Loss: 0.0002234827697975561, Validation Loss: 0.0001444837432548714\n",
      "Epoch [12/25], Train Loss: 0.00017793869483284652, Validation Loss: 0.00014479469561289686\n",
      "Epoch [12/25], Train Loss: 0.0001652305363677442, Validation Loss: 0.00014531533597619272\n",
      "Epoch [12/25], Train Loss: 0.0001348972728010267, Validation Loss: 0.00014603660359474208\n",
      "Epoch [12/25], Train Loss: 0.0001426196686225012, Validation Loss: 0.00014659736431591833\n",
      "Epoch [12/25], Train Loss: 0.00013765045150648803, Validation Loss: 0.00014709931759474177\n",
      "Epoch [12/25], Train Loss: 0.00014847306010778993, Validation Loss: 0.00014718995904938008\n",
      "Epoch [12/25], Train Loss: 0.0002089958288706839, Validation Loss: 0.000146897876887427\n",
      "Epoch [12/25], Train Loss: 0.0001509610447101295, Validation Loss: 0.0001459618387646818\n",
      "Epoch [12/25], Train Loss: 0.0001100581866921857, Validation Loss: 0.00014481999993828747\n",
      "Epoch [12/25], Train Loss: 8.630086085759103e-05, Validation Loss: 0.0001440114412010492\n",
      "Epoch [12/25], Train Loss: 0.00014559087867382914, Validation Loss: 0.00014401008787293297\n",
      "Epoch [12/25], Train Loss: 0.00016315910033881664, Validation Loss: 0.0001446042069195149\n",
      "Epoch [12/25], Train Loss: 0.00017539212421979755, Validation Loss: 0.00014502829047463212\n",
      "Epoch [12/25], Train Loss: 0.00016305044118780643, Validation Loss: 0.00014528534932954548\n",
      "Epoch [12/25], Train Loss: 0.00014355724852066487, Validation Loss: 0.00014526495894339557\n",
      "Epoch [12/25], Train Loss: 0.00016560379299335182, Validation Loss: 0.00014501651554989318\n",
      "Epoch [12/25], Train Loss: 0.00014936736261006445, Validation Loss: 0.00014450828408977637\n",
      "Epoch [12/25], Train Loss: 0.00015098688891157508, Validation Loss: 0.00014408496523780437\n",
      "Epoch [12/25], Train Loss: 0.00010306429612683132, Validation Loss: 0.0001438897365005687\n",
      "Epoch [12/25], Train Loss: 0.00018744761473499238, Validation Loss: 0.0001439124653794958\n",
      "Epoch [12/25], Train Loss: 0.00015486952906940132, Validation Loss: 0.00014407553535420448\n",
      "Epoch [12/25], Train Loss: 0.00019764539320021868, Validation Loss: 0.00014415138260422586\n",
      "Epoch [12/25], Train Loss: 0.00010708712216001004, Validation Loss: 0.00014405774078719938\n",
      "Epoch [12/25], Train Loss: 0.00013150741870049387, Validation Loss: 0.00014391944690335853\n",
      "Epoch [12/25], Train Loss: 0.0001744436303852126, Validation Loss: 0.00014385964192721682\n",
      "Epoch [12/25], Train Loss: 0.00016878373571671546, Validation Loss: 0.00014393180147938742\n",
      "Epoch [12/25], Train Loss: 0.00018084666226059198, Validation Loss: 0.00014409769598084193\n",
      "Epoch [12/25], Train Loss: 0.00017268990632146597, Validation Loss: 0.00014429115302239855\n",
      "Epoch [12/25], Train Loss: 0.00018924799223896116, Validation Loss: 0.00014434008262469432\n",
      "Epoch [12/25], Train Loss: 0.00013742422743234783, Validation Loss: 0.00014441029003743703\n",
      "Epoch [12/25], Train Loss: 0.00011782732326537371, Validation Loss: 0.00014443719167805586\n",
      "Epoch [12/25], Train Loss: 0.00014605744217988104, Validation Loss: 0.00014449901597496744\n",
      "Epoch [12/25], Train Loss: 0.00017741991905495524, Validation Loss: 0.0001444834017699274\n",
      "Epoch [12/25], Train Loss: 0.00018695584731176496, Validation Loss: 0.00014449549635173753\n",
      "Epoch [12/25], Train Loss: 0.00018520922458264977, Validation Loss: 0.00014455374039243907\n",
      "Epoch [12/25], Train Loss: 0.0001926600671140477, Validation Loss: 0.00014456700688848892\n",
      "Epoch [12/25], Train Loss: 0.00014744940563105047, Validation Loss: 0.00014459337641407424\n",
      "Epoch [12/25], Train Loss: 0.00011806790280388668, Validation Loss: 0.0001446282971301116\n",
      "Epoch [12/25], Train Loss: 0.00015850760973989964, Validation Loss: 0.0001445996296145798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], Train Loss: 8.611711382400244e-05, Validation Loss: 0.00014482817205134778\n",
      "Epoch [12/25], Train Loss: 0.0001332810497842729, Validation Loss: 0.0001450493293911374\n",
      "Epoch [12/25], Train Loss: 0.00015570342657156289, Validation Loss: 0.00014545664671459236\n",
      "Epoch [12/25], Train Loss: 0.00017866506823338568, Validation Loss: 0.00014587716408035096\n",
      "Epoch [12/25], Train Loss: 0.000183042575372383, Validation Loss: 0.00014657798334762143\n",
      "Epoch [12/25], Train Loss: 0.00015614500443916768, Validation Loss: 0.0001468083132446433\n",
      "Epoch [12/25], Train Loss: 0.00012532839900813997, Validation Loss: 0.00014644131782309461\n",
      "Epoch [12/25], Train Loss: 0.00017366069369018078, Validation Loss: 0.00014553407017956488\n",
      "Epoch [12/25], Train Loss: 0.00013696265523321927, Validation Loss: 0.00014462338246327514\n",
      "Epoch [12/25], Train Loss: 0.0001260972348973155, Validation Loss: 0.0001439944746380206\n",
      "Epoch [12/25], Train Loss: 0.00017800847126636654, Validation Loss: 0.00014387456564387928\n",
      "Epoch [12/25], Train Loss: 0.00018840236589312553, Validation Loss: 0.0001442287449511544\n",
      "Epoch [12/25], Train Loss: 0.00019891340343747288, Validation Loss: 0.00014470003443420865\n",
      "Epoch [12/25], Train Loss: 0.0001593722408870235, Validation Loss: 0.00014506494602149663\n",
      "Epoch [12/25], Train Loss: 0.0001798353623598814, Validation Loss: 0.00014527036764775403\n",
      "Epoch [12/25], Train Loss: 0.00019304826855659485, Validation Loss: 0.0001457417454124273\n",
      "Epoch [12/25], Train Loss: 9.29648449528031e-05, Validation Loss: 0.00014602989904233254\n",
      "Epoch [12/25], Train Loss: 0.00013994878099765629, Validation Loss: 0.0001459482094408789\n",
      "Epoch [12/25], Train Loss: 0.00021854873921256512, Validation Loss: 0.0001454613634753817\n",
      "Epoch [12/25], Train Loss: 0.00014994069351814687, Validation Loss: 0.00014493776713303912\n",
      "Epoch [12/25], Train Loss: 0.00019765476463362575, Validation Loss: 0.00014441346744812714\n",
      "Epoch [12/25], Train Loss: 0.0001282308658119291, Validation Loss: 0.00014398617737848934\n",
      "Epoch [12/25], Train Loss: 8.681799954501912e-05, Validation Loss: 0.00014389943947511105\n",
      "Epoch [12/25], Train Loss: 0.00017227945500053465, Validation Loss: 0.00014399788948746088\n",
      "Epoch [12/25], Train Loss: 0.00014200156147126108, Validation Loss: 0.00014425059780478477\n",
      "Epoch [12/25], Train Loss: 0.00014734947762917727, Validation Loss: 0.000144539690760818\n",
      "Epoch [12/25], Train Loss: 0.00011287233064649627, Validation Loss: 0.00014472221228061243\n",
      "Epoch [12/25], Train Loss: 0.00014255357382353395, Validation Loss: 0.00014463496287741388\n",
      "Epoch [12/25], Train Loss: 0.00014004341210238636, Validation Loss: 0.00014433604931885687\n",
      "Epoch [12/25], Train Loss: 0.00018827409076038748, Validation Loss: 0.00014401412045117468\n",
      "Epoch [12/25], Train Loss: 0.00012211038847453892, Validation Loss: 0.00014389184749840448\n",
      "Epoch [12/25], Train Loss: 0.00014299611211754382, Validation Loss: 0.00014389959251275285\n",
      "Epoch [12/25], Train Loss: 0.00021959647710900754, Validation Loss: 0.0001438829368756463\n",
      "Epoch [12/25], Train Loss: 0.00016535050235688686, Validation Loss: 0.000143928271184753\n",
      "Epoch [12/25], Train Loss: 0.00017326012311968952, Validation Loss: 0.00014408107817871497\n",
      "Epoch [12/25], Train Loss: 0.00012761354446411133, Validation Loss: 0.0001441389977117069\n",
      "Epoch [12/25], Train Loss: 0.0001485240791225806, Validation Loss: 0.0001440882078895811\n",
      "Epoch [12/25], Train Loss: 0.00014053296763449907, Validation Loss: 0.0001440886759761876\n",
      "Epoch [12/25], Train Loss: 0.0001027024700306356, Validation Loss: 0.0001441757386298074\n",
      "Epoch [12/25], Train Loss: 0.00016933432198129594, Validation Loss: 0.00014435616928191544\n",
      "Epoch [12/25], Train Loss: 0.000127086226711981, Validation Loss: 0.00014437566424021497\n",
      "Epoch [12/25], Train Loss: 0.00011590924259508029, Validation Loss: 0.0001443500048480928\n",
      "Epoch [12/25], Train Loss: 0.00014115583326201886, Validation Loss: 0.0001443397801873895\n",
      "Epoch [12/25], Train Loss: 0.00019632099429145455, Validation Loss: 0.00014428182524473717\n",
      "Epoch [12/25], Train Loss: 0.00023516808869317174, Validation Loss: 0.00014431853026811343\n",
      "Epoch [12/25], Train Loss: 0.00016870071704033762, Validation Loss: 0.00014435428844687218\n",
      "Epoch [12/25], Train Loss: 0.00017716625006869435, Validation Loss: 0.0001444076086045243\n",
      "Epoch [12/25], Train Loss: 0.00014422700041905046, Validation Loss: 0.00014452627583523282\n",
      "Epoch [12/25], Train Loss: 0.00015006159082986414, Validation Loss: 0.0001446097727845578\n",
      "Epoch [12/25], Train Loss: 0.0001661393471295014, Validation Loss: 0.00014479998668927389\n",
      "Epoch [12/25], Train Loss: 0.0001412712736055255, Validation Loss: 0.00014509366398366788\n",
      "Epoch [12/25], Train Loss: 0.00013473765284288675, Validation Loss: 0.00014556517562596126\n",
      "Epoch [12/25], Train Loss: 0.00014958280371502042, Validation Loss: 0.00014616640376819608\n",
      "Epoch [12/25], Train Loss: 0.0001527703134343028, Validation Loss: 0.00014692863066253874\n",
      "Epoch [12/25], Train Loss: 0.00018630294653121382, Validation Loss: 0.00014771911470840376\n",
      "Epoch [12/25], Train Loss: 0.00018040745635516942, Validation Loss: 0.0001485883915544643\n",
      "Epoch [12/25], Train Loss: 0.00018609479593578726, Validation Loss: 0.00014892901017447003\n",
      "Epoch [12/25], Train Loss: 0.00014844807446934283, Validation Loss: 0.00014847667310580921\n",
      "Epoch [12/25], Train Loss: 0.00011597196134971455, Validation Loss: 0.00014665428849790866\n",
      "Epoch [12/25], Train Loss: 0.0001593279594089836, Validation Loss: 0.0001446550787174298\n",
      "Epoch [12/25], Train Loss: 0.0002006015129154548, Validation Loss: 0.00014402833233665053\n",
      "Epoch [12/25], Train Loss: 0.00017708184896036983, Validation Loss: 0.0001448020137710652\n",
      "Epoch [12/25], Train Loss: 9.365576988784596e-05, Validation Loss: 0.00014565299352398142\n",
      "Epoch [12/25], Train Loss: 0.00012630097626242787, Validation Loss: 0.00014604419799676787\n",
      "Epoch [12/25], Train Loss: 0.00015097066352609545, Validation Loss: 0.0001462403704257061\n",
      "Epoch [12/25], Train Loss: 0.00011918017116840929, Validation Loss: 0.00014557240040934024\n",
      "Epoch [12/25], Train Loss: 0.00015548535156995058, Validation Loss: 0.00014492626432911494\n",
      "Epoch [12/25], Train Loss: 0.00013914720329921693, Validation Loss: 0.00014426504446115966\n",
      "Epoch [12/25], Train Loss: 0.00017566570022609085, Validation Loss: 0.00014395390365583202\n",
      "Epoch [12/25], Train Loss: 0.00018964431365020573, Validation Loss: 0.00014406715393609678\n",
      "Epoch [12/25], Train Loss: 0.00015756531502120197, Validation Loss: 0.0001443757641633662\n",
      "Epoch [12/25], Train Loss: 0.00018438755068928003, Validation Loss: 0.00014464471508593608\n",
      "Epoch [12/25], Train Loss: 0.00014700462634209543, Validation Loss: 0.00014455917965581951\n",
      "Epoch [12/25], Train Loss: 0.00015459436690434813, Validation Loss: 0.00014427915642348428\n",
      "Epoch [12/25], Train Loss: 0.00013916767784394324, Validation Loss: 0.000143982832863306\n",
      "Epoch [12/25], Train Loss: 0.00021101096353959292, Validation Loss: 0.000143859196153547\n",
      "Epoch [12/25], Train Loss: 0.00014314176223706454, Validation Loss: 0.00014381445240966664\n",
      "Epoch [12/25], Train Loss: 0.00015782323316670954, Validation Loss: 0.00014380342812122157\n",
      "Epoch [12/25], Train Loss: 0.00016779151337686926, Validation Loss: 0.0001438735945460697\n",
      "Epoch [12/25], Train Loss: 0.00014332130376715213, Validation Loss: 0.00014397530127704765\n",
      "Epoch [12/25], Train Loss: 0.00022783473832532763, Validation Loss: 0.00014410807416425085\n",
      "Epoch [12/25], Train Loss: 0.00015902664745226502, Validation Loss: 0.0001442153283278458\n",
      "Epoch [12/25], Train Loss: 0.00017398885393049568, Validation Loss: 0.00014432876099211475\n",
      "Epoch [12/25], Train Loss: 0.00016148440772667527, Validation Loss: 0.00014443538384512067\n",
      "Epoch [12/25], Train Loss: 0.0001566926803207025, Validation Loss: 0.00014459821225803655\n",
      "Epoch [12/25], Train Loss: 0.0001675236562732607, Validation Loss: 0.00014484369360919422\n",
      "Epoch [12/25], Train Loss: 0.00013478455366566777, Validation Loss: 0.0001452577957631244\n",
      "Epoch [12/25], Train Loss: 0.0001693596423137933, Validation Loss: 0.0001459318800092054\n",
      "Epoch [12/25], Train Loss: 0.00018656814063433558, Validation Loss: 0.00014696559082949534\n",
      "Epoch [12/25], Train Loss: 0.0001690848293947056, Validation Loss: 0.00014809968803698818\n",
      "Epoch [12/25], Train Loss: 0.00016722686996217817, Validation Loss: 0.00014976329572770434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], Train Loss: 0.00011167586490046233, Validation Loss: 0.0001502915130307277\n",
      "Epoch [12/25], Train Loss: 0.00014637343701906502, Validation Loss: 0.00014921363617759198\n",
      "Epoch [12/25], Train Loss: 0.00016354367835447192, Validation Loss: 0.00014640139270341023\n",
      "Epoch [12/25], Train Loss: 0.00014722220657858998, Validation Loss: 0.00014408476951454456\n",
      "Epoch [12/25], Train Loss: 0.0001532918104203418, Validation Loss: 0.00014456438802881166\n",
      "Epoch [12/25], Train Loss: 0.0001462689833715558, Validation Loss: 0.00014660008067342764\n",
      "Epoch [12/25], Train Loss: 0.0001640388072701171, Validation Loss: 0.00014829853122743468\n",
      "Epoch [12/25], Train Loss: 0.00018611854466143996, Validation Loss: 0.00014758735487703235\n",
      "Epoch [12/25], Train Loss: 0.0001502906234236434, Validation Loss: 0.00014624461061127174\n",
      "Epoch [12/25], Train Loss: 0.00010735844261944294, Validation Loss: 0.00014453586530483638\n",
      "Epoch [12/25], Train Loss: 0.00011516040831338614, Validation Loss: 0.0001439583604224026\n",
      "Epoch [12/25], Train Loss: 0.00014794991875533015, Validation Loss: 0.000145011840989658\n",
      "Epoch [12/25], Train Loss: 0.0001549574371892959, Validation Loss: 0.00014589991381702322\n",
      "Epoch [12/25], Train Loss: 9.509906521998346e-05, Validation Loss: 0.0001452565139819247\n",
      "Epoch [12/25], Train Loss: 0.00013790797675028443, Validation Loss: 0.00014413810858968646\n",
      "Epoch [12/25], Train Loss: 0.00019092776346951723, Validation Loss: 0.0001439483651968961\n",
      "Epoch [12/25], Train Loss: 0.00013012812996748835, Validation Loss: 0.000144337863457622\n",
      "Epoch [12/25], Train Loss: 0.00018589891260489821, Validation Loss: 0.00014443859521027965\n",
      "Epoch [12/25], Train Loss: 0.0001782671024557203, Validation Loss: 0.00014425243231623122\n",
      "Epoch [12/25], Train Loss: 0.00010596115316729993, Validation Loss: 0.00014397501460431764\n",
      "Epoch [12/25], Train Loss: 0.00016314734239131212, Validation Loss: 0.00014381187647813932\n",
      "Epoch [12/25], Train Loss: 0.0001845314836828038, Validation Loss: 0.00014383941330985787\n",
      "Epoch [12/25], Train Loss: 0.00017214310355484486, Validation Loss: 0.00014394824756891467\n",
      "Epoch [12/25], Train Loss: 0.00020029830920975655, Validation Loss: 0.00014398292793581883\n",
      "Epoch [12/25], Train Loss: 0.0001479726779507473, Validation Loss: 0.0001439854888303671\n",
      "Epoch [12/25], Train Loss: 0.00016674869402777404, Validation Loss: 0.00014393520395969973\n",
      "Epoch [12/25], Train Loss: 0.00013096557813696563, Validation Loss: 0.00014384890649428902\n",
      "Epoch [12/25], Train Loss: 0.00016974536993075162, Validation Loss: 0.00014383117813849823\n",
      "Epoch [12/25], Train Loss: 0.0001337422290816903, Validation Loss: 0.00014385878360675028\n",
      "Epoch [12/25], Train Loss: 0.00017613137606531382, Validation Loss: 0.00014387544943019747\n",
      "Epoch [12/25], Train Loss: 0.00012045118637615815, Validation Loss: 0.0001438239065464586\n",
      "Epoch [12/25], Train Loss: 0.00020762092026416212, Validation Loss: 0.0001437992752471473\n",
      "Epoch [12/25], Train Loss: 0.00010474100417923182, Validation Loss: 0.00014381122625006053\n",
      "Epoch [12/25], Train Loss: 0.00019156521011609584, Validation Loss: 0.00014380912374084193\n",
      "Epoch [12/25], Train Loss: 0.00015615610755048692, Validation Loss: 0.0001438216379028745\n",
      "Epoch [12/25], Train Loss: 0.00016291849897243083, Validation Loss: 0.0001438955674530007\n",
      "Epoch [12/25], Train Loss: 0.0001623926218599081, Validation Loss: 0.0001440389950100022\n",
      "Epoch [12/25], Train Loss: 0.00015108802472241223, Validation Loss: 0.0001443122935597785\n",
      "Epoch [12/25], Train Loss: 0.00013422018673736602, Validation Loss: 0.00014464037303696387\n",
      "Epoch [12/25], Train Loss: 0.0002351621224079281, Validation Loss: 0.00014554924176385004\n",
      "Epoch [12/25], Train Loss: 0.00015044532483443618, Validation Loss: 0.0001470288972389729\n",
      "Epoch [12/25], Train Loss: 0.00017719727475196123, Validation Loss: 0.00014874518359041152\n",
      "Epoch [12/25], Train Loss: 0.00019863847410306334, Validation Loss: 0.00015130424993306708\n",
      "Epoch [12/25], Train Loss: 0.00012199385673739016, Validation Loss: 0.00015269344536742815\n",
      "Epoch [12/25], Train Loss: 0.0001867681130534038, Validation Loss: 0.00015089725978517283\n",
      "Epoch [12/25], Train Loss: 0.00016478568431921303, Validation Loss: 0.00014649794926905693\n",
      "Epoch [12/25], Train Loss: 0.00012959375453647226, Validation Loss: 0.00014457646054021704\n",
      "Epoch [12/25], Train Loss: 0.00017818465130403638, Validation Loss: 0.0001461933206883259\n",
      "Epoch [12/25], Train Loss: 0.00014295535220298916, Validation Loss: 0.00014755610051603679\n",
      "Epoch [12/25], Train Loss: 0.00011270071263425052, Validation Loss: 0.0001465499835224667\n",
      "Epoch [12/25], Train Loss: 0.00015269253344740719, Validation Loss: 0.00014457995227227607\n",
      "Epoch [12/25], Train Loss: 0.00013307244807947427, Validation Loss: 0.00014396616024896501\n",
      "Epoch [12/25], Train Loss: 0.00019380886806175113, Validation Loss: 0.00014552172433468513\n",
      "Epoch [12/25], Train Loss: 0.00018200439808424562, Validation Loss: 0.0001475717501307372\n",
      "Epoch [12/25], Train Loss: 0.00016312477237079293, Validation Loss: 0.00015018525470319826\n",
      "Epoch [12/25], Train Loss: 0.00011249854287598282, Validation Loss: 0.00014927387358814787\n",
      "Epoch [12/25], Train Loss: 0.00016997520287986845, Validation Loss: 0.00014713104198259923\n",
      "Epoch [12/25], Train Loss: 0.00011768026888603345, Validation Loss: 0.00014441152792035912\n",
      "Epoch [12/25], Train Loss: 0.00010434327850816771, Validation Loss: 0.0001449156860568716\n",
      "Epoch [12/25], Train Loss: 0.00020697542640846223, Validation Loss: 0.00014710205990316658\n",
      "Epoch [12/25], Train Loss: 0.0001124328191508539, Validation Loss: 0.00014649962104158477\n",
      "Epoch [12/25], Train Loss: 0.00015294761396944523, Validation Loss: 0.0001445223344489932\n",
      "Epoch [12/25], Train Loss: 0.00020144873997196555, Validation Loss: 0.0001444164226995781\n",
      "Epoch [12/25], Train Loss: 0.00014668970834463835, Validation Loss: 0.00014580294421951597\n",
      "Epoch [12/25], Train Loss: 0.0001646448072278872, Validation Loss: 0.0001459299669174167\n",
      "Epoch [12/25], Train Loss: 0.0001727011549519375, Validation Loss: 0.00014461079093355995\n",
      "Epoch [12/25], Train Loss: 0.00016831164248287678, Validation Loss: 0.00014391005897778085\n",
      "Epoch [12/25], Train Loss: 0.00017769770056474954, Validation Loss: 0.00014449952553453234\n",
      "Epoch [12/25], Train Loss: 0.0001563533878652379, Validation Loss: 0.00014549554398399778\n",
      "Epoch [12/25], Train Loss: 0.00014001174713484943, Validation Loss: 0.0001461412667898306\n",
      "Epoch [12/25], Train Loss: 0.00015728917787782848, Validation Loss: 0.00014599051864934155\n",
      "Epoch [12/25], Train Loss: 0.00015571300173178315, Validation Loss: 0.0001454247246632197\n",
      "Epoch [12/25], Train Loss: 0.00021392789494711906, Validation Loss: 0.0001446362604231884\n",
      "Epoch [12/25], Train Loss: 0.00014571507927030325, Validation Loss: 0.00014411327356356196\n",
      "Epoch [12/25], Train Loss: 9.136176959145814e-05, Validation Loss: 0.0001438611934039121\n",
      "Epoch [12/25], Train Loss: 0.0001286356564378366, Validation Loss: 0.0001438299931275348\n",
      "Epoch [12/25], Train Loss: 0.00010707091860240325, Validation Loss: 0.00014396989257268918\n",
      "Epoch [12/25], Train Loss: 0.0001714527461444959, Validation Loss: 0.00014408099135228744\n",
      "Epoch [12/25], Train Loss: 0.00014707172522321343, Validation Loss: 0.00014412505479413084\n",
      "Epoch [12/25], Train Loss: 0.00018843299767468125, Validation Loss: 0.00014412017068631637\n",
      "Epoch [12/25], Train Loss: 0.000144061486935243, Validation Loss: 0.0001440079227904789\n",
      "Epoch [12/25], Train Loss: 0.00014109525363892317, Validation Loss: 0.0001438548308215104\n",
      "Epoch [12/25], Train Loss: 0.00020156244863756, Validation Loss: 0.00014375790973038724\n",
      "Epoch [12/25], Train Loss: 0.00016147454152815044, Validation Loss: 0.00014375335861889957\n",
      "Epoch [12/25], Train Loss: 0.00013768908684141934, Validation Loss: 0.00014380745536376102\n",
      "Epoch [12/25], Train Loss: 8.043779234867543e-05, Validation Loss: 0.00014390500655281357\n",
      "Epoch [12/25], Train Loss: 0.00011727569653885439, Validation Loss: 0.0001439860265236348\n",
      "Epoch [12/25], Train Loss: 0.0001936659391503781, Validation Loss: 0.00014400885629584081\n",
      "Epoch [12/25], Train Loss: 0.0001560004602652043, Validation Loss: 0.00014396883173806903\n",
      "Epoch [12/25], Train Loss: 0.0001279163989238441, Validation Loss: 0.0001438920941533676\n",
      "Epoch [12/25], Train Loss: 0.00011037951480830088, Validation Loss: 0.00014383435482159256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], Train Loss: 0.00010681222920538858, Validation Loss: 0.00014381288541092847\n",
      "Epoch [12/25], Train Loss: 0.00013208850577939302, Validation Loss: 0.00014377523960623268\n",
      "Epoch [12/25], Train Loss: 0.00018970703240484, Validation Loss: 0.00014381218376608256\n",
      "Epoch [12/25], Train Loss: 0.0001330832310486585, Validation Loss: 0.000143897109713483\n",
      "Epoch [12/25], Train Loss: 0.00020198897982481867, Validation Loss: 0.00014384372843778692\n",
      "Epoch [13/25], Train Loss: 0.00012476147094275802, Validation Loss: 0.00014397015911526978\n",
      "Epoch [13/25], Train Loss: 0.00017016839410644025, Validation Loss: 0.00014402603386164022\n",
      "Epoch [13/25], Train Loss: 0.00015562918270006776, Validation Loss: 0.00014401450801718358\n",
      "Epoch [13/25], Train Loss: 0.00019855728896800429, Validation Loss: 0.00014390972331360292\n",
      "Epoch [13/25], Train Loss: 0.00015128517406992614, Validation Loss: 0.00014383155236525152\n",
      "Epoch [13/25], Train Loss: 0.00012491480447351933, Validation Loss: 0.00014385824761120602\n",
      "Epoch [13/25], Train Loss: 0.0001691094075795263, Validation Loss: 0.0001438425625868452\n",
      "Epoch [13/25], Train Loss: 0.00010540205403231084, Validation Loss: 0.00014381639048224315\n",
      "Epoch [13/25], Train Loss: 0.00018211978022009134, Validation Loss: 0.00014379457885903927\n",
      "Epoch [13/25], Train Loss: 9.895586117636412e-05, Validation Loss: 0.0001438284285541158\n",
      "Epoch [13/25], Train Loss: 0.00013394647976383567, Validation Loss: 0.00014389002535608597\n",
      "Epoch [13/25], Train Loss: 0.0001473198353778571, Validation Loss: 0.00014393426633129517\n",
      "Epoch [13/25], Train Loss: 0.000194468506379053, Validation Loss: 0.00014384161792501497\n",
      "Epoch [13/25], Train Loss: 0.000187697762157768, Validation Loss: 0.0001437800468314284\n",
      "Epoch [13/25], Train Loss: 0.00023236464767251164, Validation Loss: 0.00014377377689622032\n",
      "Epoch [13/25], Train Loss: 0.00017372537695337087, Validation Loss: 0.00014371808235106678\n",
      "Epoch [13/25], Train Loss: 0.00011800867650890723, Validation Loss: 0.00014375595274032093\n",
      "Epoch [13/25], Train Loss: 0.0001289237552555278, Validation Loss: 0.00014385863129670422\n",
      "Epoch [13/25], Train Loss: 0.00014831138832960278, Validation Loss: 0.00014390162784062946\n",
      "Epoch [13/25], Train Loss: 0.00012352332123555243, Validation Loss: 0.0001439426171903809\n",
      "Epoch [13/25], Train Loss: 0.00014350310084410012, Validation Loss: 0.00014400343934539706\n",
      "Epoch [13/25], Train Loss: 0.00017760437913239002, Validation Loss: 0.0001441388138725112\n",
      "Epoch [13/25], Train Loss: 0.0001561881072120741, Validation Loss: 0.00014427253651471498\n",
      "Epoch [13/25], Train Loss: 0.00012412216165103018, Validation Loss: 0.00014442383204974854\n",
      "Epoch [13/25], Train Loss: 0.00010897486936300993, Validation Loss: 0.00014463103919600448\n",
      "Epoch [13/25], Train Loss: 0.00019059116311836988, Validation Loss: 0.00014510465601536756\n",
      "Epoch [13/25], Train Loss: 0.00019365079060662538, Validation Loss: 0.00014569013874279335\n",
      "Epoch [13/25], Train Loss: 0.00016040528134908527, Validation Loss: 0.00014641779537972373\n",
      "Epoch [13/25], Train Loss: 0.00020529580069705844, Validation Loss: 0.00014715597426402384\n",
      "Epoch [13/25], Train Loss: 0.00017461144307162613, Validation Loss: 0.00014790152830149357\n",
      "Epoch [13/25], Train Loss: 0.00015873376105446368, Validation Loss: 0.00014797522089793347\n",
      "Epoch [13/25], Train Loss: 0.00015384463767986745, Validation Loss: 0.00014783101651119068\n",
      "Epoch [13/25], Train Loss: 0.00016909399710129946, Validation Loss: 0.00014641241505159998\n",
      "Epoch [13/25], Train Loss: 0.000165194520377554, Validation Loss: 0.00014491372979440106\n",
      "Epoch [13/25], Train Loss: 0.00017483804549556226, Validation Loss: 0.0001439110569966336\n",
      "Epoch [13/25], Train Loss: 0.00011943412391701713, Validation Loss: 0.000144000987590213\n",
      "Epoch [13/25], Train Loss: 0.0001649969635764137, Validation Loss: 0.00014475215672670553\n",
      "Epoch [13/25], Train Loss: 0.00015514972619712353, Validation Loss: 0.00014516169515748817\n",
      "Epoch [13/25], Train Loss: 0.0001844236539909616, Validation Loss: 0.0001449914932891261\n",
      "Epoch [13/25], Train Loss: 0.00013684628356713802, Validation Loss: 0.00014434580710561326\n",
      "Epoch [13/25], Train Loss: 0.00014085248403716832, Validation Loss: 0.00014390517341477486\n",
      "Epoch [13/25], Train Loss: 0.00010474544251337647, Validation Loss: 0.00014379143782813724\n",
      "Epoch [13/25], Train Loss: 0.00015533171244896948, Validation Loss: 0.00014395125763257965\n",
      "Epoch [13/25], Train Loss: 0.000131735869217664, Validation Loss: 0.00014420068036997692\n",
      "Epoch [13/25], Train Loss: 0.00012042093294439837, Validation Loss: 0.00014442972242250106\n",
      "Epoch [13/25], Train Loss: 0.0001456465251976624, Validation Loss: 0.00014437318823183886\n",
      "Epoch [13/25], Train Loss: 0.00012831241474486887, Validation Loss: 0.00014414957695407793\n",
      "Epoch [13/25], Train Loss: 0.00016487854009028524, Validation Loss: 0.0001440587312875626\n",
      "Epoch [13/25], Train Loss: 0.00016758538549765944, Validation Loss: 0.00014393301244126634\n",
      "Epoch [13/25], Train Loss: 0.00015176851593423635, Validation Loss: 0.00014382734298123978\n",
      "Epoch [13/25], Train Loss: 0.00016200606478378177, Validation Loss: 0.00014375969209747078\n",
      "Epoch [13/25], Train Loss: 0.00014795588504057378, Validation Loss: 0.00014374059610418045\n",
      "Epoch [13/25], Train Loss: 0.0001331938401563093, Validation Loss: 0.0001437942073001371\n",
      "Epoch [13/25], Train Loss: 0.00016312203661073, Validation Loss: 0.00014384175495555003\n",
      "Epoch [13/25], Train Loss: 0.00018965480558108538, Validation Loss: 0.00014387732686979386\n",
      "Epoch [13/25], Train Loss: 0.00017248645599465817, Validation Loss: 0.00014387490882654673\n",
      "Epoch [13/25], Train Loss: 0.0001896327594295144, Validation Loss: 0.00014383548914338463\n",
      "Epoch [13/25], Train Loss: 0.00011007292778231204, Validation Loss: 0.0001438144693869011\n",
      "Epoch [13/25], Train Loss: 0.00014075364742893726, Validation Loss: 0.00014385852409759536\n",
      "Epoch [13/25], Train Loss: 0.00012868733028881252, Validation Loss: 0.0001439177843470437\n",
      "Epoch [13/25], Train Loss: 0.0001272539811907336, Validation Loss: 0.00014397793153572518\n",
      "Epoch [13/25], Train Loss: 0.000123816222185269, Validation Loss: 0.00014409626746783032\n",
      "Epoch [13/25], Train Loss: 0.0001651121856411919, Validation Loss: 0.00014415141340577974\n",
      "Epoch [13/25], Train Loss: 0.00021171150729060173, Validation Loss: 0.00014432232928811571\n",
      "Epoch [13/25], Train Loss: 0.0002015546924667433, Validation Loss: 0.0001446326969016809\n",
      "Epoch [13/25], Train Loss: 0.00018572442058939487, Validation Loss: 0.00014500795732601545\n",
      "Epoch [13/25], Train Loss: 0.0001347200304735452, Validation Loss: 0.00014583009469788523\n",
      "Epoch [13/25], Train Loss: 0.00014159819693304598, Validation Loss: 0.0001476978958332135\n",
      "Epoch [13/25], Train Loss: 0.00016033971041906625, Validation Loss: 0.00015119086213720342\n",
      "Epoch [13/25], Train Loss: 0.00018139601161237806, Validation Loss: 0.00015477123937065093\n",
      "Epoch [13/25], Train Loss: 0.000122235287562944, Validation Loss: 0.0001582329350640066\n",
      "Epoch [13/25], Train Loss: 0.00013183642295189202, Validation Loss: 0.0001546346507287429\n",
      "Epoch [13/25], Train Loss: 0.00015725292905699462, Validation Loss: 0.000146704619207109\n",
      "Epoch [13/25], Train Loss: 0.00020561058772727847, Validation Loss: 0.00014520098072049828\n",
      "Epoch [13/25], Train Loss: 0.00012321749818511307, Validation Loss: 0.00014930777130454467\n",
      "Epoch [13/25], Train Loss: 0.00015982749755494297, Validation Loss: 0.0001483434333446591\n",
      "Epoch [13/25], Train Loss: 0.0001855587470345199, Validation Loss: 0.00014457730067078954\n",
      "Epoch [13/25], Train Loss: 0.0001375072606606409, Validation Loss: 0.0001454465539912538\n",
      "Epoch [13/25], Train Loss: 0.0001617482048459351, Validation Loss: 0.00014856909062170112\n",
      "Epoch [13/25], Train Loss: 0.000132809451315552, Validation Loss: 0.0001479157799622044\n",
      "Epoch [13/25], Train Loss: 0.00011888590961461887, Validation Loss: 0.0001453085843725906\n",
      "Epoch [13/25], Train Loss: 0.00015019593411125243, Validation Loss: 0.00014434111944865434\n",
      "Epoch [13/25], Train Loss: 0.00017230519733857363, Validation Loss: 0.0001455384665556873\n",
      "Epoch [13/25], Train Loss: 0.0001731077500153333, Validation Loss: 0.00014649847410813283\n",
      "Epoch [13/25], Train Loss: 0.00018714317411649972, Validation Loss: 0.00014504562326086065\n",
      "Epoch [13/25], Train Loss: 0.00012018760025966913, Validation Loss: 0.0001441140996272831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Train Loss: 0.00011162937880726531, Validation Loss: 0.00014473607370746322\n",
      "Epoch [13/25], Train Loss: 9.930595842888579e-05, Validation Loss: 0.00014560436029569245\n",
      "Epoch [13/25], Train Loss: 0.0001420269109075889, Validation Loss: 0.000145283200739262\n",
      "Epoch [13/25], Train Loss: 0.0001354575651930645, Validation Loss: 0.000144239779911004\n",
      "Epoch [13/25], Train Loss: 0.00013409662642516196, Validation Loss: 0.00014414374527405016\n",
      "Epoch [13/25], Train Loss: 0.00013047376705799252, Validation Loss: 0.00014481770607138362\n",
      "Epoch [13/25], Train Loss: 0.00015859559061937034, Validation Loss: 0.00014490575461726014\n",
      "Epoch [13/25], Train Loss: 0.0001743345201248303, Validation Loss: 0.0001445174547067533\n",
      "Epoch [13/25], Train Loss: 0.00015043762687128037, Validation Loss: 0.00014402889282791876\n",
      "Epoch [13/25], Train Loss: 0.00013565389963332564, Validation Loss: 0.0001439905827282928\n",
      "Epoch [13/25], Train Loss: 0.0001469455164624378, Validation Loss: 0.00014431149805507932\n",
      "Epoch [13/25], Train Loss: 0.00012689625145867467, Validation Loss: 0.00014462435647146776\n",
      "Epoch [13/25], Train Loss: 0.0002145086182281375, Validation Loss: 0.0001449310038879048\n",
      "Epoch [13/25], Train Loss: 0.00015072982932906598, Validation Loss: 0.00014493914204649627\n",
      "Epoch [13/25], Train Loss: 9.627557301428169e-05, Validation Loss: 0.00014462022857818132\n",
      "Epoch [13/25], Train Loss: 0.00017273533740080893, Validation Loss: 0.00014416377889574504\n",
      "Epoch [13/25], Train Loss: 0.0001631695486139506, Validation Loss: 0.00014389600789096828\n",
      "Epoch [13/25], Train Loss: 0.00017143187869805843, Validation Loss: 0.00014381191916375732\n",
      "Epoch [13/25], Train Loss: 0.00012658127525355667, Validation Loss: 0.00014380287805882593\n",
      "Epoch [13/25], Train Loss: 0.00014222509344108403, Validation Loss: 0.0001439895107372043\n",
      "Epoch [13/25], Train Loss: 0.00012427964247763157, Validation Loss: 0.0001441966950854597\n",
      "Epoch [13/25], Train Loss: 0.0001349638623651117, Validation Loss: 0.0001442226792278234\n",
      "Epoch [13/25], Train Loss: 0.00023309185053221881, Validation Loss: 0.00014423625737739105\n",
      "Epoch [13/25], Train Loss: 0.00015352870104834437, Validation Loss: 0.00014428398790187203\n",
      "Epoch [13/25], Train Loss: 0.00020465477427933365, Validation Loss: 0.0001443478843915121\n",
      "Epoch [13/25], Train Loss: 0.0001754685799824074, Validation Loss: 0.00014430449809879064\n",
      "Epoch [13/25], Train Loss: 0.00016507323016412556, Validation Loss: 0.00014415797413676047\n",
      "Epoch [13/25], Train Loss: 0.00012109254748793319, Validation Loss: 0.0001439535078437378\n",
      "Epoch [13/25], Train Loss: 0.00018494845426175743, Validation Loss: 0.00014379245913005435\n",
      "Epoch [13/25], Train Loss: 0.00011167663615196943, Validation Loss: 0.00014375937656344224\n",
      "Epoch [13/25], Train Loss: 0.00012962415348738432, Validation Loss: 0.00014375478858710266\n",
      "Epoch [13/25], Train Loss: 0.00011550242197699845, Validation Loss: 0.00014379148584945747\n",
      "Epoch [13/25], Train Loss: 0.00020726834191009402, Validation Loss: 0.00014395632533705793\n",
      "Epoch [13/25], Train Loss: 0.00020926979777868837, Validation Loss: 0.00014423398873380695\n",
      "Epoch [13/25], Train Loss: 0.00012293494364712387, Validation Loss: 0.0001443307970475871\n",
      "Epoch [13/25], Train Loss: 0.00016357041022274643, Validation Loss: 0.000144325994187966\n",
      "Epoch [13/25], Train Loss: 7.723725138930604e-05, Validation Loss: 0.00014428259892156347\n",
      "Epoch [13/25], Train Loss: 0.00016301232972182333, Validation Loss: 0.00014422249514609576\n",
      "Epoch [13/25], Train Loss: 0.0001265514292754233, Validation Loss: 0.00014413345367453683\n",
      "Epoch [13/25], Train Loss: 0.00011244234337937087, Validation Loss: 0.00014398351583319406\n",
      "Epoch [13/25], Train Loss: 0.00017487957666162401, Validation Loss: 0.00014399143959356782\n",
      "Epoch [13/25], Train Loss: 0.00013485571253113449, Validation Loss: 0.00014396182305063122\n",
      "Epoch [13/25], Train Loss: 0.0001758949365466833, Validation Loss: 0.00014385848965806265\n",
      "Epoch [13/25], Train Loss: 0.00016057619359344244, Validation Loss: 0.0001437176618007167\n",
      "Epoch [13/25], Train Loss: 0.00015409216575790197, Validation Loss: 0.00014371078214026056\n",
      "Epoch [13/25], Train Loss: 0.00021233574079815298, Validation Loss: 0.00014372012398477333\n",
      "Epoch [13/25], Train Loss: 0.00018466624896973372, Validation Loss: 0.00014375278478837573\n",
      "Epoch [13/25], Train Loss: 0.00013663721620105207, Validation Loss: 0.00014379655112861656\n",
      "Epoch [13/25], Train Loss: 0.0002088322362396866, Validation Loss: 0.00014379260489173854\n",
      "Epoch [13/25], Train Loss: 0.0001957963249878958, Validation Loss: 0.00014388933826315527\n",
      "Epoch [13/25], Train Loss: 0.00015647844702471048, Validation Loss: 0.00014398182296038916\n",
      "Epoch [13/25], Train Loss: 0.00012278878421057016, Validation Loss: 0.00014402676727816774\n",
      "Epoch [13/25], Train Loss: 0.00011252773401793092, Validation Loss: 0.00014411331794690342\n",
      "Epoch [13/25], Train Loss: 0.000183835654752329, Validation Loss: 0.00014425797902125245\n",
      "Epoch [13/25], Train Loss: 0.00015653850277885795, Validation Loss: 0.00014447615321842023\n",
      "Epoch [13/25], Train Loss: 0.00013363908510655165, Validation Loss: 0.0001446924259653315\n",
      "Epoch [13/25], Train Loss: 0.00013081190991215408, Validation Loss: 0.00014475846498195703\n",
      "Epoch [13/25], Train Loss: 0.0001772561518009752, Validation Loss: 0.00014494991022123333\n",
      "Epoch [13/25], Train Loss: 0.00016560486983507872, Validation Loss: 0.00014505795819180395\n",
      "Epoch [13/25], Train Loss: 0.00017284984642174095, Validation Loss: 0.00014513880499483395\n",
      "Epoch [13/25], Train Loss: 0.00016571988817304373, Validation Loss: 0.000145106377749471\n",
      "Epoch [13/25], Train Loss: 0.00018300300871487707, Validation Loss: 0.00014501015223989573\n",
      "Epoch [13/25], Train Loss: 0.0002575450052972883, Validation Loss: 0.00014473010839234727\n",
      "Epoch [13/25], Train Loss: 0.00016210028843488544, Validation Loss: 0.00014445579169356884\n",
      "Epoch [13/25], Train Loss: 0.00014728245150763541, Validation Loss: 0.000144162952589492\n",
      "Epoch [13/25], Train Loss: 0.00022325142344925553, Validation Loss: 0.00014391843578778207\n",
      "Epoch [13/25], Train Loss: 0.0002571056829765439, Validation Loss: 0.00014373374142451212\n",
      "Epoch [13/25], Train Loss: 0.00013716224930249155, Validation Loss: 0.00014372450653657627\n",
      "Epoch [13/25], Train Loss: 0.00016040378250181675, Validation Loss: 0.0001438492722324251\n",
      "Epoch [13/25], Train Loss: 0.0001274886162718758, Validation Loss: 0.0001440570939545675\n",
      "Epoch [13/25], Train Loss: 0.00020064339332748204, Validation Loss: 0.00014426122991911445\n",
      "Epoch [13/25], Train Loss: 0.00013877805031370372, Validation Loss: 0.00014451004996468935\n",
      "Epoch [13/25], Train Loss: 0.0001756487472448498, Validation Loss: 0.00014489813717470193\n",
      "Epoch [13/25], Train Loss: 0.00013048113032709807, Validation Loss: 0.00014528593407400574\n",
      "Epoch [13/25], Train Loss: 0.0001498589845141396, Validation Loss: 0.00014577999803198812\n",
      "Epoch [13/25], Train Loss: 0.00012754739145748317, Validation Loss: 0.00014611606748076155\n",
      "Epoch [13/25], Train Loss: 0.00015775511565152556, Validation Loss: 0.0001464658717547233\n",
      "Epoch [13/25], Train Loss: 0.00010742865561041981, Validation Loss: 0.00014620275663522383\n",
      "Epoch [13/25], Train Loss: 0.00015551902470178902, Validation Loss: 0.00014553183694564117\n",
      "Epoch [13/25], Train Loss: 0.0001416741724824533, Validation Loss: 0.00014459314552368597\n",
      "Epoch [13/25], Train Loss: 0.00015517436258960515, Validation Loss: 0.00014399928550119513\n",
      "Epoch [13/25], Train Loss: 0.00015413072833325714, Validation Loss: 0.00014382392109837382\n",
      "Epoch [13/25], Train Loss: 0.00015642678772564977, Validation Loss: 0.00014416153765826797\n",
      "Epoch [13/25], Train Loss: 0.00014936931256670505, Validation Loss: 0.00014468280229872714\n",
      "Epoch [13/25], Train Loss: 0.0001776877761585638, Validation Loss: 0.00014513618055692252\n",
      "Epoch [13/25], Train Loss: 0.0001506493572378531, Validation Loss: 0.00014528317672860187\n",
      "Epoch [13/25], Train Loss: 0.00017694100097287446, Validation Loss: 0.0001449962748059382\n",
      "Epoch [13/25], Train Loss: 0.00014724000357091427, Validation Loss: 0.00014455680817870113\n",
      "Epoch [13/25], Train Loss: 0.00015710899606347084, Validation Loss: 0.0001441120977688115\n",
      "Epoch [13/25], Train Loss: 0.0001684959133854136, Validation Loss: 0.000143808775950068\n",
      "Epoch [13/25], Train Loss: 0.00014298064343165606, Validation Loss: 0.00014374771029300367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Train Loss: 0.00012522374163381755, Validation Loss: 0.00014384092234346704\n",
      "Epoch [13/25], Train Loss: 0.00010857775487238541, Validation Loss: 0.00014405197240800285\n",
      "Epoch [13/25], Train Loss: 0.00010222806304227561, Validation Loss: 0.00014441724973342693\n",
      "Epoch [13/25], Train Loss: 0.00013839860912412405, Validation Loss: 0.0001446397315400342\n",
      "Epoch [13/25], Train Loss: 0.00013941714132670313, Validation Loss: 0.00014458454315899872\n",
      "Epoch [13/25], Train Loss: 0.00015994839486666024, Validation Loss: 0.0001445427376893349\n",
      "Epoch [13/25], Train Loss: 0.00015055411495268345, Validation Loss: 0.00014439719234360382\n",
      "Epoch [13/25], Train Loss: 0.00017151213251054287, Validation Loss: 0.00014418702412513084\n",
      "Epoch [13/25], Train Loss: 0.00017008843133226037, Validation Loss: 0.00014388697575971795\n",
      "Epoch [13/25], Train Loss: 0.00013427009980659932, Validation Loss: 0.00014372193739594272\n",
      "Epoch [13/25], Train Loss: 0.00013152183964848518, Validation Loss: 0.0001437115063405751\n",
      "Epoch [13/25], Train Loss: 0.00018925205222330987, Validation Loss: 0.00014378428313648327\n",
      "Epoch [13/25], Train Loss: 0.0001643581927055493, Validation Loss: 0.0001439138761876772\n",
      "Epoch [13/25], Train Loss: 0.00018403159629087895, Validation Loss: 0.0001439978873046736\n",
      "Epoch [13/25], Train Loss: 0.00016525216051377356, Validation Loss: 0.00014409068535314873\n",
      "Epoch [13/25], Train Loss: 0.00013510612188838422, Validation Loss: 0.00014405094407266006\n",
      "Epoch [13/25], Train Loss: 0.00017307781672570854, Validation Loss: 0.00014407353737624362\n",
      "Epoch [13/25], Train Loss: 0.0001468011614633724, Validation Loss: 0.00014410687702669142\n",
      "Epoch [13/25], Train Loss: 0.00015535333659499884, Validation Loss: 0.00014411113370442762\n",
      "Epoch [13/25], Train Loss: 0.00019265133596491069, Validation Loss: 0.00014414267958879162\n",
      "Epoch [13/25], Train Loss: 0.00010416143777547404, Validation Loss: 0.00014421039644124296\n",
      "Epoch [13/25], Train Loss: 0.00011049785825889558, Validation Loss: 0.00014431350646191277\n",
      "Epoch [13/25], Train Loss: 0.0001527273707324639, Validation Loss: 0.00014448471653546828\n",
      "Epoch [13/25], Train Loss: 0.0001422896166332066, Validation Loss: 0.00014471219474216924\n",
      "Epoch [13/25], Train Loss: 0.0001189528702525422, Validation Loss: 0.00014518032548949122\n",
      "Epoch [13/25], Train Loss: 0.00015111698303371668, Validation Loss: 0.00014586495647866588\n",
      "Epoch [13/25], Train Loss: 0.00015868240734562278, Validation Loss: 0.00014691828182549215\n",
      "Epoch [13/25], Train Loss: 0.0001835060684243217, Validation Loss: 0.00014795184251852332\n",
      "Epoch [13/25], Train Loss: 0.00012776395305991173, Validation Loss: 0.00014918982803161876\n",
      "Epoch [13/25], Train Loss: 0.00021525057672988623, Validation Loss: 0.00014924118925894921\n",
      "Epoch [13/25], Train Loss: 7.701884169364348e-05, Validation Loss: 0.00014814101596130058\n",
      "Epoch [13/25], Train Loss: 0.000185559198143892, Validation Loss: 0.00014581315335817635\n",
      "Epoch [13/25], Train Loss: 0.0001172626216430217, Validation Loss: 0.0001440216515523692\n",
      "Epoch [13/25], Train Loss: 0.00014185250620357692, Validation Loss: 0.00014394518827126984\n",
      "Epoch [13/25], Train Loss: 0.00014263266348280013, Validation Loss: 0.00014518319488464234\n",
      "Epoch [13/25], Train Loss: 0.00011769652337534353, Validation Loss: 0.00014577080922511715\n",
      "Epoch [13/25], Train Loss: 0.00013891671551391482, Validation Loss: 0.00014481493296140494\n",
      "Epoch [13/25], Train Loss: 0.00011932397319469601, Validation Loss: 0.00014384305880715448\n",
      "Epoch [13/25], Train Loss: 0.0001767836802173406, Validation Loss: 0.00014423612252964328\n",
      "Epoch [13/25], Train Loss: 0.00020158858387731016, Validation Loss: 0.000144940786655449\n",
      "Epoch [13/25], Train Loss: 0.0001781307510100305, Validation Loss: 0.000145188185221438\n",
      "Epoch [13/25], Train Loss: 0.00017573387594893575, Validation Loss: 0.00014452403023218114\n",
      "Epoch [13/25], Train Loss: 0.00010442475468153134, Validation Loss: 0.00014388852578122168\n",
      "Epoch [13/25], Train Loss: 0.00013359746662899852, Validation Loss: 0.00014410327518514046\n",
      "Epoch [13/25], Train Loss: 8.03798611741513e-05, Validation Loss: 0.00014507530528741578\n",
      "Epoch [13/25], Train Loss: 0.00012229138519614935, Validation Loss: 0.00014571008335527343\n",
      "Epoch [13/25], Train Loss: 0.00018564885249361396, Validation Loss: 0.0001451484805632693\n",
      "Epoch [13/25], Train Loss: 0.000225303418119438, Validation Loss: 0.00014452103399283564\n",
      "Epoch [13/25], Train Loss: 0.00011101434211013839, Validation Loss: 0.0001438845123630017\n",
      "Epoch [13/25], Train Loss: 0.0001954345207195729, Validation Loss: 0.00014379873367336888\n",
      "Epoch [13/25], Train Loss: 0.00017252948600798845, Validation Loss: 0.00014411204950495933\n",
      "Epoch [13/25], Train Loss: 0.00018420435662847012, Validation Loss: 0.00014439445949392392\n",
      "Epoch [13/25], Train Loss: 0.00012312756734900177, Validation Loss: 0.00014436204631541235\n",
      "Epoch [13/25], Train Loss: 0.00015657863696105778, Validation Loss: 0.00014407871300742652\n",
      "Epoch [13/25], Train Loss: 0.00021088385256007314, Validation Loss: 0.00014383267868349017\n",
      "Epoch [13/25], Train Loss: 0.00017116908566094935, Validation Loss: 0.0001437105658017875\n",
      "Epoch [13/25], Train Loss: 0.00011522608838276938, Validation Loss: 0.00014380708259219925\n",
      "Epoch [13/25], Train Loss: 0.0001309937797486782, Validation Loss: 0.00014396231514789785\n",
      "Epoch [13/25], Train Loss: 0.00010629301687004045, Validation Loss: 0.00014408283702020222\n",
      "Epoch [13/25], Train Loss: 0.0001954295439645648, Validation Loss: 0.00014435581712556693\n",
      "Epoch [13/25], Train Loss: 0.00019380470621399581, Validation Loss: 0.00014466105543154602\n",
      "Epoch [13/25], Train Loss: 0.00016398051229771227, Validation Loss: 0.0001451819101930596\n",
      "Epoch [13/25], Train Loss: 0.00014589518832508475, Validation Loss: 0.00014565211507336546\n",
      "Epoch [13/25], Train Loss: 0.00013100683281663805, Validation Loss: 0.00014630756922997534\n",
      "Epoch [13/25], Train Loss: 0.00012562789197545499, Validation Loss: 0.0001465857067766289\n",
      "Epoch [13/25], Train Loss: 0.00014600346912629902, Validation Loss: 0.00014678650162143944\n",
      "Epoch [13/25], Train Loss: 0.00013647017476614565, Validation Loss: 0.00014626520860474556\n",
      "Epoch [13/25], Train Loss: 0.00014000022201798856, Validation Loss: 0.0001455115037970245\n",
      "Epoch [13/25], Train Loss: 0.00017622682207729667, Validation Loss: 0.00014454056072281672\n",
      "Epoch [13/25], Train Loss: 0.00019426245125941932, Validation Loss: 0.00014381895768262137\n",
      "Epoch [13/25], Train Loss: 0.00012088254879927263, Validation Loss: 0.00014387609456510594\n",
      "Epoch [13/25], Train Loss: 0.0001809308014344424, Validation Loss: 0.00014451905444730074\n",
      "Epoch [13/25], Train Loss: 0.00014513630594592541, Validation Loss: 0.0001450524272513576\n",
      "Epoch [13/25], Train Loss: 0.00021390640176832676, Validation Loss: 0.00014500990752518798\n",
      "Epoch [13/25], Train Loss: 0.00011255822755629197, Validation Loss: 0.00014454329624034775\n",
      "Epoch [13/25], Train Loss: 0.00017138825205620378, Validation Loss: 0.0001439258000270153\n",
      "Epoch [13/25], Train Loss: 0.0001602729462319985, Validation Loss: 0.00014367140029207802\n",
      "Epoch [13/25], Train Loss: 0.00014295992150437087, Validation Loss: 0.00014394093086593783\n",
      "Epoch [13/25], Train Loss: 0.00012539491581264883, Validation Loss: 0.0001444866681898323\n",
      "Epoch [13/25], Train Loss: 0.00015790134784765542, Validation Loss: 0.00014513778223772532\n",
      "Epoch [13/25], Train Loss: 0.00015226074901875108, Validation Loss: 0.0001457862364380465\n",
      "Epoch [13/25], Train Loss: 0.00017431366723030806, Validation Loss: 0.000146670670316477\n",
      "Epoch [13/25], Train Loss: 0.00016961389337666333, Validation Loss: 0.00014643054261493187\n",
      "Epoch [13/25], Train Loss: 0.00017175893299281597, Validation Loss: 0.0001459613507904578\n",
      "Epoch [13/25], Train Loss: 0.00012908536882605404, Validation Loss: 0.00014473725241259672\n",
      "Epoch [13/25], Train Loss: 0.00020285675418563187, Validation Loss: 0.00014393229915488822\n",
      "Epoch [13/25], Train Loss: 0.0001227143220603466, Validation Loss: 0.00014391051590791903\n",
      "Epoch [13/25], Train Loss: 0.00018130654643755406, Validation Loss: 0.00014438428697758355\n",
      "Epoch [13/25], Train Loss: 0.00017370384011883289, Validation Loss: 0.00014485978851250063\n",
      "Epoch [13/25], Train Loss: 0.00019670254550874233, Validation Loss: 0.00014496233270619996\n",
      "Epoch [13/25], Train Loss: 0.00015877770783845335, Validation Loss: 0.00014469482945666338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], Train Loss: 0.0001290853979298845, Validation Loss: 0.0001440232995567688\n",
      "Epoch [14/25], Train Loss: 0.00012507666542660445, Validation Loss: 0.00014373570932851482\n",
      "Epoch [14/25], Train Loss: 0.0001468860573368147, Validation Loss: 0.00014395129813541037\n",
      "Epoch [14/25], Train Loss: 7.850834663258865e-05, Validation Loss: 0.0001441979887507235\n",
      "Epoch [14/25], Train Loss: 0.00013113078603055328, Validation Loss: 0.00014437244972214102\n",
      "Epoch [14/25], Train Loss: 0.00014368750271387398, Validation Loss: 0.0001445620640879497\n",
      "Epoch [14/25], Train Loss: 0.00015633046859875321, Validation Loss: 0.00014469663486427938\n",
      "Epoch [14/25], Train Loss: 0.00015170736878644675, Validation Loss: 0.00014480286166265917\n",
      "Epoch [14/25], Train Loss: 0.00014635888510383666, Validation Loss: 0.0001447319807387733\n",
      "Epoch [14/25], Train Loss: 0.0001067061020876281, Validation Loss: 0.0001443877149237475\n",
      "Epoch [14/25], Train Loss: 0.0001765736669767648, Validation Loss: 0.00014412781844536465\n",
      "Epoch [14/25], Train Loss: 0.00015142619668040425, Validation Loss: 0.00014391362904765022\n",
      "Epoch [14/25], Train Loss: 0.0002166926715290174, Validation Loss: 0.00014372757893094484\n",
      "Epoch [14/25], Train Loss: 0.00014780726633034647, Validation Loss: 0.00014364998811894719\n",
      "Epoch [14/25], Train Loss: 0.00015924873878248036, Validation Loss: 0.00014362504759143728\n",
      "Epoch [14/25], Train Loss: 0.00013304909225553274, Validation Loss: 0.00014368097981787287\n",
      "Epoch [14/25], Train Loss: 0.00014952756464481354, Validation Loss: 0.00014382174146400454\n",
      "Epoch [14/25], Train Loss: 0.00013085184036754072, Validation Loss: 0.0001439462864558057\n",
      "Epoch [14/25], Train Loss: 0.00016804595361463726, Validation Loss: 0.0001440402234341794\n",
      "Epoch [14/25], Train Loss: 0.00015594789874739945, Validation Loss: 0.0001441950000298675\n",
      "Epoch [14/25], Train Loss: 0.00020751576812472194, Validation Loss: 0.0001443216383146743\n",
      "Epoch [14/25], Train Loss: 0.00014268387167248875, Validation Loss: 0.00014442094519229916\n",
      "Epoch [14/25], Train Loss: 0.00018243977683596313, Validation Loss: 0.00014448277142946607\n",
      "Epoch [14/25], Train Loss: 0.00015028710186015815, Validation Loss: 0.0001446182975390305\n",
      "Epoch [14/25], Train Loss: 0.00018788993475027382, Validation Loss: 0.00014451631359406746\n",
      "Epoch [14/25], Train Loss: 0.00014070454926695675, Validation Loss: 0.00014456446854940925\n",
      "Epoch [14/25], Train Loss: 0.00016788642096798867, Validation Loss: 0.00014455212149186992\n",
      "Epoch [14/25], Train Loss: 0.00018713822646532208, Validation Loss: 0.00014446362474700435\n",
      "Epoch [14/25], Train Loss: 0.00014953601930756122, Validation Loss: 0.00014427521722003197\n",
      "Epoch [14/25], Train Loss: 0.00015656132018193603, Validation Loss: 0.00014402482484001666\n",
      "Epoch [14/25], Train Loss: 0.00016943816444836557, Validation Loss: 0.00014386083251641442\n",
      "Epoch [14/25], Train Loss: 0.0001468755945097655, Validation Loss: 0.00014377173332225842\n",
      "Epoch [14/25], Train Loss: 0.00012016438995487988, Validation Loss: 0.00014369837954291141\n",
      "Epoch [14/25], Train Loss: 0.0001416784361936152, Validation Loss: 0.00014363435305616198\n",
      "Epoch [14/25], Train Loss: 0.00018398783868178725, Validation Loss: 0.00014357884332033183\n",
      "Epoch [14/25], Train Loss: 0.0001974970946321264, Validation Loss: 0.00014362618458108045\n",
      "Epoch [14/25], Train Loss: 8.118730329442769e-05, Validation Loss: 0.00014374462625710294\n",
      "Epoch [14/25], Train Loss: 0.00014833077148068696, Validation Loss: 0.0001437818915292155\n",
      "Epoch [14/25], Train Loss: 0.00017065969586838037, Validation Loss: 0.00014379630010807887\n",
      "Epoch [14/25], Train Loss: 0.00016944101662375033, Validation Loss: 0.0001438541624035376\n",
      "Epoch [14/25], Train Loss: 0.0001354758715024218, Validation Loss: 0.00014398990339638355\n",
      "Epoch [14/25], Train Loss: 0.0001760231825755909, Validation Loss: 0.00014430540104513057\n",
      "Epoch [14/25], Train Loss: 7.898497278802097e-05, Validation Loss: 0.0001445435998903122\n",
      "Epoch [14/25], Train Loss: 0.00017508535529486835, Validation Loss: 0.00014461182023903045\n",
      "Epoch [14/25], Train Loss: 8.420078665949404e-05, Validation Loss: 0.00014477330793548996\n",
      "Epoch [14/25], Train Loss: 0.0001897658803500235, Validation Loss: 0.00014499605361682674\n",
      "Epoch [14/25], Train Loss: 0.00011856909259222448, Validation Loss: 0.00014501733530778438\n",
      "Epoch [14/25], Train Loss: 0.00012619193876162171, Validation Loss: 0.00014507165275669346\n",
      "Epoch [14/25], Train Loss: 0.00014205639308784157, Validation Loss: 0.00014522984129143878\n",
      "Epoch [14/25], Train Loss: 0.00017054528871085495, Validation Loss: 0.00014497526935883798\n",
      "Epoch [14/25], Train Loss: 0.0001922410592669621, Validation Loss: 0.0001447538634238299\n",
      "Epoch [14/25], Train Loss: 0.00011270828690612689, Validation Loss: 0.0001442004172228432\n",
      "Epoch [14/25], Train Loss: 0.000160618539666757, Validation Loss: 0.00014371901997947134\n",
      "Epoch [14/25], Train Loss: 0.00013574687181971967, Validation Loss: 0.00014373748805761958\n",
      "Epoch [14/25], Train Loss: 0.00017458874208386987, Validation Loss: 0.00014377666884684003\n",
      "Epoch [14/25], Train Loss: 0.0001458383194403723, Validation Loss: 0.00014395230246009306\n",
      "Epoch [14/25], Train Loss: 0.00016081944340839982, Validation Loss: 0.00014421113276815353\n",
      "Epoch [14/25], Train Loss: 0.00012154663272667676, Validation Loss: 0.0001442593515093904\n",
      "Epoch [14/25], Train Loss: 0.0001699725689832121, Validation Loss: 0.00014430396064805487\n",
      "Epoch [14/25], Train Loss: 0.00012109895033063367, Validation Loss: 0.00014427141213673166\n",
      "Epoch [14/25], Train Loss: 0.00015969255764503032, Validation Loss: 0.00014412434732851882\n",
      "Epoch [14/25], Train Loss: 0.00011551346688065678, Validation Loss: 0.00014401308483987427\n",
      "Epoch [14/25], Train Loss: 0.00019831214740406722, Validation Loss: 0.00014388015357932697\n",
      "Epoch [14/25], Train Loss: 0.000152082386193797, Validation Loss: 0.00014371417055372148\n",
      "Epoch [14/25], Train Loss: 0.00015665106184314936, Validation Loss: 0.00014364546586875804\n",
      "Epoch [14/25], Train Loss: 0.0001488721027271822, Validation Loss: 0.0001436539212591015\n",
      "Epoch [14/25], Train Loss: 0.00016335753025487065, Validation Loss: 0.00014358729337497303\n",
      "Epoch [14/25], Train Loss: 0.0001208126122946851, Validation Loss: 0.00014365017535358977\n",
      "Epoch [14/25], Train Loss: 0.00023451609013136476, Validation Loss: 0.00014376713006640783\n",
      "Epoch [14/25], Train Loss: 0.00016472696734126657, Validation Loss: 0.00014391169800849943\n",
      "Epoch [14/25], Train Loss: 0.00015717235510237515, Validation Loss: 0.00014408827264560385\n",
      "Epoch [14/25], Train Loss: 0.00023367482936009765, Validation Loss: 0.00014420402197477719\n",
      "Epoch [14/25], Train Loss: 0.00015044516476336867, Validation Loss: 0.000144533749941426\n",
      "Epoch [14/25], Train Loss: 0.00010487556573934853, Validation Loss: 0.00014512099951389246\n",
      "Epoch [14/25], Train Loss: 0.00018369688768871129, Validation Loss: 0.00014603213179119242\n",
      "Epoch [14/25], Train Loss: 0.00015291932504624128, Validation Loss: 0.00014725937823338123\n",
      "Epoch [14/25], Train Loss: 0.00011543038272066042, Validation Loss: 0.00014890328578379315\n",
      "Epoch [14/25], Train Loss: 0.000279760715784505, Validation Loss: 0.00014948136037370812\n",
      "Epoch [14/25], Train Loss: 0.00016898704052437097, Validation Loss: 0.0001487343261639277\n",
      "Epoch [14/25], Train Loss: 8.760061609791592e-05, Validation Loss: 0.00014595988444246661\n",
      "Epoch [14/25], Train Loss: 0.00022132009326014668, Validation Loss: 0.00014396917322301306\n",
      "Epoch [14/25], Train Loss: 0.00017121125711128116, Validation Loss: 0.00014527144618720438\n",
      "Epoch [14/25], Train Loss: 0.0001086894772015512, Validation Loss: 0.00014706237246476424\n",
      "Epoch [14/25], Train Loss: 0.00016714143566787243, Validation Loss: 0.0001471337331167888\n",
      "Epoch [14/25], Train Loss: 0.00020687570213340223, Validation Loss: 0.0001460893858165946\n",
      "Epoch [14/25], Train Loss: 0.00016415980644524097, Validation Loss: 0.0001447153040013897\n",
      "Epoch [14/25], Train Loss: 0.00015590517432428896, Validation Loss: 0.0001437710353153913\n",
      "Epoch [14/25], Train Loss: 0.0001729003561194986, Validation Loss: 0.00014414585311897098\n",
      "Epoch [14/25], Train Loss: 0.0001340918242931366, Validation Loss: 0.00014492667784603934\n",
      "Epoch [14/25], Train Loss: 0.00019028241513296962, Validation Loss: 0.0001451527464572185\n",
      "Epoch [14/25], Train Loss: 0.0001804529456421733, Validation Loss: 0.00014575117505349528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], Train Loss: 0.00016400316962972283, Validation Loss: 0.00014586399047402664\n",
      "Epoch [14/25], Train Loss: 0.00016891890845727175, Validation Loss: 0.00014619666399084963\n",
      "Epoch [14/25], Train Loss: 0.00016936735482886434, Validation Loss: 0.00014557615237814996\n",
      "Epoch [14/25], Train Loss: 0.00012901212903670967, Validation Loss: 0.00014457408591018369\n",
      "Epoch [14/25], Train Loss: 0.00014890830789227039, Validation Loss: 0.00014395609105122276\n",
      "Epoch [14/25], Train Loss: 0.00015376383089460433, Validation Loss: 0.0001440897016436793\n",
      "Epoch [14/25], Train Loss: 0.00016428470553364605, Validation Loss: 0.0001442933142243419\n",
      "Epoch [14/25], Train Loss: 0.00013849491369910538, Validation Loss: 0.0001444838354170012\n",
      "Epoch [14/25], Train Loss: 0.0001447643298888579, Validation Loss: 0.00014447124534247753\n",
      "Epoch [14/25], Train Loss: 0.00014491828915197402, Validation Loss: 0.00014424379551201128\n",
      "Epoch [14/25], Train Loss: 0.0002304125519003719, Validation Loss: 0.00014415547314759655\n",
      "Epoch [14/25], Train Loss: 0.00014021761307958513, Validation Loss: 0.00014402542195360486\n",
      "Epoch [14/25], Train Loss: 0.00012583952047862113, Validation Loss: 0.00014382658531152023\n",
      "Epoch [14/25], Train Loss: 0.0002515659434720874, Validation Loss: 0.00014400721241448385\n",
      "Epoch [14/25], Train Loss: 0.00021150668908376247, Validation Loss: 0.00014406231542428335\n",
      "Epoch [14/25], Train Loss: 0.00016227303422056139, Validation Loss: 0.00014401375810848548\n",
      "Epoch [14/25], Train Loss: 0.00010314633982488886, Validation Loss: 0.00014401254811673424\n",
      "Epoch [14/25], Train Loss: 0.0001726630871417001, Validation Loss: 0.0001440023525598614\n",
      "Epoch [14/25], Train Loss: 0.000165136661962606, Validation Loss: 0.00014412300176142406\n",
      "Epoch [14/25], Train Loss: 0.00017715881404001266, Validation Loss: 0.00014442596050988262\n",
      "Epoch [14/25], Train Loss: 0.00020972636411897838, Validation Loss: 0.0001447121673360622\n",
      "Epoch [14/25], Train Loss: 0.00016674915968906134, Validation Loss: 0.00014546208428024935\n",
      "Epoch [14/25], Train Loss: 0.000174912202055566, Validation Loss: 0.00014682531376214076\n",
      "Epoch [14/25], Train Loss: 0.00014597827976103872, Validation Loss: 0.00014912648354462968\n",
      "Epoch [14/25], Train Loss: 0.0001621454139240086, Validation Loss: 0.00015139366587391123\n",
      "Epoch [14/25], Train Loss: 0.0001708792260615155, Validation Loss: 0.0001546920104980624\n",
      "Epoch [14/25], Train Loss: 0.00020703367772512138, Validation Loss: 0.00015490175913631296\n",
      "Epoch [14/25], Train Loss: 0.00018490244110580534, Validation Loss: 0.00015148420425248332\n",
      "Epoch [14/25], Train Loss: 0.00016768819477874786, Validation Loss: 0.00014517289431144795\n",
      "Epoch [14/25], Train Loss: 0.00013493817823473364, Validation Loss: 0.00014538546917416777\n",
      "Epoch [14/25], Train Loss: 0.00011653800174826756, Validation Loss: 0.00014977453708221825\n",
      "Epoch [14/25], Train Loss: 0.00022500843624584377, Validation Loss: 0.00014845320280680122\n",
      "Epoch [14/25], Train Loss: 0.00014084187569096684, Validation Loss: 0.00014463434902912316\n",
      "Epoch [14/25], Train Loss: 0.00023292326659429818, Validation Loss: 0.00014482918898769034\n",
      "Epoch [14/25], Train Loss: 0.00013813149416819215, Validation Loss: 0.0001471722830804841\n",
      "Epoch [14/25], Train Loss: 0.00019711859931703657, Validation Loss: 0.00014817747360211796\n",
      "Epoch [14/25], Train Loss: 0.00016699331172276288, Validation Loss: 0.00014534284637193195\n",
      "Epoch [14/25], Train Loss: 0.00010121001832885668, Validation Loss: 0.0001439832296455279\n",
      "Epoch [14/25], Train Loss: 0.00010345808550482616, Validation Loss: 0.00014530886959012907\n",
      "Epoch [14/25], Train Loss: 0.00021330801246222109, Validation Loss: 0.0001458592191435552\n",
      "Epoch [14/25], Train Loss: 0.00014602213923353702, Validation Loss: 0.00014594612390889477\n",
      "Epoch [14/25], Train Loss: 0.00023807074467185885, Validation Loss: 0.00014426150834575917\n",
      "Epoch [14/25], Train Loss: 0.0002277339663123712, Validation Loss: 0.00014392243950472523\n",
      "Epoch [14/25], Train Loss: 0.0001328537764493376, Validation Loss: 0.00014384376918314957\n",
      "Epoch [14/25], Train Loss: 0.00015519974112976342, Validation Loss: 0.0001438217113900464\n",
      "Epoch [14/25], Train Loss: 0.00018484031897969544, Validation Loss: 0.00014390465415393313\n",
      "Epoch [14/25], Train Loss: 0.00014697914593853056, Validation Loss: 0.00014390503395892058\n",
      "Epoch [14/25], Train Loss: 0.00011189660290256143, Validation Loss: 0.00014372750665643254\n",
      "Epoch [14/25], Train Loss: 0.00014406684204004705, Validation Loss: 0.0001437058412799767\n",
      "Epoch [14/25], Train Loss: 0.00010172618931392208, Validation Loss: 0.00014378699682614146\n",
      "Epoch [14/25], Train Loss: 0.00018719160289037973, Validation Loss: 0.0001438098314489859\n",
      "Epoch [14/25], Train Loss: 0.00013281886640470475, Validation Loss: 0.00014381651102060762\n",
      "Epoch [14/25], Train Loss: 0.00012190119014121592, Validation Loss: 0.0001437600619586495\n",
      "Epoch [14/25], Train Loss: 0.00018810451729223132, Validation Loss: 0.0001436674050637521\n",
      "Epoch [14/25], Train Loss: 9.528213558951393e-05, Validation Loss: 0.00014368445505776132\n",
      "Epoch [14/25], Train Loss: 0.00023050398158375174, Validation Loss: 0.00014364566231961362\n",
      "Epoch [14/25], Train Loss: 0.0001771044626366347, Validation Loss: 0.000143698542524362\n",
      "Epoch [14/25], Train Loss: 0.00012873638479504734, Validation Loss: 0.0001438144223357085\n",
      "Epoch [14/25], Train Loss: 0.0001585939317010343, Validation Loss: 0.0001437918115698267\n",
      "Epoch [14/25], Train Loss: 0.00018206446839030832, Validation Loss: 0.00014370445375485967\n",
      "Epoch [14/25], Train Loss: 0.0001429529656888917, Validation Loss: 0.0001437286635336932\n",
      "Epoch [14/25], Train Loss: 0.00013739244604948908, Validation Loss: 0.00014370493566578563\n",
      "Epoch [14/25], Train Loss: 0.0001653523213462904, Validation Loss: 0.0001436977962536427\n",
      "Epoch [14/25], Train Loss: 0.00012995494762435555, Validation Loss: 0.00014379629234705741\n",
      "Epoch [14/25], Train Loss: 0.00013045419473201036, Validation Loss: 0.00014369035173634377\n",
      "Epoch [14/25], Train Loss: 0.00016582751413807273, Validation Loss: 0.0001436090145337706\n",
      "Epoch [14/25], Train Loss: 0.00011382556112948805, Validation Loss: 0.00014361884677782655\n",
      "Epoch [14/25], Train Loss: 0.00015821839042473584, Validation Loss: 0.00014360095325779791\n",
      "Epoch [14/25], Train Loss: 0.00017568995826877654, Validation Loss: 0.00014359021370182745\n",
      "Epoch [14/25], Train Loss: 0.00016161588428076357, Validation Loss: 0.00014358266295554737\n",
      "Epoch [14/25], Train Loss: 0.00013603454863186926, Validation Loss: 0.00014357340623973868\n",
      "Epoch [14/25], Train Loss: 0.00010611458128551021, Validation Loss: 0.00014359933314456915\n",
      "Epoch [14/25], Train Loss: 0.00018230102432426065, Validation Loss: 0.00014360716765319617\n",
      "Epoch [14/25], Train Loss: 0.00010808164370246232, Validation Loss: 0.00014360541293475157\n",
      "Epoch [14/25], Train Loss: 0.0001748275535646826, Validation Loss: 0.00014362565877187687\n",
      "Epoch [14/25], Train Loss: 6.62759121041745e-05, Validation Loss: 0.00014367014809977264\n",
      "Epoch [14/25], Train Loss: 0.00017457830836065114, Validation Loss: 0.00014371777118261282\n",
      "Epoch [14/25], Train Loss: 0.00015118723968043923, Validation Loss: 0.00014372149501771977\n",
      "Epoch [14/25], Train Loss: 0.00018893250671681017, Validation Loss: 0.00014370374119607732\n",
      "Epoch [14/25], Train Loss: 0.00012229950516484678, Validation Loss: 0.0001436797530914191\n",
      "Epoch [14/25], Train Loss: 0.00016545335529372096, Validation Loss: 0.00014363396257977001\n",
      "Epoch [14/25], Train Loss: 0.00015764147974550724, Validation Loss: 0.00014357526112386646\n",
      "Epoch [14/25], Train Loss: 0.00012588519894052297, Validation Loss: 0.0001435618818504736\n",
      "Epoch [14/25], Train Loss: 0.00013822213804814965, Validation Loss: 0.0001435895103592581\n",
      "Epoch [14/25], Train Loss: 8.554831583751366e-05, Validation Loss: 0.00014358940679812804\n",
      "Epoch [14/25], Train Loss: 0.00015375179646071047, Validation Loss: 0.00014356627580127678\n",
      "Epoch [14/25], Train Loss: 0.00013124433462508023, Validation Loss: 0.00014359222501904393\n",
      "Epoch [14/25], Train Loss: 0.0001518578501418233, Validation Loss: 0.00014373861558851785\n",
      "Epoch [14/25], Train Loss: 0.0001458178594475612, Validation Loss: 0.00014379180259614562\n",
      "Epoch [14/25], Train Loss: 0.0001517174532637, Validation Loss: 0.00014367319333056608\n",
      "Epoch [14/25], Train Loss: 0.0001391825353493914, Validation Loss: 0.0001436276705741572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], Train Loss: 0.00016256819071713835, Validation Loss: 0.00014368480575891832\n",
      "Epoch [14/25], Train Loss: 0.00019003677880391479, Validation Loss: 0.00014373791782418266\n",
      "Epoch [14/25], Train Loss: 0.00017281995678786188, Validation Loss: 0.00014376696684242535\n",
      "Epoch [14/25], Train Loss: 0.00015812754281796515, Validation Loss: 0.00014384821867376255\n",
      "Epoch [14/25], Train Loss: 0.00011643766629276797, Validation Loss: 0.00014396724921728795\n",
      "Epoch [14/25], Train Loss: 0.00010116896737599745, Validation Loss: 0.0001440956060832832\n",
      "Epoch [14/25], Train Loss: 0.000132805245812051, Validation Loss: 0.00014414033551778024\n",
      "Epoch [14/25], Train Loss: 9.104367200052366e-05, Validation Loss: 0.00014418634867373233\n",
      "Epoch [14/25], Train Loss: 0.00018618219473864883, Validation Loss: 0.00014434700739608767\n",
      "Epoch [14/25], Train Loss: 0.00018640536291059107, Validation Loss: 0.00014448029639121767\n",
      "Epoch [14/25], Train Loss: 0.0001426104427082464, Validation Loss: 0.0001446576645927659\n",
      "Epoch [14/25], Train Loss: 0.00013054857845418155, Validation Loss: 0.00014476398840391387\n",
      "Epoch [14/25], Train Loss: 0.0001576748036313802, Validation Loss: 0.000144777110836003\n",
      "Epoch [14/25], Train Loss: 0.0001834709692047909, Validation Loss: 0.00014469660600298085\n",
      "Epoch [14/25], Train Loss: 0.0001497944031143561, Validation Loss: 0.00014444484331761488\n",
      "Epoch [14/25], Train Loss: 0.00011832232848973945, Validation Loss: 0.00014403588623584557\n",
      "Epoch [14/25], Train Loss: 0.0001461865904275328, Validation Loss: 0.0001437982456991449\n",
      "Epoch [14/25], Train Loss: 0.00021968288638163358, Validation Loss: 0.00014366927741017813\n",
      "Epoch [14/25], Train Loss: 0.00011397853813832626, Validation Loss: 0.00014355711124759788\n",
      "Epoch [14/25], Train Loss: 0.00012824196892324835, Validation Loss: 0.0001436233537485047\n",
      "Epoch [14/25], Train Loss: 0.0001752763637341559, Validation Loss: 0.00014370633458990294\n",
      "Epoch [14/25], Train Loss: 0.0001714794198051095, Validation Loss: 0.000143766870314721\n",
      "Epoch [14/25], Train Loss: 0.00013367936480790377, Validation Loss: 0.00014394964285505314\n",
      "Epoch [14/25], Train Loss: 0.0001808809902286157, Validation Loss: 0.00014419710884491603\n",
      "Epoch [14/25], Train Loss: 0.00015891590737737715, Validation Loss: 0.0001442801161222936\n",
      "Epoch [14/25], Train Loss: 0.00018680098582990468, Validation Loss: 0.0001445265137590468\n",
      "Epoch [14/25], Train Loss: 0.00019217860244680196, Validation Loss: 0.00014468272032293802\n",
      "Epoch [14/25], Train Loss: 0.00019829522352665663, Validation Loss: 0.0001448330197793742\n",
      "Epoch [14/25], Train Loss: 0.00014181520964484662, Validation Loss: 0.00014480213479449352\n",
      "Epoch [14/25], Train Loss: 0.00012901936133857816, Validation Loss: 0.0001447503294912167\n",
      "Epoch [14/25], Train Loss: 0.00015209066623356193, Validation Loss: 0.0001445158755814191\n",
      "Epoch [14/25], Train Loss: 0.00012340908870100975, Validation Loss: 0.00014430685308373844\n",
      "Epoch [14/25], Train Loss: 0.00015675107715651393, Validation Loss: 0.00014401654601291133\n",
      "Epoch [14/25], Train Loss: 0.00020643201423808932, Validation Loss: 0.00014372342678446633\n",
      "Epoch [14/25], Train Loss: 0.00019092345610260963, Validation Loss: 0.00014358040037526127\n",
      "Epoch [14/25], Train Loss: 0.00012151461851317436, Validation Loss: 0.00014362849591028257\n",
      "Epoch [14/25], Train Loss: 0.00014988567272666842, Validation Loss: 0.00014375610893087773\n",
      "Epoch [14/25], Train Loss: 0.00017116829985752702, Validation Loss: 0.00014394841661366324\n",
      "Epoch [14/25], Train Loss: 0.00016598403453826904, Validation Loss: 0.00014405455828333895\n",
      "Epoch [14/25], Train Loss: 0.0001841609482653439, Validation Loss: 0.00014423223159004312\n",
      "Epoch [14/25], Train Loss: 0.00015932122187223285, Validation Loss: 0.0001443473074080733\n",
      "Epoch [14/25], Train Loss: 0.00014946966257411987, Validation Loss: 0.00014434646000154315\n",
      "Epoch [14/25], Train Loss: 0.00013009017857257277, Validation Loss: 0.00014426754278247246\n",
      "Epoch [14/25], Train Loss: 7.704427844146267e-05, Validation Loss: 0.00014419285750288206\n",
      "Epoch [14/25], Train Loss: 0.000153127868543379, Validation Loss: 0.00014415821812387245\n",
      "Epoch [14/25], Train Loss: 0.00017969252075999975, Validation Loss: 0.00014399173984808538\n",
      "Epoch [14/25], Train Loss: 0.00013434869470074773, Validation Loss: 0.00014384314151053937\n",
      "Epoch [14/25], Train Loss: 0.00018386841111350805, Validation Loss: 0.00014372677639282\n",
      "Epoch [14/25], Train Loss: 0.00015275653277058154, Validation Loss: 0.00014366403556778095\n",
      "Epoch [14/25], Train Loss: 0.00010190775356022641, Validation Loss: 0.000143641641625436\n",
      "Epoch [14/25], Train Loss: 0.0001478996709920466, Validation Loss: 0.00014362489794924235\n",
      "Epoch [14/25], Train Loss: 0.00017255214333999902, Validation Loss: 0.00014356211734896837\n",
      "Epoch [14/25], Train Loss: 0.00020305717771407217, Validation Loss: 0.00014359459964907728\n",
      "Epoch [14/25], Train Loss: 0.00013871070405002683, Validation Loss: 0.0001436137911999443\n",
      "Epoch [14/25], Train Loss: 0.0001470352290198207, Validation Loss: 0.00014357400505105034\n",
      "Epoch [14/25], Train Loss: 0.00017595509416423738, Validation Loss: 0.00014357161877948482\n",
      "Epoch [14/25], Train Loss: 0.00013488787226378918, Validation Loss: 0.00014356016933258313\n",
      "Epoch [14/25], Train Loss: 0.0001911256113089621, Validation Loss: 0.00014356656780970903\n",
      "Epoch [14/25], Train Loss: 0.00012659725325647742, Validation Loss: 0.000143643462312563\n",
      "Epoch [14/25], Train Loss: 0.00014002001262269914, Validation Loss: 0.00014365405828963655\n",
      "Epoch [14/25], Train Loss: 0.00016570328443776816, Validation Loss: 0.00014364707700830574\n",
      "Epoch [14/25], Train Loss: 0.00010858281166292727, Validation Loss: 0.00014374739257618784\n",
      "Epoch [14/25], Train Loss: 0.00010894023580476642, Validation Loss: 0.00014375259173296702\n",
      "Epoch [14/25], Train Loss: 0.00016256592061836272, Validation Loss: 0.00014385180232541947\n",
      "Epoch [14/25], Train Loss: 0.00016283160948660225, Validation Loss: 0.00014410971028458637\n",
      "Epoch [14/25], Train Loss: 0.00012655863247346133, Validation Loss: 0.00014446698454169868\n",
      "Epoch [14/25], Train Loss: 0.00014453187759499997, Validation Loss: 0.00014514168215100652\n",
      "Epoch [14/25], Train Loss: 0.00011710502440109849, Validation Loss: 0.00014640181252616457\n",
      "Epoch [14/25], Train Loss: 0.00010169211600441486, Validation Loss: 0.00014796042790597616\n",
      "Epoch [14/25], Train Loss: 0.00015209267439786345, Validation Loss: 0.00015041398801258766\n",
      "Epoch [14/25], Train Loss: 0.0001501944352639839, Validation Loss: 0.0001522707881425352\n",
      "Epoch [14/25], Train Loss: 0.00013267554459162056, Validation Loss: 0.00015316317367251032\n",
      "Epoch [14/25], Train Loss: 0.00018919537251349539, Validation Loss: 0.0001504221664314779\n",
      "Epoch [14/25], Train Loss: 0.00020503028645180166, Validation Loss: 0.00014567074977094306\n",
      "Epoch [14/25], Train Loss: 0.0001411173288943246, Validation Loss: 0.0001442312493357652\n",
      "Epoch [14/25], Train Loss: 0.00010916914470726624, Validation Loss: 0.0001468498470785562\n",
      "Epoch [14/25], Train Loss: 0.00017037200450431556, Validation Loss: 0.00014755654216666396\n",
      "Epoch [14/25], Train Loss: 0.00019335158867761493, Validation Loss: 0.00014502154614698764\n",
      "Epoch [14/25], Train Loss: 0.00010888620454352349, Validation Loss: 0.0001438632963981945\n",
      "Epoch [14/25], Train Loss: 0.0001906071265693754, Validation Loss: 0.00014562054711859674\n",
      "Epoch [14/25], Train Loss: 0.00012613733997568488, Validation Loss: 0.00014727565333790457\n",
      "Epoch [14/25], Train Loss: 0.00013815001875627786, Validation Loss: 0.00014736023974061633\n",
      "Epoch [14/25], Train Loss: 0.0001440609630662948, Validation Loss: 0.0001451292606361676\n",
      "Epoch [14/25], Train Loss: 0.00013834833225701004, Validation Loss: 0.0001437941444843697\n",
      "Epoch [14/25], Train Loss: 0.00010941508662654087, Validation Loss: 0.00014413947113401567\n",
      "Epoch [14/25], Train Loss: 8.770314889261499e-05, Validation Loss: 0.0001459769729990512\n",
      "Epoch [15/25], Train Loss: 0.00013267097529023886, Validation Loss: 0.0001470043699858555\n",
      "Epoch [15/25], Train Loss: 0.00016991164011415094, Validation Loss: 0.00014561720066315806\n",
      "Epoch [15/25], Train Loss: 0.0001650547346798703, Validation Loss: 0.00014421037800881701\n",
      "Epoch [15/25], Train Loss: 0.0001291109947487712, Validation Loss: 0.00014382804535368148\n",
      "Epoch [15/25], Train Loss: 0.0001953758910531178, Validation Loss: 0.00014426792986341753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], Train Loss: 0.00013210417819209397, Validation Loss: 0.00014486969230347312\n",
      "Epoch [15/25], Train Loss: 0.0001737370912451297, Validation Loss: 0.00014454070745462862\n",
      "Epoch [15/25], Train Loss: 0.000253246515057981, Validation Loss: 0.0001440216469442627\n",
      "Epoch [15/25], Train Loss: 0.0001475902390666306, Validation Loss: 0.00014377013067132794\n",
      "Epoch [15/25], Train Loss: 0.00015345939027611166, Validation Loss: 0.00014385829175201555\n",
      "Epoch [15/25], Train Loss: 0.00021732358436565846, Validation Loss: 0.0001441978478396777\n",
      "Epoch [15/25], Train Loss: 9.426322503713891e-05, Validation Loss: 0.00014466323288312802\n",
      "Epoch [15/25], Train Loss: 0.0001575163914822042, Validation Loss: 0.00014496197957972374\n",
      "Epoch [15/25], Train Loss: 0.00016452826093882322, Validation Loss: 0.00014483445217289653\n",
      "Epoch [15/25], Train Loss: 0.00014227008796297014, Validation Loss: 0.0001447115913227511\n",
      "Epoch [15/25], Train Loss: 0.0001335109118372202, Validation Loss: 0.0001445338493795134\n",
      "Epoch [15/25], Train Loss: 0.0001463060762034729, Validation Loss: 0.00014439958007036088\n",
      "Epoch [15/25], Train Loss: 0.00012352134217508137, Validation Loss: 0.00014422717940760777\n",
      "Epoch [15/25], Train Loss: 0.0001585977297509089, Validation Loss: 0.00014400351986599465\n",
      "Epoch [15/25], Train Loss: 0.00016612109902780503, Validation Loss: 0.00014389558079225633\n",
      "Epoch [15/25], Train Loss: 0.00010689101327443495, Validation Loss: 0.0001438394019108576\n",
      "Epoch [15/25], Train Loss: 0.00015866794274188578, Validation Loss: 0.00014364531574149927\n",
      "Epoch [15/25], Train Loss: 0.000152888911543414, Validation Loss: 0.0001436127012614937\n",
      "Epoch [15/25], Train Loss: 9.441758447792381e-05, Validation Loss: 0.00014367652741687683\n",
      "Epoch [15/25], Train Loss: 0.0001788625813787803, Validation Loss: 0.0001437262411248715\n",
      "Epoch [15/25], Train Loss: 0.00012118784798076376, Validation Loss: 0.00014389402713277377\n",
      "Epoch [15/25], Train Loss: 0.00012798297393601388, Validation Loss: 0.00014394370494604422\n",
      "Epoch [15/25], Train Loss: 0.00011415977496653795, Validation Loss: 0.00014382140191931587\n",
      "Epoch [15/25], Train Loss: 0.00014317217573989183, Validation Loss: 0.00014376805265783332\n",
      "Epoch [15/25], Train Loss: 0.00014783420192543417, Validation Loss: 0.00014376625961934526\n",
      "Epoch [15/25], Train Loss: 0.0002072216011583805, Validation Loss: 0.0001437069729339176\n",
      "Epoch [15/25], Train Loss: 0.00025194123736582696, Validation Loss: 0.00014371812091364214\n",
      "Epoch [15/25], Train Loss: 0.00017200309957843274, Validation Loss: 0.00014371068609762006\n",
      "Epoch [15/25], Train Loss: 0.0002039462560787797, Validation Loss: 0.00014365904013781498\n",
      "Epoch [15/25], Train Loss: 0.00014780253695789725, Validation Loss: 0.00014361336288857274\n",
      "Epoch [15/25], Train Loss: 0.00015036958211567253, Validation Loss: 0.00014359972289336534\n",
      "Epoch [15/25], Train Loss: 0.00015879781858529896, Validation Loss: 0.00014358044742645386\n",
      "Epoch [15/25], Train Loss: 0.00019693555077537894, Validation Loss: 0.00014355482999235393\n",
      "Epoch [15/25], Train Loss: 0.0001395608123857528, Validation Loss: 0.0001435367118877669\n",
      "Epoch [15/25], Train Loss: 0.00016198644880205393, Validation Loss: 0.00014362412791039483\n",
      "Epoch [15/25], Train Loss: 0.00012458905985113233, Validation Loss: 0.0001436164782110912\n",
      "Epoch [15/25], Train Loss: 0.0001900802890304476, Validation Loss: 0.00014355450184666552\n",
      "Epoch [15/25], Train Loss: 0.00011621806334005669, Validation Loss: 0.00014362672275941197\n",
      "Epoch [15/25], Train Loss: 0.00014516364899463952, Validation Loss: 0.00014365372577837359\n",
      "Epoch [15/25], Train Loss: 0.00013023281644564122, Validation Loss: 0.00014365606257342732\n",
      "Epoch [15/25], Train Loss: 0.00015601649647578597, Validation Loss: 0.00014372813820955342\n",
      "Epoch [15/25], Train Loss: 0.00012145644723204896, Validation Loss: 0.0001438664204518621\n",
      "Epoch [15/25], Train Loss: 0.00011207850184291601, Validation Loss: 0.00014406123639976916\n",
      "Epoch [15/25], Train Loss: 0.00018282847304362804, Validation Loss: 0.00014431445257893452\n",
      "Epoch [15/25], Train Loss: 0.00016690432676114142, Validation Loss: 0.00014464438427239657\n",
      "Epoch [15/25], Train Loss: 0.0001675634557614103, Validation Loss: 0.00014505856403654132\n",
      "Epoch [15/25], Train Loss: 0.00014879918307997286, Validation Loss: 0.00014581479457168218\n",
      "Epoch [15/25], Train Loss: 0.00015670941502321512, Validation Loss: 0.00014659607889673984\n",
      "Epoch [15/25], Train Loss: 0.00014195918629411608, Validation Loss: 0.00014764477503680004\n",
      "Epoch [15/25], Train Loss: 0.00010889613622566685, Validation Loss: 0.00014807235517461475\n",
      "Epoch [15/25], Train Loss: 0.0001657441898714751, Validation Loss: 0.0001480936004857843\n",
      "Epoch [15/25], Train Loss: 0.0001090094810933806, Validation Loss: 0.00014651026819289352\n",
      "Epoch [15/25], Train Loss: 0.00013813354598823935, Validation Loss: 0.00014466504168619091\n",
      "Epoch [15/25], Train Loss: 0.00010378746083006263, Validation Loss: 0.00014369164176362877\n",
      "Epoch [15/25], Train Loss: 0.00013712517102248967, Validation Loss: 0.00014436848990347548\n",
      "Epoch [15/25], Train Loss: 0.00017260437016375363, Validation Loss: 0.00014534472599431563\n",
      "Epoch [15/25], Train Loss: 0.0001407866511726752, Validation Loss: 0.00014510840022315583\n",
      "Epoch [15/25], Train Loss: 0.00010707311594160274, Validation Loss: 0.00014416879954903076\n",
      "Epoch [15/25], Train Loss: 0.00013870552356820554, Validation Loss: 0.00014363597923268875\n",
      "Epoch [15/25], Train Loss: 0.00016153114847838879, Validation Loss: 0.00014394060999620706\n",
      "Epoch [15/25], Train Loss: 0.00012357627565506846, Validation Loss: 0.0001444966452254448\n",
      "Epoch [15/25], Train Loss: 0.00016366306226700544, Validation Loss: 0.00014454185026503789\n",
      "Epoch [15/25], Train Loss: 0.00018759522936306894, Validation Loss: 0.00014446435573821267\n",
      "Epoch [15/25], Train Loss: 0.00014017554349265993, Validation Loss: 0.00014406380723812616\n",
      "Epoch [15/25], Train Loss: 0.00017525264411233366, Validation Loss: 0.0001437493633905736\n",
      "Epoch [15/25], Train Loss: 0.0001286687038373202, Validation Loss: 0.000143601087135418\n",
      "Epoch [15/25], Train Loss: 0.00013869264512322843, Validation Loss: 0.00014359270232186343\n",
      "Epoch [15/25], Train Loss: 0.00019436694856267422, Validation Loss: 0.00014374967455902757\n",
      "Epoch [15/25], Train Loss: 0.00018125686619896442, Validation Loss: 0.00014392524729676855\n",
      "Epoch [15/25], Train Loss: 0.00020547887834254652, Validation Loss: 0.00014430686375514293\n",
      "Epoch [15/25], Train Loss: 0.00016876617155503482, Validation Loss: 0.00014450015078182332\n",
      "Epoch [15/25], Train Loss: 0.00015255740436259657, Validation Loss: 0.0001445323093018184\n",
      "Epoch [15/25], Train Loss: 0.00011445148993516341, Validation Loss: 0.00014441252666680764\n",
      "Epoch [15/25], Train Loss: 0.00016029058315325528, Validation Loss: 0.00014428706466181515\n",
      "Epoch [15/25], Train Loss: 0.00012627305113710463, Validation Loss: 0.00014409082408140724\n",
      "Epoch [15/25], Train Loss: 0.00017457387002650648, Validation Loss: 0.00014388925192179158\n",
      "Epoch [15/25], Train Loss: 0.00011807600094471127, Validation Loss: 0.0001437041627165551\n",
      "Epoch [15/25], Train Loss: 0.00015923811588436365, Validation Loss: 0.00014361661233124322\n",
      "Epoch [15/25], Train Loss: 0.0001775130076566711, Validation Loss: 0.00014360194375816112\n",
      "Epoch [15/25], Train Loss: 0.00017271928663831204, Validation Loss: 0.00014356276975983442\n",
      "Epoch [15/25], Train Loss: 0.00016302875883411616, Validation Loss: 0.0001435988728189841\n",
      "Epoch [15/25], Train Loss: 0.0001225262094521895, Validation Loss: 0.00014360493805725126\n",
      "Epoch [15/25], Train Loss: 0.00019622952095232904, Validation Loss: 0.0001436775894641566\n",
      "Epoch [15/25], Train Loss: 0.00013909714471083134, Validation Loss: 0.00014378197520272806\n",
      "Epoch [15/25], Train Loss: 0.00016398377192672342, Validation Loss: 0.00014370715580298566\n",
      "Epoch [15/25], Train Loss: 0.00014274759450927377, Validation Loss: 0.00014366889008670113\n",
      "Epoch [15/25], Train Loss: 0.0001520121586509049, Validation Loss: 0.00014372208970598878\n",
      "Epoch [15/25], Train Loss: 0.00011385064135538414, Validation Loss: 0.00014372657024068757\n",
      "Epoch [15/25], Train Loss: 0.00017596570251043886, Validation Loss: 0.00014374552362520868\n",
      "Epoch [15/25], Train Loss: 0.0001497523917350918, Validation Loss: 0.00014397388076758943\n",
      "Epoch [15/25], Train Loss: 0.00013159401714801788, Validation Loss: 0.00014435495419699387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], Train Loss: 0.00011792912846431136, Validation Loss: 0.00014482492697425186\n",
      "Epoch [15/25], Train Loss: 0.0001597574882907793, Validation Loss: 0.0001459628908681528\n",
      "Epoch [15/25], Train Loss: 0.0001531977322883904, Validation Loss: 0.00014745595884354165\n",
      "Epoch [15/25], Train Loss: 0.0002012234035646543, Validation Loss: 0.00014982413170703998\n",
      "Epoch [15/25], Train Loss: 0.00013950681022834033, Validation Loss: 0.0001511034851622147\n",
      "Epoch [15/25], Train Loss: 0.00013721580035053194, Validation Loss: 0.00015071077771911708\n",
      "Epoch [15/25], Train Loss: 0.00016677648818586022, Validation Loss: 0.00014720173324652328\n",
      "Epoch [15/25], Train Loss: 0.00010024952644016594, Validation Loss: 0.00014424519686144776\n",
      "Epoch [15/25], Train Loss: 0.0001601845578989014, Validation Loss: 0.00014457018939234938\n",
      "Epoch [15/25], Train Loss: 0.0001712135854177177, Validation Loss: 0.00014659503406922643\n",
      "Epoch [15/25], Train Loss: 0.00015668377454858273, Validation Loss: 0.00014698263839818537\n",
      "Epoch [15/25], Train Loss: 0.0001480018545407802, Validation Loss: 0.0001449229872378055\n",
      "Epoch [15/25], Train Loss: 0.00015825044829398394, Validation Loss: 0.00014371694099584904\n",
      "Epoch [15/25], Train Loss: 0.0001746411871863529, Validation Loss: 0.00014489915605129985\n",
      "Epoch [15/25], Train Loss: 0.00016148368013091385, Validation Loss: 0.0001461848102432365\n",
      "Epoch [15/25], Train Loss: 0.00015926524065434933, Validation Loss: 0.00014649785686439524\n",
      "Epoch [15/25], Train Loss: 0.0001013975270325318, Validation Loss: 0.00014531211248443773\n",
      "Epoch [15/25], Train Loss: 0.00011661088501568884, Validation Loss: 0.00014381764801025081\n",
      "Epoch [15/25], Train Loss: 0.00017797444888856262, Validation Loss: 0.00014424828065481659\n",
      "Epoch [15/25], Train Loss: 0.00019641387916635722, Validation Loss: 0.00014525778242386879\n",
      "Epoch [15/25], Train Loss: 0.0001647360622882843, Validation Loss: 0.00014618232647383895\n",
      "Epoch [15/25], Train Loss: 0.000102414378488902, Validation Loss: 0.00014602494484279306\n",
      "Epoch [15/25], Train Loss: 0.00019173169857822359, Validation Loss: 0.00014501125503253812\n",
      "Epoch [15/25], Train Loss: 0.0001768924412317574, Validation Loss: 0.00014374393576872536\n",
      "Epoch [15/25], Train Loss: 0.00013820121239405125, Validation Loss: 0.00014372701528676164\n",
      "Epoch [15/25], Train Loss: 0.00012762079131789505, Validation Loss: 0.00014450313901761546\n",
      "Epoch [15/25], Train Loss: 0.00013833484263159335, Validation Loss: 0.00014506451019163553\n",
      "Epoch [15/25], Train Loss: 0.00020820382633246481, Validation Loss: 0.00014500732334757533\n",
      "Epoch [15/25], Train Loss: 0.00012682403030339628, Validation Loss: 0.0001444132016331423\n",
      "Epoch [15/25], Train Loss: 0.00014253659173846245, Validation Loss: 0.00014383506738037491\n",
      "Epoch [15/25], Train Loss: 0.00015256955521181226, Validation Loss: 0.00014363252121256663\n",
      "Epoch [15/25], Train Loss: 0.00018045329488813877, Validation Loss: 0.00014392447143715497\n",
      "Epoch [15/25], Train Loss: 0.00016095531464088708, Validation Loss: 0.00014444890669741047\n",
      "Epoch [15/25], Train Loss: 9.488136129220948e-05, Validation Loss: 0.00014463424619558888\n",
      "Epoch [15/25], Train Loss: 0.00015347478620242327, Validation Loss: 0.00014448000874835997\n",
      "Epoch [15/25], Train Loss: 0.00018188924877904356, Validation Loss: 0.0001441352670857062\n",
      "Epoch [15/25], Train Loss: 0.0001716658443911001, Validation Loss: 0.00014379108494419295\n",
      "Epoch [15/25], Train Loss: 0.00018729516887106001, Validation Loss: 0.00014361558593615578\n",
      "Epoch [15/25], Train Loss: 0.00013067512190900743, Validation Loss: 0.00014357312005207253\n",
      "Epoch [15/25], Train Loss: 0.00016861721815075725, Validation Loss: 0.00014357463878695854\n",
      "Epoch [15/25], Train Loss: 0.00013847365335095674, Validation Loss: 0.00014371184200475304\n",
      "Epoch [15/25], Train Loss: 0.0001442412321921438, Validation Loss: 0.00014397088331558432\n",
      "Epoch [15/25], Train Loss: 0.00018201944476459175, Validation Loss: 0.00014421926462091506\n",
      "Epoch [15/25], Train Loss: 0.0001645545125938952, Validation Loss: 0.00014436402683107492\n",
      "Epoch [15/25], Train Loss: 0.00014381372602656484, Validation Loss: 0.00014449350516467044\n",
      "Epoch [15/25], Train Loss: 0.00014745857333764434, Validation Loss: 0.00014471640436871289\n",
      "Epoch [15/25], Train Loss: 0.00016883296484593302, Validation Loss: 0.00014484167574361587\n",
      "Epoch [15/25], Train Loss: 0.00022155887563712895, Validation Loss: 0.00014506022926070728\n",
      "Epoch [15/25], Train Loss: 0.00013615391799248755, Validation Loss: 0.0001448711630170389\n",
      "Epoch [15/25], Train Loss: 0.00017053339979611337, Validation Loss: 0.00014460957051293615\n",
      "Epoch [15/25], Train Loss: 0.000124167010653764, Validation Loss: 0.00014420461302506738\n",
      "Epoch [15/25], Train Loss: 0.00013540479994844645, Validation Loss: 0.00014378410026741524\n",
      "Epoch [15/25], Train Loss: 0.00014365720562636852, Validation Loss: 0.00014361614448716865\n",
      "Epoch [15/25], Train Loss: 0.00017660697631072253, Validation Loss: 0.00014368009869940578\n",
      "Epoch [15/25], Train Loss: 0.00013369569205679, Validation Loss: 0.00014378496198332868\n",
      "Epoch [15/25], Train Loss: 0.00018278339121025056, Validation Loss: 0.00014412317686947064\n",
      "Epoch [15/25], Train Loss: 0.00012042367598041892, Validation Loss: 0.00014426427830282288\n",
      "Epoch [15/25], Train Loss: 0.00016536783368792385, Validation Loss: 0.00014441076806785227\n",
      "Epoch [15/25], Train Loss: 0.00015126349171623588, Validation Loss: 0.00014448757210629992\n",
      "Epoch [15/25], Train Loss: 0.00014693493722006679, Validation Loss: 0.00014421311983217795\n",
      "Epoch [15/25], Train Loss: 0.0001922438241308555, Validation Loss: 0.00014397992102506882\n",
      "Epoch [15/25], Train Loss: 0.00018344257841818035, Validation Loss: 0.00014378344252084693\n",
      "Epoch [15/25], Train Loss: 0.00014019201626069844, Validation Loss: 0.00014361975190695374\n",
      "Epoch [15/25], Train Loss: 0.00014661671593785286, Validation Loss: 0.0001435937258065678\n",
      "Epoch [15/25], Train Loss: 0.00014906827709637582, Validation Loss: 0.00014356343332716886\n",
      "Epoch [15/25], Train Loss: 0.00017838188796304166, Validation Loss: 0.00014358573850283088\n",
      "Epoch [15/25], Train Loss: 0.00017963236314244568, Validation Loss: 0.00014373478212898288\n",
      "Epoch [15/25], Train Loss: 9.710153972264379e-05, Validation Loss: 0.00014364676583985178\n",
      "Epoch [15/25], Train Loss: 0.0001531032903585583, Validation Loss: 0.00014374268139363266\n",
      "Epoch [15/25], Train Loss: 0.00013109530846122652, Validation Loss: 0.00014397358803156142\n",
      "Epoch [15/25], Train Loss: 0.00014618579007219523, Validation Loss: 0.00014411169662101504\n",
      "Epoch [15/25], Train Loss: 0.00023084106214810163, Validation Loss: 0.00014422330520271013\n",
      "Epoch [15/25], Train Loss: 0.00016272709763143212, Validation Loss: 0.00014454043630394154\n",
      "Epoch [15/25], Train Loss: 0.0002099904086207971, Validation Loss: 0.0001448485539488805\n",
      "Epoch [15/25], Train Loss: 0.0001958016655407846, Validation Loss: 0.00014503490504769919\n",
      "Epoch [15/25], Train Loss: 0.00013217928062658757, Validation Loss: 0.00014529303201319028\n",
      "Epoch [15/25], Train Loss: 0.00012097340368200094, Validation Loss: 0.00014565123152957918\n",
      "Epoch [15/25], Train Loss: 0.00011062370322179049, Validation Loss: 0.00014575603733343694\n",
      "Epoch [15/25], Train Loss: 0.00016128452261909842, Validation Loss: 0.00014538211059213305\n",
      "Epoch [15/25], Train Loss: 0.00015284758410416543, Validation Loss: 0.00014490077786225204\n",
      "Epoch [15/25], Train Loss: 0.00014653107791673392, Validation Loss: 0.0001443438624846749\n",
      "Epoch [15/25], Train Loss: 0.00022502739739138633, Validation Loss: 0.00014399804470788998\n",
      "Epoch [15/25], Train Loss: 9.09885493456386e-05, Validation Loss: 0.000143651973970312\n",
      "Epoch [15/25], Train Loss: 0.00014379509957507253, Validation Loss: 0.00014360273902032836\n",
      "Epoch [15/25], Train Loss: 0.00012047433847328648, Validation Loss: 0.0001437651083203188\n",
      "Epoch [15/25], Train Loss: 0.00012354699720162898, Validation Loss: 0.00014388578177507345\n",
      "Epoch [15/25], Train Loss: 0.0001284819736611098, Validation Loss: 0.00014395178222912364\n",
      "Epoch [15/25], Train Loss: 0.0001364254712825641, Validation Loss: 0.0001439074153798477\n",
      "Epoch [15/25], Train Loss: 0.00019264793081674725, Validation Loss: 0.00014376457135464686\n",
      "Epoch [15/25], Train Loss: 0.00015379759133793414, Validation Loss: 0.00014368216870934701\n",
      "Epoch [15/25], Train Loss: 0.00014240786549635231, Validation Loss: 0.00014362452372248906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], Train Loss: 0.00014898204244673252, Validation Loss: 0.00014359740016516297\n",
      "Epoch [15/25], Train Loss: 0.0001981795358005911, Validation Loss: 0.0001436471002913701\n",
      "Epoch [15/25], Train Loss: 0.00014355932944454253, Validation Loss: 0.0001437240069208201\n",
      "Epoch [15/25], Train Loss: 0.00014557257236447185, Validation Loss: 0.0001438169407871707\n",
      "Epoch [15/25], Train Loss: 0.00015752902254462242, Validation Loss: 0.000143942177722541\n",
      "Epoch [15/25], Train Loss: 0.00020132158533670008, Validation Loss: 0.00014411589654628187\n",
      "Epoch [15/25], Train Loss: 0.00013155894703231752, Validation Loss: 0.00014408495868944253\n",
      "Epoch [15/25], Train Loss: 0.00014959866530261934, Validation Loss: 0.00014400511475590367\n",
      "Epoch [15/25], Train Loss: 0.00017508305609226227, Validation Loss: 0.0001440423356446748\n",
      "Epoch [15/25], Train Loss: 0.00013169573503546417, Validation Loss: 0.0001441180625988636\n",
      "Epoch [15/25], Train Loss: 0.00018444351735524833, Validation Loss: 0.00014419563788881836\n",
      "Epoch [15/25], Train Loss: 0.00021405972074717283, Validation Loss: 0.0001441873234095207\n",
      "Epoch [15/25], Train Loss: 0.00014283093332778662, Validation Loss: 0.00014449351219809615\n",
      "Epoch [15/25], Train Loss: 0.0001726222108118236, Validation Loss: 0.0001447718772396911\n",
      "Epoch [15/25], Train Loss: 0.00012687307025771588, Validation Loss: 0.00014517438515516308\n",
      "Epoch [15/25], Train Loss: 0.00012130192044423893, Validation Loss: 0.00014563718141289428\n",
      "Epoch [15/25], Train Loss: 0.00020567013416439295, Validation Loss: 0.00014599275988681862\n",
      "Epoch [15/25], Train Loss: 0.00015284163237083703, Validation Loss: 0.00014633066991033653\n",
      "Epoch [15/25], Train Loss: 0.0001533837930765003, Validation Loss: 0.00014665086467478735\n",
      "Epoch [15/25], Train Loss: 0.00014041543181519955, Validation Loss: 0.0001463705271210832\n",
      "Epoch [15/25], Train Loss: 0.00012812131899408996, Validation Loss: 0.0001458012055081781\n",
      "Epoch [15/25], Train Loss: 0.00015814437938388437, Validation Loss: 0.0001447115876847723\n",
      "Epoch [15/25], Train Loss: 0.00019110870198346674, Validation Loss: 0.0001437438380283614\n",
      "Epoch [15/25], Train Loss: 0.00010652135824784636, Validation Loss: 0.00014373395485260213\n",
      "Epoch [15/25], Train Loss: 0.00014524049765896052, Validation Loss: 0.00014446643884487761\n",
      "Epoch [15/25], Train Loss: 0.00016194194904528558, Validation Loss: 0.00014515943597264897\n",
      "Epoch [15/25], Train Loss: 0.00012064489419572055, Validation Loss: 0.00014526570181866799\n",
      "Epoch [15/25], Train Loss: 0.0001227085740538314, Validation Loss: 0.00014463758755785724\n",
      "Epoch [15/25], Train Loss: 0.00011772594007197767, Validation Loss: 0.00014389573722534504\n",
      "Epoch [15/25], Train Loss: 0.00020104915893170983, Validation Loss: 0.00014358379145657334\n",
      "Epoch [15/25], Train Loss: 0.00022663256095256656, Validation Loss: 0.00014379427278375564\n",
      "Epoch [15/25], Train Loss: 0.00014710755203850567, Validation Loss: 0.00014423921262884202\n",
      "Epoch [15/25], Train Loss: 0.00017825537361204624, Validation Loss: 0.00014464525908503372\n",
      "Epoch [15/25], Train Loss: 0.00014399261272046715, Validation Loss: 0.00014473005455026093\n",
      "Epoch [15/25], Train Loss: 8.570106001570821e-05, Validation Loss: 0.0001443911632425928\n",
      "Epoch [15/25], Train Loss: 0.00015081644232850522, Validation Loss: 0.0001440472874188951\n",
      "Epoch [15/25], Train Loss: 0.00022675778018310666, Validation Loss: 0.0001439231132584003\n",
      "Epoch [15/25], Train Loss: 0.0001515230833319947, Validation Loss: 0.00014383590872360703\n",
      "Epoch [15/25], Train Loss: 0.00013315949763637036, Validation Loss: 0.00014364314217042798\n",
      "Epoch [15/25], Train Loss: 0.00017647907952778041, Validation Loss: 0.00014350704101768013\n",
      "Epoch [15/25], Train Loss: 0.00016763544408604503, Validation Loss: 0.00014348914288954499\n",
      "Epoch [15/25], Train Loss: 0.00013746169861406088, Validation Loss: 0.000143549770291429\n",
      "Epoch [15/25], Train Loss: 0.00015498384891543537, Validation Loss: 0.00014366233517648652\n",
      "Epoch [15/25], Train Loss: 0.00011128265759907663, Validation Loss: 0.00014364290327648633\n",
      "Epoch [15/25], Train Loss: 0.00012096855789422989, Validation Loss: 0.0001437298613988484\n",
      "Epoch [15/25], Train Loss: 0.00015596073353663087, Validation Loss: 0.0001437969547017322\n",
      "Epoch [15/25], Train Loss: 0.00014277314767241478, Validation Loss: 0.0001438401067086185\n",
      "Epoch [15/25], Train Loss: 0.0001323506876360625, Validation Loss: 0.00014391850345418788\n",
      "Epoch [15/25], Train Loss: 0.00018845310842152685, Validation Loss: 0.00014408839876220252\n",
      "Epoch [15/25], Train Loss: 0.0001521691883681342, Validation Loss: 0.00014420551378862\n",
      "Epoch [15/25], Train Loss: 0.00021007133182138205, Validation Loss: 0.00014437546172606138\n",
      "Epoch [15/25], Train Loss: 0.00014010115410201252, Validation Loss: 0.00014476012584054842\n",
      "Epoch [15/25], Train Loss: 0.00016313967353198677, Validation Loss: 0.00014520337693587257\n",
      "Epoch [15/25], Train Loss: 0.00017220624431502074, Validation Loss: 0.00014562758418226924\n",
      "Epoch [15/25], Train Loss: 0.00016502704238519073, Validation Loss: 0.0001460098872485105\n",
      "Epoch [15/25], Train Loss: 0.00015173570136539638, Validation Loss: 0.0001466343453406201\n",
      "Epoch [15/25], Train Loss: 0.00013219128595665097, Validation Loss: 0.00014688989176647738\n",
      "Epoch [15/25], Train Loss: 0.00012850554776377976, Validation Loss: 0.00014681160246254876\n",
      "Epoch [15/25], Train Loss: 0.00016735905956011266, Validation Loss: 0.00014587926101133537\n",
      "Epoch [15/25], Train Loss: 0.00014576449757441878, Validation Loss: 0.00014459380884848845\n",
      "Epoch [15/25], Train Loss: 0.00015446469478774816, Validation Loss: 0.0001437048013031017\n",
      "Epoch [15/25], Train Loss: 0.00013439581380225718, Validation Loss: 0.00014401213896538442\n",
      "Epoch [15/25], Train Loss: 0.0001586749276611954, Validation Loss: 0.00014481732408360887\n",
      "Epoch [15/25], Train Loss: 0.00012765002611558884, Validation Loss: 0.00014499393461543756\n",
      "Epoch [15/25], Train Loss: 0.0001735924743115902, Validation Loss: 0.00014472471981813822\n",
      "Epoch [15/25], Train Loss: 0.00014581670984625816, Validation Loss: 0.0001441203984237897\n",
      "Epoch [15/25], Train Loss: 9.88439642242156e-05, Validation Loss: 0.00014358122619645048\n",
      "Epoch [15/25], Train Loss: 0.00022210732277017087, Validation Loss: 0.00014362730362336152\n",
      "Epoch [15/25], Train Loss: 0.0001354433043161407, Validation Loss: 0.0001440845507507523\n",
      "Epoch [15/25], Train Loss: 0.00021903266315348446, Validation Loss: 0.00014478553081668602\n",
      "Epoch [15/25], Train Loss: 0.00015907325723674148, Validation Loss: 0.00014520095307185937\n",
      "Epoch [15/25], Train Loss: 0.0001793424744391814, Validation Loss: 0.00014555232555721887\n",
      "Epoch [15/25], Train Loss: 0.00017552397912368178, Validation Loss: 0.00014545760301795478\n",
      "Epoch [15/25], Train Loss: 0.00018540388555265963, Validation Loss: 0.0001452957888735303\n",
      "Epoch [15/25], Train Loss: 0.00014710452524013817, Validation Loss: 0.00014475078011552494\n",
      "Epoch [15/25], Train Loss: 0.00018829967302735895, Validation Loss: 0.00014421131733494502\n",
      "Epoch [15/25], Train Loss: 0.0001616960798855871, Validation Loss: 0.0001437265217343035\n",
      "Epoch [15/25], Train Loss: 0.0001239526318386197, Validation Loss: 0.00014356318206409924\n",
      "Epoch [15/25], Train Loss: 0.0001397935120621696, Validation Loss: 0.0001436406130475613\n",
      "Epoch [15/25], Train Loss: 0.00016927081742323935, Validation Loss: 0.00014398669566920337\n",
      "Epoch [15/25], Train Loss: 0.0001314186811214313, Validation Loss: 0.0001443854756265258\n",
      "Epoch [16/25], Train Loss: 0.00014327064855024219, Validation Loss: 0.00014450826565735043\n",
      "Epoch [16/25], Train Loss: 0.0001614601642359048, Validation Loss: 0.00014456254696900336\n",
      "Epoch [16/25], Train Loss: 0.00012663344386965036, Validation Loss: 0.00014430370720219798\n",
      "Epoch [16/25], Train Loss: 0.00015949897351674736, Validation Loss: 0.00014428486319957302\n",
      "Epoch [16/25], Train Loss: 0.0001672103098826483, Validation Loss: 0.00014401430889847687\n",
      "Epoch [16/25], Train Loss: 0.00015299591177608818, Validation Loss: 0.00014369344668618093\n",
      "Epoch [16/25], Train Loss: 0.00016561751544941217, Validation Loss: 0.00014374065673716056\n",
      "Epoch [16/25], Train Loss: 0.00011337827163515612, Validation Loss: 0.00014350802666740492\n",
      "Epoch [16/25], Train Loss: 0.00013079198834020644, Validation Loss: 0.00014384044334292412\n",
      "Epoch [16/25], Train Loss: 0.00023825981770642102, Validation Loss: 0.00014393015323245587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], Train Loss: 0.00015522488683927804, Validation Loss: 0.00014394588385281774\n",
      "Epoch [16/25], Train Loss: 0.00012837313988711685, Validation Loss: 0.00014419435829040594\n",
      "Epoch [16/25], Train Loss: 0.00011272253323113546, Validation Loss: 0.0001440575123221303\n",
      "Epoch [16/25], Train Loss: 0.00012105663336114958, Validation Loss: 0.0001441268927010242\n",
      "Epoch [16/25], Train Loss: 0.00012711307499557734, Validation Loss: 0.00014413262033485807\n",
      "Epoch [16/25], Train Loss: 0.00014412819291464984, Validation Loss: 0.0001439978009633099\n",
      "Epoch [16/25], Train Loss: 0.000166357567650266, Validation Loss: 0.000143979316635523\n",
      "Epoch [16/25], Train Loss: 9.153733117273077e-05, Validation Loss: 0.00014387142170259418\n",
      "Epoch [16/25], Train Loss: 0.0001832552661653608, Validation Loss: 0.00014376089990643474\n",
      "Epoch [16/25], Train Loss: 9.296854113927111e-05, Validation Loss: 0.00014369643686222843\n",
      "Epoch [16/25], Train Loss: 0.000157272006617859, Validation Loss: 0.00014361236559731576\n",
      "Epoch [16/25], Train Loss: 0.00011587391054490581, Validation Loss: 0.0001434985063193987\n",
      "Epoch [16/25], Train Loss: 0.0001617570233065635, Validation Loss: 0.00014351933326300543\n",
      "Epoch [16/25], Train Loss: 0.0002457356022205204, Validation Loss: 0.00014357256101599585\n",
      "Epoch [16/25], Train Loss: 0.00014817465853411704, Validation Loss: 0.00014357672625919804\n",
      "Epoch [16/25], Train Loss: 0.00010931836004601792, Validation Loss: 0.00014362585376754094\n",
      "Epoch [16/25], Train Loss: 0.00016553024761378765, Validation Loss: 0.00014359736766588564\n",
      "Epoch [16/25], Train Loss: 0.00013950312859378755, Validation Loss: 0.000143584439016801\n",
      "Epoch [16/25], Train Loss: 0.00015601619088556617, Validation Loss: 0.00014366787703086933\n",
      "Epoch [16/25], Train Loss: 0.00014137460675556213, Validation Loss: 0.00014379317484175164\n",
      "Epoch [16/25], Train Loss: 0.0001984972186619416, Validation Loss: 0.00014407945539763508\n",
      "Epoch [16/25], Train Loss: 0.00016990398580674082, Validation Loss: 0.0001445364660564034\n",
      "Epoch [16/25], Train Loss: 0.00018432733486406505, Validation Loss: 0.00014516230803565122\n",
      "Epoch [16/25], Train Loss: 0.00014594962703995407, Validation Loss: 0.00014629403934425986\n",
      "Epoch [16/25], Train Loss: 0.00014338067558128387, Validation Loss: 0.0001478121914260555\n",
      "Epoch [16/25], Train Loss: 0.000120739103294909, Validation Loss: 0.0001504619220213499\n",
      "Epoch [16/25], Train Loss: 0.00010231928172288463, Validation Loss: 0.00015205113231786527\n",
      "Epoch [16/25], Train Loss: 0.00022265399456955492, Validation Loss: 0.0001523352094712512\n",
      "Epoch [16/25], Train Loss: 0.00014307809760794044, Validation Loss: 0.0001478916793227351\n",
      "Epoch [16/25], Train Loss: 0.00013283497537486255, Validation Loss: 0.0001443872160355871\n",
      "Epoch [16/25], Train Loss: 0.0001500053913332522, Validation Loss: 0.000146740269944227\n",
      "Epoch [16/25], Train Loss: 0.00012414144293870777, Validation Loss: 0.00014862613825243897\n",
      "Epoch [16/25], Train Loss: 0.0001713692327030003, Validation Loss: 0.00014618817537363307\n",
      "Epoch [16/25], Train Loss: 0.00015824634465388954, Validation Loss: 0.00014395721033603573\n",
      "Epoch [16/25], Train Loss: 0.00015654762682970613, Validation Loss: 0.00014510583472050105\n",
      "Epoch [16/25], Train Loss: 0.00013839194434694946, Validation Loss: 0.00014717567658711534\n",
      "Epoch [16/25], Train Loss: 8.502407581545413e-05, Validation Loss: 0.00014655510773688244\n",
      "Epoch [16/25], Train Loss: 0.00011934490612475201, Validation Loss: 0.00014479363599093631\n",
      "Epoch [16/25], Train Loss: 0.00019203261763323098, Validation Loss: 0.00014386786060640588\n",
      "Epoch [16/25], Train Loss: 0.00018446090689394623, Validation Loss: 0.00014444616996721986\n",
      "Epoch [16/25], Train Loss: 0.00021097241551615298, Validation Loss: 0.0001468236708509115\n",
      "Epoch [16/25], Train Loss: 0.00014622212620452046, Validation Loss: 0.0001482445101525324\n",
      "Epoch [16/25], Train Loss: 0.0001583415869390592, Validation Loss: 0.00014908272520794223\n",
      "Epoch [16/25], Train Loss: 0.00020539353135973215, Validation Loss: 0.00014658635773230345\n",
      "Epoch [16/25], Train Loss: 0.0001594351342646405, Validation Loss: 0.00014404399456301084\n",
      "Epoch [16/25], Train Loss: 0.00017620250582695007, Validation Loss: 0.00014392368951424335\n",
      "Epoch [16/25], Train Loss: 0.00015405856538563967, Validation Loss: 0.0001457957691551807\n",
      "Epoch [16/25], Train Loss: 0.0001488442358095199, Validation Loss: 0.0001466830666079962\n",
      "Epoch [16/25], Train Loss: 0.00015511798847001046, Validation Loss: 0.0001454435060926092\n",
      "Epoch [16/25], Train Loss: 0.00015182566130533814, Validation Loss: 0.00014388976899984602\n",
      "Epoch [16/25], Train Loss: 0.00015736615750938654, Validation Loss: 0.0001438803187435648\n",
      "Epoch [16/25], Train Loss: 0.00012097387661924586, Validation Loss: 0.000145008422259707\n",
      "Epoch [16/25], Train Loss: 9.396210225531831e-05, Validation Loss: 0.0001455946219114897\n",
      "Epoch [16/25], Train Loss: 0.0001806538930395618, Validation Loss: 0.00014479817012518954\n",
      "Epoch [16/25], Train Loss: 0.00020922347903251648, Validation Loss: 0.0001439178093278315\n",
      "Epoch [16/25], Train Loss: 0.00018090767844114453, Validation Loss: 0.0001436336472882734\n",
      "Epoch [16/25], Train Loss: 0.000175915498402901, Validation Loss: 0.00014413525107859944\n",
      "Epoch [16/25], Train Loss: 0.00019049952970817685, Validation Loss: 0.00014513142765887704\n",
      "Epoch [16/25], Train Loss: 0.00016219107783399522, Validation Loss: 0.00014559967700430814\n",
      "Epoch [16/25], Train Loss: 0.00015893872478045523, Validation Loss: 0.0001459127716467871\n",
      "Epoch [16/25], Train Loss: 0.00015985939535312355, Validation Loss: 0.00014542596885197174\n",
      "Epoch [16/25], Train Loss: 0.00017600160208530724, Validation Loss: 0.00014478480491864805\n",
      "Epoch [16/25], Train Loss: 0.00014658474538009614, Validation Loss: 0.0001439781405982406\n",
      "Epoch [16/25], Train Loss: 0.00011489446478663012, Validation Loss: 0.0001435879610653501\n",
      "Epoch [16/25], Train Loss: 0.00015618796169292182, Validation Loss: 0.00014393139305563334\n",
      "Epoch [16/25], Train Loss: 0.00014339371409732848, Validation Loss: 0.00014444159363241244\n",
      "Epoch [16/25], Train Loss: 0.00013510607823263854, Validation Loss: 0.00014452089186913023\n",
      "Epoch [16/25], Train Loss: 0.00021144455240573734, Validation Loss: 0.0001442774970200844\n",
      "Epoch [16/25], Train Loss: 0.00015642355720046908, Validation Loss: 0.0001438654260709882\n",
      "Epoch [16/25], Train Loss: 0.00014849247236270458, Validation Loss: 0.00014358986033281932\n",
      "Epoch [16/25], Train Loss: 0.00013388952356763184, Validation Loss: 0.00014363463584838126\n",
      "Epoch [16/25], Train Loss: 0.00012886483455076814, Validation Loss: 0.00014392847297131083\n",
      "Epoch [16/25], Train Loss: 0.00014146568719297647, Validation Loss: 0.00014413629930155972\n",
      "Epoch [16/25], Train Loss: 0.0001345484342891723, Validation Loss: 0.00014411798207826602\n",
      "Epoch [16/25], Train Loss: 0.00013718032278120518, Validation Loss: 0.00014397042056468005\n",
      "Epoch [16/25], Train Loss: 0.00016894267173483968, Validation Loss: 0.00014386092904411878\n",
      "Epoch [16/25], Train Loss: 0.00012884281750302762, Validation Loss: 0.0001437046426872257\n",
      "Epoch [16/25], Train Loss: 0.00016335984400939196, Validation Loss: 0.0001435492820746731\n",
      "Epoch [16/25], Train Loss: 0.00020572722132783383, Validation Loss: 0.00014360127048954988\n",
      "Epoch [16/25], Train Loss: 0.00018714855832513422, Validation Loss: 0.00014363910304382443\n",
      "Epoch [16/25], Train Loss: 0.00017739237227942795, Validation Loss: 0.00014362747242557816\n",
      "Epoch [16/25], Train Loss: 0.00011907445878023282, Validation Loss: 0.00014359537793401007\n",
      "Epoch [16/25], Train Loss: 0.00015681705554015934, Validation Loss: 0.0001436110333694766\n",
      "Epoch [16/25], Train Loss: 0.00018770807946566492, Validation Loss: 0.000143619034497533\n",
      "Epoch [16/25], Train Loss: 0.00013842244516126812, Validation Loss: 0.00014360176307188038\n",
      "Epoch [16/25], Train Loss: 0.0001649199111852795, Validation Loss: 0.00014355010231762813\n",
      "Epoch [16/25], Train Loss: 0.00013237811799626797, Validation Loss: 0.00014350965551178282\n",
      "Epoch [16/25], Train Loss: 0.00017747035599313676, Validation Loss: 0.00014350421333801932\n",
      "Epoch [16/25], Train Loss: 0.0001319179282290861, Validation Loss: 0.00014357156833284536\n",
      "Epoch [16/25], Train Loss: 0.00018101386376656592, Validation Loss: 0.00014362919925285192\n",
      "Epoch [16/25], Train Loss: 0.00016879680333659053, Validation Loss: 0.00014358099579112605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], Train Loss: 9.732653415994719e-05, Validation Loss: 0.0001435768193914555\n",
      "Epoch [16/25], Train Loss: 0.00015264746616594493, Validation Loss: 0.00014360415710446735\n",
      "Epoch [16/25], Train Loss: 0.00015242195513565093, Validation Loss: 0.0001435383678957199\n",
      "Epoch [16/25], Train Loss: 0.00020605532336048782, Validation Loss: 0.00014349514361432133\n",
      "Epoch [16/25], Train Loss: 0.00011205570626771078, Validation Loss: 0.0001435148427844979\n",
      "Epoch [16/25], Train Loss: 0.00012109969247831032, Validation Loss: 0.0001435479149222374\n",
      "Epoch [16/25], Train Loss: 0.00020331921405158937, Validation Loss: 0.00014351324862218461\n",
      "Epoch [16/25], Train Loss: 0.00013600535748992115, Validation Loss: 0.00014348234568994182\n",
      "Epoch [16/25], Train Loss: 0.00011272868869127706, Validation Loss: 0.00014351629627829728\n",
      "Epoch [16/25], Train Loss: 0.0001894613087642938, Validation Loss: 0.0001436249614926055\n",
      "Epoch [16/25], Train Loss: 0.0001421801425749436, Validation Loss: 0.00014353698934428395\n",
      "Epoch [16/25], Train Loss: 0.00013190109166316688, Validation Loss: 0.00014346568592979263\n",
      "Epoch [16/25], Train Loss: 0.0001980039814952761, Validation Loss: 0.000143517712179649\n",
      "Epoch [16/25], Train Loss: 0.0001313578977715224, Validation Loss: 0.00014349243598796118\n",
      "Epoch [16/25], Train Loss: 0.0001294888206757605, Validation Loss: 0.0001435224410670344\n",
      "Epoch [16/25], Train Loss: 0.00013887426757719368, Validation Loss: 0.0001434861956416474\n",
      "Epoch [16/25], Train Loss: 0.0001091437297873199, Validation Loss: 0.00014344542845113513\n",
      "Epoch [16/25], Train Loss: 0.00015803641872480512, Validation Loss: 0.00014345747670934845\n",
      "Epoch [16/25], Train Loss: 0.00018003242439590394, Validation Loss: 0.00014350282484277462\n",
      "Epoch [16/25], Train Loss: 0.00016672993660904467, Validation Loss: 0.00014350631972774862\n",
      "Epoch [16/25], Train Loss: 0.00016159334336407483, Validation Loss: 0.00014345817883925822\n",
      "Epoch [16/25], Train Loss: 0.00013277695688884705, Validation Loss: 0.00014345511687376226\n",
      "Epoch [16/25], Train Loss: 0.00015523504407610744, Validation Loss: 0.00014350667916005478\n",
      "Epoch [16/25], Train Loss: 0.00021118062431924045, Validation Loss: 0.0001435145289481928\n",
      "Epoch [16/25], Train Loss: 0.0001628658064873889, Validation Loss: 0.00014363285251117\n",
      "Epoch [16/25], Train Loss: 0.00012020616122754291, Validation Loss: 0.0001436176154432663\n",
      "Epoch [16/25], Train Loss: 0.00012951223470736295, Validation Loss: 0.0001436778111383319\n",
      "Epoch [16/25], Train Loss: 0.00015736243221908808, Validation Loss: 0.00014371341094374657\n",
      "Epoch [16/25], Train Loss: 0.0002136844996130094, Validation Loss: 0.00014382336545774402\n",
      "Epoch [16/25], Train Loss: 0.0001485220855101943, Validation Loss: 0.0001442001603815394\n",
      "Epoch [16/25], Train Loss: 0.0001503314997535199, Validation Loss: 0.00014439106550222884\n",
      "Epoch [16/25], Train Loss: 0.0001285398320760578, Validation Loss: 0.00014482184924418106\n",
      "Epoch [16/25], Train Loss: 0.0001559245865792036, Validation Loss: 0.00014563064614776523\n",
      "Epoch [16/25], Train Loss: 0.00011878362420247868, Validation Loss: 0.00014628568630238684\n",
      "Epoch [16/25], Train Loss: 0.0001707593910396099, Validation Loss: 0.0001472324659213579\n",
      "Epoch [16/25], Train Loss: 0.0001613780768821016, Validation Loss: 0.00014718494567205197\n",
      "Epoch [16/25], Train Loss: 7.965299300849438e-05, Validation Loss: 0.00014701578353803295\n",
      "Epoch [16/25], Train Loss: 0.00014268442464526743, Validation Loss: 0.00014578281795062746\n",
      "Epoch [16/25], Train Loss: 0.00019559355860110372, Validation Loss: 0.00014422668512755385\n",
      "Epoch [16/25], Train Loss: 0.00015621798229403794, Validation Loss: 0.00014369489775466112\n",
      "Epoch [16/25], Train Loss: 0.00015173341671470553, Validation Loss: 0.00014417248918713693\n",
      "Epoch [16/25], Train Loss: 0.00018706711125560105, Validation Loss: 0.00014502841877401807\n",
      "Epoch [16/25], Train Loss: 0.0001663542352616787, Validation Loss: 0.00014534760994138195\n",
      "Epoch [16/25], Train Loss: 0.00015541007451247424, Validation Loss: 0.00014503660446886594\n",
      "Epoch [16/25], Train Loss: 0.00012830671039409935, Validation Loss: 0.0001442901882304189\n",
      "Epoch [16/25], Train Loss: 0.00016412393597420305, Validation Loss: 0.0001436917298027159\n",
      "Epoch [16/25], Train Loss: 0.0002830934536177665, Validation Loss: 0.00014362223834420244\n",
      "Epoch [16/25], Train Loss: 0.00018948664364870638, Validation Loss: 0.00014395788093679585\n",
      "Epoch [16/25], Train Loss: 0.00019087166583631188, Validation Loss: 0.00014452929608523845\n",
      "Epoch [16/25], Train Loss: 0.0001650147169129923, Validation Loss: 0.00014508024687529542\n",
      "Epoch [16/25], Train Loss: 0.00020096724620088935, Validation Loss: 0.00014600904808806565\n",
      "Epoch [16/25], Train Loss: 0.0002170828520320356, Validation Loss: 0.00014626631261004756\n",
      "Epoch [16/25], Train Loss: 0.0001551380119053647, Validation Loss: 0.00014627062967823197\n",
      "Epoch [16/25], Train Loss: 0.0001068507699528709, Validation Loss: 0.0001455490504061648\n",
      "Epoch [16/25], Train Loss: 0.00012662462540902197, Validation Loss: 0.00014465873828157784\n",
      "Epoch [16/25], Train Loss: 0.00014705819194205105, Validation Loss: 0.00014389432423437634\n",
      "Epoch [16/25], Train Loss: 9.864132880466059e-05, Validation Loss: 0.00014354631033105156\n",
      "Epoch [16/25], Train Loss: 0.00016515982861164957, Validation Loss: 0.00014369742857525125\n",
      "Epoch [16/25], Train Loss: 0.0001665236777625978, Validation Loss: 0.00014413811174260143\n",
      "Epoch [16/25], Train Loss: 0.00014629003999289125, Validation Loss: 0.0001442821761884261\n",
      "Epoch [16/25], Train Loss: 0.00018622608331497759, Validation Loss: 0.0001442039627969886\n",
      "Epoch [16/25], Train Loss: 0.00012586606317199767, Validation Loss: 0.00014403116729226894\n",
      "Epoch [16/25], Train Loss: 0.00012331630568951368, Validation Loss: 0.00014366752075147815\n",
      "Epoch [16/25], Train Loss: 0.00016679296095389873, Validation Loss: 0.0001435502403182909\n",
      "Epoch [16/25], Train Loss: 0.0001393249985994771, Validation Loss: 0.0001435758565397312\n",
      "Epoch [16/25], Train Loss: 0.0002151103544747457, Validation Loss: 0.00014359241516406958\n",
      "Epoch [16/25], Train Loss: 0.00011398759670555592, Validation Loss: 0.00014377980235925255\n",
      "Epoch [16/25], Train Loss: 0.00012853284715674818, Validation Loss: 0.00014389392890734598\n",
      "Epoch [16/25], Train Loss: 0.00012426507601048797, Validation Loss: 0.0001438745469689214\n",
      "Epoch [16/25], Train Loss: 0.00011548995826160535, Validation Loss: 0.00014409290597541257\n",
      "Epoch [16/25], Train Loss: 0.00015661134966649115, Validation Loss: 0.00014407936857120754\n",
      "Epoch [16/25], Train Loss: 0.00017230941739398986, Validation Loss: 0.00014424750406760722\n",
      "Epoch [16/25], Train Loss: 0.00023481142125092447, Validation Loss: 0.00014447359911476572\n",
      "Epoch [16/25], Train Loss: 0.00017276332073379308, Validation Loss: 0.00014451384728696818\n",
      "Epoch [16/25], Train Loss: 8.278524910565466e-05, Validation Loss: 0.00014452117781426447\n",
      "Epoch [16/25], Train Loss: 0.00017586519243195653, Validation Loss: 0.00014434791834598096\n",
      "Epoch [16/25], Train Loss: 0.00015516082930844277, Validation Loss: 0.00014411321778122024\n",
      "Epoch [16/25], Train Loss: 0.00011326933599775657, Validation Loss: 0.0001439828588142215\n",
      "Epoch [16/25], Train Loss: 0.0002102065773215145, Validation Loss: 0.00014356180472532287\n",
      "Epoch [16/25], Train Loss: 0.0001505481923231855, Validation Loss: 0.00014361523911550952\n",
      "Epoch [16/25], Train Loss: 0.00015457358676940203, Validation Loss: 0.00014364340798541284\n",
      "Epoch [16/25], Train Loss: 0.0001449886622140184, Validation Loss: 0.00014371599657655074\n",
      "Epoch [16/25], Train Loss: 0.0001805260544642806, Validation Loss: 0.00014398627802923633\n",
      "Epoch [16/25], Train Loss: 0.00014908851881045848, Validation Loss: 0.0001440042227235002\n",
      "Epoch [16/25], Train Loss: 0.00017316230514552444, Validation Loss: 0.00014431753976775022\n",
      "Epoch [16/25], Train Loss: 0.00012395984958857298, Validation Loss: 0.0001444719380136424\n",
      "Epoch [16/25], Train Loss: 0.00013360117736738175, Validation Loss: 0.00014466822249232792\n",
      "Epoch [16/25], Train Loss: 0.00016807207430247217, Validation Loss: 0.00014525839069392532\n",
      "Epoch [16/25], Train Loss: 0.00015723201795481145, Validation Loss: 0.00014526034550120434\n",
      "Epoch [16/25], Train Loss: 0.00020347906684037298, Validation Loss: 0.00014571450374205596\n",
      "Epoch [16/25], Train Loss: 0.00016296582180075347, Validation Loss: 0.00014595926622860134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], Train Loss: 0.00011955187801504508, Validation Loss: 0.0001461017881714118\n",
      "Epoch [16/25], Train Loss: 0.00014863797696307302, Validation Loss: 0.00014582024305127562\n",
      "Epoch [16/25], Train Loss: 0.00019488594261929393, Validation Loss: 0.00014499947307437347\n",
      "Epoch [16/25], Train Loss: 0.0002130233042407781, Validation Loss: 0.00014397704411142815\n",
      "Epoch [16/25], Train Loss: 0.00016355514526367188, Validation Loss: 0.00014365512712781006\n",
      "Epoch [16/25], Train Loss: 0.00014829881547484547, Validation Loss: 0.00014406746364935922\n",
      "Epoch [16/25], Train Loss: 0.00013868097448721528, Validation Loss: 0.0001447508308046963\n",
      "Epoch [16/25], Train Loss: 0.00015367154264822602, Validation Loss: 0.00014523413992719724\n",
      "Epoch [16/25], Train Loss: 0.00013766229676548392, Validation Loss: 0.0001449756001723775\n",
      "Epoch [16/25], Train Loss: 0.00015246255497913808, Validation Loss: 0.0001445038656432492\n",
      "Epoch [16/25], Train Loss: 0.00015791854821145535, Validation Loss: 0.00014395440909235427\n",
      "Epoch [16/25], Train Loss: 0.0001388971140841022, Validation Loss: 0.00014366218238137662\n",
      "Epoch [16/25], Train Loss: 0.00012777929077856243, Validation Loss: 0.0001436987004126422\n",
      "Epoch [16/25], Train Loss: 0.00015976358554325998, Validation Loss: 0.00014386683057333963\n",
      "Epoch [16/25], Train Loss: 0.0002267168601974845, Validation Loss: 0.00014422716243037332\n",
      "Epoch [16/25], Train Loss: 0.00012028881610604003, Validation Loss: 0.00014466199160475905\n",
      "Epoch [16/25], Train Loss: 0.00014109211042523384, Validation Loss: 0.00014513105634250678\n",
      "Epoch [16/25], Train Loss: 0.00012001804861938581, Validation Loss: 0.00014546455592305088\n",
      "Epoch [16/25], Train Loss: 0.00012387626338750124, Validation Loss: 0.00014562558887215953\n",
      "Epoch [16/25], Train Loss: 0.000114433016278781, Validation Loss: 0.0001454959235464533\n",
      "Epoch [16/25], Train Loss: 0.00017641752492636442, Validation Loss: 0.0001451670902800591\n",
      "Epoch [16/25], Train Loss: 9.204164962284267e-05, Validation Loss: 0.00014447460756249105\n",
      "Epoch [16/25], Train Loss: 0.00013264727022033185, Validation Loss: 0.00014389262166029462\n",
      "Epoch [16/25], Train Loss: 0.00013122761447448283, Validation Loss: 0.00014360771335001724\n",
      "Epoch [16/25], Train Loss: 0.00012418594269547611, Validation Loss: 0.00014360129231742272\n",
      "Epoch [16/25], Train Loss: 0.0001484339009039104, Validation Loss: 0.00014389975937471414\n",
      "Epoch [16/25], Train Loss: 0.00013949953427072614, Validation Loss: 0.00014423571774386802\n",
      "Epoch [16/25], Train Loss: 0.00012220695498399436, Validation Loss: 0.0001443791792553384\n",
      "Epoch [16/25], Train Loss: 0.00013671191118191928, Validation Loss: 0.00014423910373200972\n",
      "Epoch [16/25], Train Loss: 0.0001382434566039592, Validation Loss: 0.00014405122856260278\n",
      "Epoch [16/25], Train Loss: 0.00013731286162510514, Validation Loss: 0.00014374494057847186\n",
      "Epoch [16/25], Train Loss: 0.00016470068658236414, Validation Loss: 0.00014355508828884923\n",
      "Epoch [16/25], Train Loss: 0.00013941249926574528, Validation Loss: 0.00014348480641880693\n",
      "Epoch [16/25], Train Loss: 0.0001396930165356025, Validation Loss: 0.00014356587101550152\n",
      "Epoch [16/25], Train Loss: 0.00021139536693226546, Validation Loss: 0.0001437678380170837\n",
      "Epoch [16/25], Train Loss: 0.00021120805467944592, Validation Loss: 0.00014387944150560846\n",
      "Epoch [16/25], Train Loss: 0.00014466769061982632, Validation Loss: 0.00014410161893465556\n",
      "Epoch [16/25], Train Loss: 0.00018309563165530562, Validation Loss: 0.00014432314904600692\n",
      "Epoch [16/25], Train Loss: 0.0001414341968484223, Validation Loss: 0.00014480805499867227\n",
      "Epoch [16/25], Train Loss: 0.00014646077761426568, Validation Loss: 0.0001453064828334997\n",
      "Epoch [16/25], Train Loss: 0.00013026069791521877, Validation Loss: 0.00014605671773703459\n",
      "Epoch [16/25], Train Loss: 0.00020190465147607028, Validation Loss: 0.00014677184663014486\n",
      "Epoch [16/25], Train Loss: 0.00013134386972524226, Validation Loss: 0.0001475532813249932\n",
      "Epoch [16/25], Train Loss: 0.0001327924255747348, Validation Loss: 0.0001472658043591461\n",
      "Epoch [16/25], Train Loss: 0.0001898500049719587, Validation Loss: 0.0001463046947416539\n",
      "Epoch [16/25], Train Loss: 0.0001377621229039505, Validation Loss: 0.00014454318976883467\n",
      "Epoch [16/25], Train Loss: 0.00024783509434200823, Validation Loss: 0.00014359913451092627\n",
      "Epoch [16/25], Train Loss: 0.0002015131467487663, Validation Loss: 0.00014403980312636123\n",
      "Epoch [16/25], Train Loss: 0.00018098649161402136, Validation Loss: 0.0001448648370569572\n",
      "Epoch [16/25], Train Loss: 0.0001261216966668144, Validation Loss: 0.00014522495548590085\n",
      "Epoch [16/25], Train Loss: 0.00014820411161053926, Validation Loss: 0.00014470434495403122\n",
      "Epoch [16/25], Train Loss: 0.00011969944171141833, Validation Loss: 0.00014389888795752389\n",
      "Epoch [16/25], Train Loss: 0.00011033850751118734, Validation Loss: 0.0001435073427273892\n",
      "Epoch [16/25], Train Loss: 0.00011245517089264467, Validation Loss: 0.00014364309899974614\n",
      "Epoch [16/25], Train Loss: 0.000156612804858014, Validation Loss: 0.0001440927379007917\n",
      "Epoch [16/25], Train Loss: 0.00017780718917492777, Validation Loss: 0.0001446690065980268\n",
      "Epoch [16/25], Train Loss: 0.00018091170932166278, Validation Loss: 0.00014542281666460137\n",
      "Epoch [16/25], Train Loss: 0.00013856487930752337, Validation Loss: 0.000145803258298353\n",
      "Epoch [16/25], Train Loss: 0.00017585464229341596, Validation Loss: 0.00014645751822778646\n",
      "Epoch [16/25], Train Loss: 0.00016140015213750303, Validation Loss: 0.00014621563192728597\n",
      "Epoch [16/25], Train Loss: 0.00011581886792555451, Validation Loss: 0.00014537517927237786\n",
      "Epoch [16/25], Train Loss: 0.00014026148710399866, Validation Loss: 0.0001442615466658026\n",
      "Epoch [16/25], Train Loss: 0.00018295810150448233, Validation Loss: 0.00014354225083176668\n",
      "Epoch [16/25], Train Loss: 0.00017845426918938756, Validation Loss: 0.0001435648024198599\n",
      "Epoch [16/25], Train Loss: 0.00017741150804795325, Validation Loss: 0.00014413830746586124\n",
      "Epoch [16/25], Train Loss: 0.00016236495866905898, Validation Loss: 0.00014462660910794512\n",
      "Epoch [16/25], Train Loss: 0.0001376150466967374, Validation Loss: 0.00014464839526529734\n",
      "Epoch [16/25], Train Loss: 0.000154481953359209, Validation Loss: 0.0001442904938206387\n",
      "Epoch [16/25], Train Loss: 0.00017590346396900713, Validation Loss: 0.00014379782927183746\n",
      "Epoch [16/25], Train Loss: 0.0001548027212265879, Validation Loss: 0.0001434654761396814\n",
      "Epoch [16/25], Train Loss: 0.00012556463479995728, Validation Loss: 0.0001434356323443353\n",
      "Epoch [16/25], Train Loss: 0.00015166428056545556, Validation Loss: 0.00014364025021980828\n",
      "Epoch [16/25], Train Loss: 0.00019293086370453238, Validation Loss: 0.00014407251292141153\n",
      "Epoch [16/25], Train Loss: 0.00016393724945373833, Validation Loss: 0.00014451221116663267\n",
      "Epoch [16/25], Train Loss: 0.0001300308940699324, Validation Loss: 0.00014483172926702537\n",
      "Epoch [16/25], Train Loss: 0.00017115057562477887, Validation Loss: 0.00014483909835689702\n",
      "Epoch [17/25], Train Loss: 0.0001782089238986373, Validation Loss: 0.00014474137205979786\n",
      "Epoch [17/25], Train Loss: 0.00014418560022022575, Validation Loss: 0.00014456446636662196\n",
      "Epoch [17/25], Train Loss: 0.00014878076035529375, Validation Loss: 0.00014418117401267713\n",
      "Epoch [17/25], Train Loss: 0.00018721715605352074, Validation Loss: 0.0001438174738723319\n",
      "Epoch [17/25], Train Loss: 0.00011789191194111481, Validation Loss: 0.0001435544695899201\n",
      "Epoch [17/25], Train Loss: 0.00013033834693487734, Validation Loss: 0.00014351124724877687\n",
      "Epoch [17/25], Train Loss: 0.00016552128363400698, Validation Loss: 0.0001436709972040262\n",
      "Epoch [17/25], Train Loss: 0.00020477929501794279, Validation Loss: 0.0001438591760233976\n",
      "Epoch [17/25], Train Loss: 0.00013326659973245114, Validation Loss: 0.00014410147268790753\n",
      "Epoch [17/25], Train Loss: 0.0002046743466053158, Validation Loss: 0.000144191064464394\n",
      "Epoch [17/25], Train Loss: 0.00014100265980232507, Validation Loss: 0.00014418089049286209\n",
      "Epoch [17/25], Train Loss: 0.0001716137194307521, Validation Loss: 0.00014403419093772147\n",
      "Epoch [17/25], Train Loss: 0.00015356905350927263, Validation Loss: 0.00014390808258516092\n",
      "Epoch [17/25], Train Loss: 0.0001549402077216655, Validation Loss: 0.00014372082684227888\n",
      "Epoch [17/25], Train Loss: 0.00013940462667960674, Validation Loss: 0.00014354563196927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], Train Loss: 0.00013295249664224684, Validation Loss: 0.00014344808732857928\n",
      "Epoch [17/25], Train Loss: 0.00014296866720542312, Validation Loss: 0.00014340611669467763\n",
      "Epoch [17/25], Train Loss: 0.000203967239940539, Validation Loss: 0.00014346784106843795\n",
      "Epoch [17/25], Train Loss: 0.00015556700236629695, Validation Loss: 0.00014355809034896083\n",
      "Epoch [17/25], Train Loss: 0.0002402012178208679, Validation Loss: 0.00014382138300182609\n",
      "Epoch [17/25], Train Loss: 0.00018398613610770553, Validation Loss: 0.00014415981228618573\n",
      "Epoch [17/25], Train Loss: 0.00017238035798072815, Validation Loss: 0.00014440005834330804\n",
      "Epoch [17/25], Train Loss: 0.000173347580130212, Validation Loss: 0.0001446457458465981\n",
      "Epoch [17/25], Train Loss: 0.00021096183627378196, Validation Loss: 0.0001452328823991896\n",
      "Epoch [17/25], Train Loss: 0.00014851389278192073, Validation Loss: 0.00014617693692950222\n",
      "Epoch [17/25], Train Loss: 0.00015770705067552626, Validation Loss: 0.000146826633620852\n",
      "Epoch [17/25], Train Loss: 0.0001376773725496605, Validation Loss: 0.00014729774193256162\n",
      "Epoch [17/25], Train Loss: 0.00017337688768748194, Validation Loss: 0.0001469324129478385\n",
      "Epoch [17/25], Train Loss: 0.00013078712800052017, Validation Loss: 0.0001461221205924327\n",
      "Epoch [17/25], Train Loss: 0.00013206519361119717, Validation Loss: 0.00014438794629919964\n",
      "Epoch [17/25], Train Loss: 0.00014033736078999937, Validation Loss: 0.00014362124105294545\n",
      "Epoch [17/25], Train Loss: 0.00019956288451794535, Validation Loss: 0.0001442219509044662\n",
      "Epoch [17/25], Train Loss: 9.849821799434721e-05, Validation Loss: 0.00014498335658572615\n",
      "Epoch [17/25], Train Loss: 0.00014492445916403085, Validation Loss: 0.00014518089519697242\n",
      "Epoch [17/25], Train Loss: 0.00012357564992271364, Validation Loss: 0.0001445405449582419\n",
      "Epoch [17/25], Train Loss: 0.00017459951050113887, Validation Loss: 0.00014390011225865842\n",
      "Epoch [17/25], Train Loss: 0.00018979892774950713, Validation Loss: 0.0001435867195444492\n",
      "Epoch [17/25], Train Loss: 0.00013976305490359664, Validation Loss: 0.00014391522878819766\n",
      "Epoch [17/25], Train Loss: 0.00016181972750928253, Validation Loss: 0.00014460558328816358\n",
      "Epoch [17/25], Train Loss: 0.00010666905291145667, Validation Loss: 0.00014509247436459796\n",
      "Epoch [17/25], Train Loss: 0.00017993093933910131, Validation Loss: 0.00014539716503350064\n",
      "Epoch [17/25], Train Loss: 0.00015721615636721253, Validation Loss: 0.00014504790936674302\n",
      "Epoch [17/25], Train Loss: 0.0001353995903627947, Validation Loss: 0.00014452969626290723\n",
      "Epoch [17/25], Train Loss: 0.0001359209418296814, Validation Loss: 0.00014397417568640474\n",
      "Epoch [17/25], Train Loss: 0.0001332881220150739, Validation Loss: 0.00014358961755836692\n",
      "Epoch [17/25], Train Loss: 0.0001070140497176908, Validation Loss: 0.00014352936292804467\n",
      "Epoch [17/25], Train Loss: 0.000149435771163553, Validation Loss: 0.00014360655428996932\n",
      "Epoch [17/25], Train Loss: 0.00015048599743749946, Validation Loss: 0.0001438125989807304\n",
      "Epoch [17/25], Train Loss: 0.00013017702440265566, Validation Loss: 0.00014402233694757645\n",
      "Epoch [17/25], Train Loss: 0.0001297274575335905, Validation Loss: 0.000144199136896835\n",
      "Epoch [17/25], Train Loss: 0.00011620938312262297, Validation Loss: 0.0001442518514522817\n",
      "Epoch [17/25], Train Loss: 0.00012512413377407938, Validation Loss: 0.00014419121750203583\n",
      "Epoch [17/25], Train Loss: 0.000199874397367239, Validation Loss: 0.0001440095230160902\n",
      "Epoch [17/25], Train Loss: 0.0001204540312755853, Validation Loss: 0.0001437820680924536\n",
      "Epoch [17/25], Train Loss: 0.00014005361299496144, Validation Loss: 0.000143534284143243\n",
      "Epoch [17/25], Train Loss: 0.0001763994077919051, Validation Loss: 0.00014345808012876659\n",
      "Epoch [17/25], Train Loss: 0.0001784812775440514, Validation Loss: 0.0001433829648400812\n",
      "Epoch [17/25], Train Loss: 0.00013066970859654248, Validation Loss: 0.00014343326062468502\n",
      "Epoch [17/25], Train Loss: 0.00013715532259084284, Validation Loss: 0.0001435042332256368\n",
      "Epoch [17/25], Train Loss: 0.00011522778368089348, Validation Loss: 0.0001435962512914557\n",
      "Epoch [17/25], Train Loss: 0.00016653510101605207, Validation Loss: 0.00014368697229656392\n",
      "Epoch [17/25], Train Loss: 0.0001901333889691159, Validation Loss: 0.000144016324338736\n",
      "Epoch [17/25], Train Loss: 0.00017442366515751928, Validation Loss: 0.00014439432125072925\n",
      "Epoch [17/25], Train Loss: 0.0001474906603107229, Validation Loss: 0.00014456852780616222\n",
      "Epoch [17/25], Train Loss: 0.00016114709433168173, Validation Loss: 0.0001449292481993325\n",
      "Epoch [17/25], Train Loss: 0.00017317032325081527, Validation Loss: 0.00014533163630403578\n",
      "Epoch [17/25], Train Loss: 0.00018998264567926526, Validation Loss: 0.00014582714696492378\n",
      "Epoch [17/25], Train Loss: 0.00017705943901091814, Validation Loss: 0.0001458644105393129\n",
      "Epoch [17/25], Train Loss: 0.00018473812087904662, Validation Loss: 0.0001457436502581307\n",
      "Epoch [17/25], Train Loss: 0.00017388189735356718, Validation Loss: 0.00014517008530674503\n",
      "Epoch [17/25], Train Loss: 0.00019975064788013697, Validation Loss: 0.00014453284602495843\n",
      "Epoch [17/25], Train Loss: 0.0001466881949454546, Validation Loss: 0.00014385294416570106\n",
      "Epoch [17/25], Train Loss: 0.0001434957084711641, Validation Loss: 0.00014358050175360403\n",
      "Epoch [17/25], Train Loss: 0.00016460989718325436, Validation Loss: 0.00014355809180415234\n",
      "Epoch [17/25], Train Loss: 0.0001379322784487158, Validation Loss: 0.00014381357250385917\n",
      "Epoch [17/25], Train Loss: 0.00013198168016970158, Validation Loss: 0.0001440082082505493\n",
      "Epoch [17/25], Train Loss: 0.00014421498053707182, Validation Loss: 0.0001439472282072529\n",
      "Epoch [17/25], Train Loss: 0.00012865432654507458, Validation Loss: 0.00014374777504902643\n",
      "Epoch [17/25], Train Loss: 0.0001844156940933317, Validation Loss: 0.00014359114696465742\n",
      "Epoch [17/25], Train Loss: 0.00016111759759951383, Validation Loss: 0.00014347544323148516\n",
      "Epoch [17/25], Train Loss: 0.00017809290147852153, Validation Loss: 0.00014345106853094574\n",
      "Epoch [17/25], Train Loss: 0.00018741512030828744, Validation Loss: 0.00014357615703678068\n",
      "Epoch [17/25], Train Loss: 0.0001481007639085874, Validation Loss: 0.00014379043229079496\n",
      "Epoch [17/25], Train Loss: 0.00022383502800948918, Validation Loss: 0.00014393832849843118\n",
      "Epoch [17/25], Train Loss: 0.00018261790683027357, Validation Loss: 0.0001441110633701707\n",
      "Epoch [17/25], Train Loss: 0.0001338215806754306, Validation Loss: 0.00014432064272114077\n",
      "Epoch [17/25], Train Loss: 0.00013363122707232833, Validation Loss: 0.00014451448611604671\n",
      "Epoch [17/25], Train Loss: 0.00019692073692567647, Validation Loss: 0.0001445244420513821\n",
      "Epoch [17/25], Train Loss: 0.00010903362999670208, Validation Loss: 0.00014464725342501575\n",
      "Epoch [17/25], Train Loss: 9.889325883705169e-05, Validation Loss: 0.00014448128543638936\n",
      "Epoch [17/25], Train Loss: 0.00015321078535635024, Validation Loss: 0.0001445510339787385\n",
      "Epoch [17/25], Train Loss: 0.00011641426681308076, Validation Loss: 0.00014432277651697708\n",
      "Epoch [17/25], Train Loss: 0.00016047354438342154, Validation Loss: 0.00014442593310377562\n",
      "Epoch [17/25], Train Loss: 0.00015146717487368733, Validation Loss: 0.00014424578427375915\n",
      "Epoch [17/25], Train Loss: 0.00018624713993631303, Validation Loss: 0.0001438271809699169\n",
      "Epoch [17/25], Train Loss: 0.0001883278164314106, Validation Loss: 0.00014382942899828776\n",
      "Epoch [17/25], Train Loss: 0.00013843270426150411, Validation Loss: 0.00014347766894691935\n",
      "Epoch [17/25], Train Loss: 0.00015605459338985384, Validation Loss: 0.00014360860829280378\n",
      "Epoch [17/25], Train Loss: 0.00016753454110585153, Validation Loss: 0.00014353191242359267\n",
      "Epoch [17/25], Train Loss: 0.00016309189959429204, Validation Loss: 0.00014347136093419976\n",
      "Epoch [17/25], Train Loss: 0.00013318349374458194, Validation Loss: 0.0001437724296314021\n",
      "Epoch [17/25], Train Loss: 0.00019800421432591975, Validation Loss: 0.00014362672566979503\n",
      "Epoch [17/25], Train Loss: 0.00018200627528131008, Validation Loss: 0.0001439584169323401\n",
      "Epoch [17/25], Train Loss: 0.00016563069948460907, Validation Loss: 0.00014421541345654987\n",
      "Epoch [17/25], Train Loss: 0.00015125051140785217, Validation Loss: 0.0001439064108126331\n",
      "Epoch [17/25], Train Loss: 0.00014907849254086614, Validation Loss: 0.00014438951523819318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], Train Loss: 0.00015237317711580545, Validation Loss: 0.0001443254327265701\n",
      "Epoch [17/25], Train Loss: 0.0001723461609799415, Validation Loss: 0.00014496536144482282\n",
      "Epoch [17/25], Train Loss: 0.00018709103460423648, Validation Loss: 0.000145703036832856\n",
      "Epoch [17/25], Train Loss: 0.0001566274295328185, Validation Loss: 0.00014581046731715713\n",
      "Epoch [17/25], Train Loss: 0.00011043879931094125, Validation Loss: 0.00014665940131332414\n",
      "Epoch [17/25], Train Loss: 0.00012781676196027547, Validation Loss: 0.00014704151205175247\n",
      "Epoch [17/25], Train Loss: 0.0001826836378313601, Validation Loss: 0.0001467502343681796\n",
      "Epoch [17/25], Train Loss: 0.0001616685331100598, Validation Loss: 0.00014623505703639239\n",
      "Epoch [17/25], Train Loss: 0.00014887894212733954, Validation Loss: 0.0001442786400730256\n",
      "Epoch [17/25], Train Loss: 0.0001516271149739623, Validation Loss: 0.00014360739248028647\n",
      "Epoch [17/25], Train Loss: 0.00011343751975800842, Validation Loss: 0.00014470962802685486\n",
      "Epoch [17/25], Train Loss: 0.00014698084851261228, Validation Loss: 0.00014554743053546798\n",
      "Epoch [17/25], Train Loss: 0.00020640459842979908, Validation Loss: 0.0001463868470940118\n",
      "Epoch [17/25], Train Loss: 0.00021777432993985713, Validation Loss: 0.0001461272428665931\n",
      "Epoch [17/25], Train Loss: 0.00018979368906002492, Validation Loss: 0.00014458734076470137\n",
      "Epoch [17/25], Train Loss: 0.0001614892389625311, Validation Loss: 0.00014374518189773274\n",
      "Epoch [17/25], Train Loss: 0.00018291712331119925, Validation Loss: 0.00014405290470070515\n",
      "Epoch [17/25], Train Loss: 0.00015065405750647187, Validation Loss: 0.00014436168397272315\n",
      "Epoch [17/25], Train Loss: 0.00013348610082175583, Validation Loss: 0.00014542570788762533\n",
      "Epoch [17/25], Train Loss: 0.00015291648742277175, Validation Loss: 0.00014607642515329644\n",
      "Epoch [17/25], Train Loss: 0.00012121124018449336, Validation Loss: 0.00014546475140377879\n",
      "Epoch [17/25], Train Loss: 0.00015907586202956736, Validation Loss: 0.00014487468943116255\n",
      "Epoch [17/25], Train Loss: 0.0001520795631222427, Validation Loss: 0.00014411491768745083\n",
      "Epoch [17/25], Train Loss: 0.00014334158913698047, Validation Loss: 0.00014367677625462723\n",
      "Epoch [17/25], Train Loss: 0.00015149258251767606, Validation Loss: 0.00014376648808441436\n",
      "Epoch [17/25], Train Loss: 0.00013678880350198597, Validation Loss: 0.0001441598831055065\n",
      "Epoch [17/25], Train Loss: 0.00016769225476309657, Validation Loss: 0.00014451596192278278\n",
      "Epoch [17/25], Train Loss: 0.0001543703256174922, Validation Loss: 0.00014452786417677997\n",
      "Epoch [17/25], Train Loss: 0.00013741756265517324, Validation Loss: 0.00014422107827461635\n",
      "Epoch [17/25], Train Loss: 0.00012410702765919268, Validation Loss: 0.00014380743911412234\n",
      "Epoch [17/25], Train Loss: 0.0001404849608661607, Validation Loss: 0.0001434924743080046\n",
      "Epoch [17/25], Train Loss: 0.0001443799992557615, Validation Loss: 0.00014341799687827005\n",
      "Epoch [17/25], Train Loss: 0.00016662305279169232, Validation Loss: 0.00014353280445599618\n",
      "Epoch [17/25], Train Loss: 0.00013390980893746018, Validation Loss: 0.00014369536850911875\n",
      "Epoch [17/25], Train Loss: 0.00020058536028955132, Validation Loss: 0.00014392960583791136\n",
      "Epoch [17/25], Train Loss: 0.00010714064410421997, Validation Loss: 0.0001440167742354485\n",
      "Epoch [17/25], Train Loss: 0.00014735024888068438, Validation Loss: 0.00014388663039426318\n",
      "Epoch [17/25], Train Loss: 0.00017268999363295734, Validation Loss: 0.00014386630743198718\n",
      "Epoch [17/25], Train Loss: 0.0001638829126022756, Validation Loss: 0.0001437948965758551\n",
      "Epoch [17/25], Train Loss: 0.0001418980973539874, Validation Loss: 0.00014372577134054155\n",
      "Epoch [17/25], Train Loss: 0.00017420461517758667, Validation Loss: 0.00014360853238031267\n",
      "Epoch [17/25], Train Loss: 0.00013491447316482663, Validation Loss: 0.00014350272564721915\n",
      "Epoch [17/25], Train Loss: 0.00010820503666764125, Validation Loss: 0.00014345265954034402\n",
      "Epoch [17/25], Train Loss: 0.000130211265059188, Validation Loss: 0.00014347357694835712\n",
      "Epoch [17/25], Train Loss: 8.942857675720006e-05, Validation Loss: 0.00014345076148553442\n",
      "Epoch [17/25], Train Loss: 0.0001365471543977037, Validation Loss: 0.00014348493544578863\n",
      "Epoch [17/25], Train Loss: 0.0001681905414443463, Validation Loss: 0.00014353304965576777\n",
      "Epoch [17/25], Train Loss: 0.0001970338635146618, Validation Loss: 0.00014357970867422408\n",
      "Epoch [17/25], Train Loss: 0.00010635593207553029, Validation Loss: 0.00014368099024674545\n",
      "Epoch [17/25], Train Loss: 0.00018981371249537915, Validation Loss: 0.0001436725171515718\n",
      "Epoch [17/25], Train Loss: 0.00014059057866688818, Validation Loss: 0.00014362003760955608\n",
      "Epoch [17/25], Train Loss: 0.00016160662926267833, Validation Loss: 0.00014358402744013194\n",
      "Epoch [17/25], Train Loss: 0.00016787312051746994, Validation Loss: 0.00014355560245652061\n",
      "Epoch [17/25], Train Loss: 0.00013638872769661248, Validation Loss: 0.0001435390040569473\n",
      "Epoch [17/25], Train Loss: 0.00017283970373682678, Validation Loss: 0.00014358378102770076\n",
      "Epoch [17/25], Train Loss: 0.00019239711400587112, Validation Loss: 0.00014366816055068435\n",
      "Epoch [17/25], Train Loss: 0.00013293421943672001, Validation Loss: 0.00014374773721404684\n",
      "Epoch [17/25], Train Loss: 0.00012831938511226326, Validation Loss: 0.0001438876849230534\n",
      "Epoch [17/25], Train Loss: 0.00016630717436783016, Validation Loss: 0.0001440919394857095\n",
      "Epoch [17/25], Train Loss: 0.00014460533566307276, Validation Loss: 0.00014426110792555846\n",
      "Epoch [17/25], Train Loss: 0.0001612887717783451, Validation Loss: 0.000144417801493546\n",
      "Epoch [17/25], Train Loss: 0.0002036729856627062, Validation Loss: 0.00014475069474428892\n",
      "Epoch [17/25], Train Loss: 0.00014431923045776784, Validation Loss: 0.00014521208189156217\n",
      "Epoch [17/25], Train Loss: 0.00018201088823843747, Validation Loss: 0.0001458341590478085\n",
      "Epoch [17/25], Train Loss: 0.0001425520022166893, Validation Loss: 0.00014651806607920056\n",
      "Epoch [17/25], Train Loss: 0.00010240254778182134, Validation Loss: 0.00014674329941044562\n",
      "Epoch [17/25], Train Loss: 0.00014079116226639599, Validation Loss: 0.00014688644417522786\n",
      "Epoch [17/25], Train Loss: 0.00018550404638517648, Validation Loss: 0.0001460902613568275\n",
      "Epoch [17/25], Train Loss: 0.00016247296298388392, Validation Loss: 0.00014512802808894776\n",
      "Epoch [17/25], Train Loss: 0.00013980877702124417, Validation Loss: 0.00014404925144238708\n",
      "Epoch [17/25], Train Loss: 0.00022045218793209642, Validation Loss: 0.00014358030360502502\n",
      "Epoch [17/25], Train Loss: 0.000157930378918536, Validation Loss: 0.0001435069602545506\n",
      "Epoch [17/25], Train Loss: 0.00015951009117998183, Validation Loss: 0.00014383748566615395\n",
      "Epoch [17/25], Train Loss: 0.00015827635070309043, Validation Loss: 0.00014416924508016868\n",
      "Epoch [17/25], Train Loss: 0.00016222336853388697, Validation Loss: 0.00014419755364845818\n",
      "Epoch [17/25], Train Loss: 0.00013006861263420433, Validation Loss: 0.00014404462854145095\n",
      "Epoch [17/25], Train Loss: 0.00010261025454383343, Validation Loss: 0.0001437905099010095\n",
      "Epoch [17/25], Train Loss: 0.0001172009579022415, Validation Loss: 0.00014357004280706557\n",
      "Epoch [17/25], Train Loss: 0.0001779467856977135, Validation Loss: 0.00014353425746473173\n",
      "Epoch [17/25], Train Loss: 0.00017317869060207158, Validation Loss: 0.0001435726636069982\n",
      "Epoch [17/25], Train Loss: 0.0001653397484915331, Validation Loss: 0.00014372319637914188\n",
      "Epoch [17/25], Train Loss: 0.0001876707683550194, Validation Loss: 0.000144302115222672\n",
      "Epoch [17/25], Train Loss: 0.0001620291732251644, Validation Loss: 0.00014429660489743886\n",
      "Epoch [17/25], Train Loss: 8.616948616690934e-05, Validation Loss: 0.00014492115539421017\n",
      "Epoch [17/25], Train Loss: 0.00015096548304427415, Validation Loss: 0.00014530441743166496\n",
      "Epoch [17/25], Train Loss: 0.00014264810306485742, Validation Loss: 0.00014570378819674565\n",
      "Epoch [17/25], Train Loss: 0.0001691622455837205, Validation Loss: 0.00014612336696397202\n",
      "Epoch [17/25], Train Loss: 0.00016038607282098383, Validation Loss: 0.00014608900261616024\n",
      "Epoch [17/25], Train Loss: 0.0001892560685519129, Validation Loss: 0.00014560905377341744\n",
      "Epoch [17/25], Train Loss: 0.0001540155935799703, Validation Loss: 0.00014457628082406397\n",
      "Epoch [17/25], Train Loss: 0.0001791862305253744, Validation Loss: 0.00014368467551927703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], Train Loss: 0.00011553427611943334, Validation Loss: 0.00014348734039231202\n",
      "Epoch [17/25], Train Loss: 0.0001599214447196573, Validation Loss: 0.0001439809479052201\n",
      "Epoch [17/25], Train Loss: 0.00012030385551042855, Validation Loss: 0.0001445969821361359\n",
      "Epoch [17/25], Train Loss: 0.00014051978359930217, Validation Loss: 0.00014468583976849913\n",
      "Epoch [17/25], Train Loss: 0.00014974211808294058, Validation Loss: 0.00014443915230610098\n",
      "Epoch [17/25], Train Loss: 0.00015695152978878468, Validation Loss: 0.0001440698290631796\n",
      "Epoch [17/25], Train Loss: 0.00017821892106439918, Validation Loss: 0.0001436773430517254\n",
      "Epoch [17/25], Train Loss: 0.00011961668496951461, Validation Loss: 0.0001434418171508393\n",
      "Epoch [17/25], Train Loss: 0.00010571553866611794, Validation Loss: 0.00014349443808896466\n",
      "Epoch [17/25], Train Loss: 0.0001227019092766568, Validation Loss: 0.0001437723253426763\n",
      "Epoch [17/25], Train Loss: 0.0002360792859690264, Validation Loss: 0.0001440511007482807\n",
      "Epoch [17/25], Train Loss: 0.00016821513418108225, Validation Loss: 0.00014454863170006624\n",
      "Epoch [17/25], Train Loss: 0.0001485359825892374, Validation Loss: 0.0001448799557692837\n",
      "Epoch [17/25], Train Loss: 0.00014325800293590873, Validation Loss: 0.0001455300213516845\n",
      "Epoch [17/25], Train Loss: 0.00014783455117139965, Validation Loss: 0.0001459420607716311\n",
      "Epoch [17/25], Train Loss: 0.00013173389015719295, Validation Loss: 0.00014645632860871654\n",
      "Epoch [17/25], Train Loss: 0.00014635987463407218, Validation Loss: 0.00014646662239101716\n",
      "Epoch [17/25], Train Loss: 0.00010871006816159934, Validation Loss: 0.00014596831339683074\n",
      "Epoch [17/25], Train Loss: 0.0002191696548834443, Validation Loss: 0.00014501421222424445\n",
      "Epoch [17/25], Train Loss: 0.00017339971964247525, Validation Loss: 0.0001440536965674255\n",
      "Epoch [17/25], Train Loss: 0.00012216878531035036, Validation Loss: 0.00014357731512670095\n",
      "Epoch [17/25], Train Loss: 0.0002042119886027649, Validation Loss: 0.00014372281851440978\n",
      "Epoch [17/25], Train Loss: 0.0001352570834569633, Validation Loss: 0.0001442399361015608\n",
      "Epoch [17/25], Train Loss: 0.0001742315653245896, Validation Loss: 0.00014465046939828122\n",
      "Epoch [17/25], Train Loss: 0.00011057677329517901, Validation Loss: 0.0001444058686805268\n",
      "Epoch [17/25], Train Loss: 0.00015880970750004053, Validation Loss: 0.00014392172743100674\n",
      "Epoch [17/25], Train Loss: 0.00011147712939418852, Validation Loss: 0.00014348656307750692\n",
      "Epoch [17/25], Train Loss: 0.0001759216538630426, Validation Loss: 0.00014351089121191762\n",
      "Epoch [17/25], Train Loss: 0.00013175472849979997, Validation Loss: 0.0001441364470034993\n",
      "Epoch [17/25], Train Loss: 0.00019692126079462469, Validation Loss: 0.00014490543204980592\n",
      "Epoch [17/25], Train Loss: 0.00017345804371871054, Validation Loss: 0.0001454147355010112\n",
      "Epoch [17/25], Train Loss: 0.00011859805817948654, Validation Loss: 0.00014523072944333155\n",
      "Epoch [17/25], Train Loss: 0.00012073675316059962, Validation Loss: 0.000144844843938093\n",
      "Epoch [17/25], Train Loss: 0.00013398646842688322, Validation Loss: 0.0001442519853299018\n",
      "Epoch [17/25], Train Loss: 0.00019726251775864512, Validation Loss: 0.00014385596975140895\n",
      "Epoch [17/25], Train Loss: 0.0001204965592478402, Validation Loss: 0.00014347975399383965\n",
      "Epoch [17/25], Train Loss: 0.00016435494762845337, Validation Loss: 0.00014357274655291501\n",
      "Epoch [17/25], Train Loss: 0.00015577557496726513, Validation Loss: 0.00014400993435022733\n",
      "Epoch [17/25], Train Loss: 0.00015501781308557838, Validation Loss: 0.00014411703329339314\n",
      "Epoch [17/25], Train Loss: 0.000151724525494501, Validation Loss: 0.00014447238839541873\n",
      "Epoch [17/25], Train Loss: 0.00021137505245860666, Validation Loss: 0.00014465080118194844\n",
      "Epoch [17/25], Train Loss: 0.00012491385859902948, Validation Loss: 0.00014432881895724373\n",
      "Epoch [17/25], Train Loss: 0.00014057962107472122, Validation Loss: 0.0001444224120253542\n",
      "Epoch [17/25], Train Loss: 0.00016098853666335344, Validation Loss: 0.00014398055379084932\n",
      "Epoch [17/25], Train Loss: 0.0001497384364483878, Validation Loss: 0.0001439874589171571\n",
      "Epoch [17/25], Train Loss: 0.0001194166688947007, Validation Loss: 0.00014368513899777705\n",
      "Epoch [17/25], Train Loss: 0.00016834649431984872, Validation Loss: 0.00014348991147320096\n",
      "Epoch [17/25], Train Loss: 0.0001776766439434141, Validation Loss: 0.00014349568640075933\n",
      "Epoch [17/25], Train Loss: 0.0001326828496530652, Validation Loss: 0.00014350102307313743\n",
      "Epoch [17/25], Train Loss: 0.00013370576198212802, Validation Loss: 0.00014363591641692136\n",
      "Epoch [17/25], Train Loss: 0.00010750135697890073, Validation Loss: 0.0001436615648951071\n",
      "Epoch [17/25], Train Loss: 0.00011030869063688442, Validation Loss: 0.00014371520470983037\n",
      "Epoch [17/25], Train Loss: 0.00020326759840827435, Validation Loss: 0.00014375015986540046\n",
      "Epoch [17/25], Train Loss: 0.00013612344628199935, Validation Loss: 0.00014377691186382436\n",
      "Epoch [17/25], Train Loss: 0.0001335602137260139, Validation Loss: 0.00014383185480255635\n",
      "Epoch [17/25], Train Loss: 0.00011904291022801772, Validation Loss: 0.0001438506811003511\n",
      "Epoch [17/25], Train Loss: 0.00013418591697700322, Validation Loss: 0.00014384327053752107\n",
      "Epoch [17/25], Train Loss: 0.00010930324060609564, Validation Loss: 0.00014384310925379394\n",
      "Epoch [17/25], Train Loss: 0.0001533024333184585, Validation Loss: 0.00014383680002841477\n",
      "Epoch [17/25], Train Loss: 0.00017087605374399573, Validation Loss: 0.00014388566754253891\n",
      "Epoch [17/25], Train Loss: 0.0001416078011970967, Validation Loss: 0.00014396959134804394\n",
      "Epoch [17/25], Train Loss: 0.00018975799321196973, Validation Loss: 0.0001441244181478396\n",
      "Epoch [17/25], Train Loss: 0.00022833839466329664, Validation Loss: 0.00014418333521462045\n",
      "Epoch [17/25], Train Loss: 0.00019915594020858407, Validation Loss: 0.00014423795631349396\n",
      "Epoch [17/25], Train Loss: 0.00012934619735460728, Validation Loss: 0.00014423393270893332\n",
      "Epoch [17/25], Train Loss: 0.00014005816774442792, Validation Loss: 0.00014425245050612526\n",
      "Epoch [17/25], Train Loss: 0.00013120996300131083, Validation Loss: 0.00014421010249255535\n",
      "Epoch [17/25], Train Loss: 0.0001441133499611169, Validation Loss: 0.00014408840773588356\n",
      "Epoch [17/25], Train Loss: 0.00013236697122920305, Validation Loss: 0.0001438303913649482\n",
      "Epoch [17/25], Train Loss: 0.0001545219129184261, Validation Loss: 0.0001437595773798724\n",
      "Epoch [17/25], Train Loss: 8.989687921712175e-05, Validation Loss: 0.00014367105371396367\n",
      "Epoch [18/25], Train Loss: 0.00013720858260057867, Validation Loss: 0.00014351689169416203\n",
      "Epoch [18/25], Train Loss: 0.00011607922351686284, Validation Loss: 0.00014341435647414376\n",
      "Epoch [18/25], Train Loss: 0.000134975605760701, Validation Loss: 0.00014340538230802244\n",
      "Epoch [18/25], Train Loss: 0.00017573511286173016, Validation Loss: 0.00014351255110038133\n",
      "Epoch [18/25], Train Loss: 0.00014648328942712396, Validation Loss: 0.00014354315959887268\n",
      "Epoch [18/25], Train Loss: 0.00017286450020037591, Validation Loss: 0.0001434947320376523\n",
      "Epoch [18/25], Train Loss: 0.00016831954417284578, Validation Loss: 0.00014360151908476837\n",
      "Epoch [18/25], Train Loss: 0.00013093558663967997, Validation Loss: 0.0001435832341182201\n",
      "Epoch [18/25], Train Loss: 0.0001506362168584019, Validation Loss: 0.0001436850271905617\n",
      "Epoch [18/25], Train Loss: 0.00014776033640373498, Validation Loss: 0.00014358389210732033\n",
      "Epoch [18/25], Train Loss: 0.0001318312861258164, Validation Loss: 0.0001438234799328105\n",
      "Epoch [18/25], Train Loss: 0.00021213832951616496, Validation Loss: 0.0001439493615180254\n",
      "Epoch [18/25], Train Loss: 0.0001386483636451885, Validation Loss: 0.00014410844329783382\n",
      "Epoch [18/25], Train Loss: 0.00010345449118176475, Validation Loss: 0.00014459890638439296\n",
      "Epoch [18/25], Train Loss: 0.0001720972068142146, Validation Loss: 0.00014491238980554044\n",
      "Epoch [18/25], Train Loss: 0.00013609806774184108, Validation Loss: 0.00014580970600945874\n",
      "Epoch [18/25], Train Loss: 0.00015536925639025867, Validation Loss: 0.00014684479707890812\n",
      "Epoch [18/25], Train Loss: 0.0002105683961417526, Validation Loss: 0.0001479286940593738\n",
      "Epoch [18/25], Train Loss: 0.0001909413404064253, Validation Loss: 0.00014839542782283389\n",
      "Epoch [18/25], Train Loss: 0.00010969060531351715, Validation Loss: 0.00014806914938768993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], Train Loss: 0.00013499819033313543, Validation Loss: 0.0001457866967636316\n",
      "Epoch [18/25], Train Loss: 0.00016183665138669312, Validation Loss: 0.0001437542193646853\n",
      "Epoch [18/25], Train Loss: 0.00016680409316904843, Validation Loss: 0.00014424889498817114\n",
      "Epoch [18/25], Train Loss: 0.00011315932351863012, Validation Loss: 0.000145903461331424\n",
      "Epoch [18/25], Train Loss: 0.00017894397024065256, Validation Loss: 0.00014700955798616633\n",
      "Epoch [18/25], Train Loss: 0.00014657578140031546, Validation Loss: 0.00014624938388199855\n",
      "Epoch [18/25], Train Loss: 0.0001241822901647538, Validation Loss: 0.00014477560204492572\n",
      "Epoch [18/25], Train Loss: 0.00010776452836580575, Validation Loss: 0.00014376742547028698\n",
      "Epoch [18/25], Train Loss: 0.00014137481048237532, Validation Loss: 0.00014372112612666874\n",
      "Epoch [18/25], Train Loss: 0.00013010673865210265, Validation Loss: 0.00014440888238217062\n",
      "Epoch [18/25], Train Loss: 0.00016780401347205043, Validation Loss: 0.00014528918642705928\n",
      "Epoch [18/25], Train Loss: 0.00014673377154394984, Validation Loss: 0.0001456156314816326\n",
      "Epoch [18/25], Train Loss: 6.844292511232197e-05, Validation Loss: 0.00014533825839559237\n",
      "Epoch [18/25], Train Loss: 0.00016510453133378178, Validation Loss: 0.000144871659722412\n",
      "Epoch [18/25], Train Loss: 0.00013688707258552313, Validation Loss: 0.00014415525777925116\n",
      "Epoch [18/25], Train Loss: 0.0001180492399726063, Validation Loss: 0.00014359175935775662\n",
      "Epoch [18/25], Train Loss: 0.0001543787366244942, Validation Loss: 0.00014345332917097646\n",
      "Epoch [18/25], Train Loss: 0.00014493470371235162, Validation Loss: 0.0001439130954774252\n",
      "Epoch [18/25], Train Loss: 0.00013715139357373118, Validation Loss: 0.00014434615635157874\n",
      "Epoch [18/25], Train Loss: 0.00018468379857949913, Validation Loss: 0.0001444041850239349\n",
      "Epoch [18/25], Train Loss: 0.0001314135588472709, Validation Loss: 0.00014413079552468843\n",
      "Epoch [18/25], Train Loss: 0.0001408341049682349, Validation Loss: 0.00014359929045895114\n",
      "Epoch [18/25], Train Loss: 0.00016225398576352745, Validation Loss: 0.00014348172626341694\n",
      "Epoch [18/25], Train Loss: 0.0001245657476829365, Validation Loss: 0.00014348302914489371\n",
      "Epoch [18/25], Train Loss: 0.00016197527293115854, Validation Loss: 0.00014377692859852688\n",
      "Epoch [18/25], Train Loss: 0.00010987850691890344, Validation Loss: 0.00014404539057674508\n",
      "Epoch [18/25], Train Loss: 0.00019497936591506004, Validation Loss: 0.00014409879440790974\n",
      "Epoch [18/25], Train Loss: 0.0001174585340777412, Validation Loss: 0.00014443066611420364\n",
      "Epoch [18/25], Train Loss: 0.00011518545943545178, Validation Loss: 0.00014450580759633642\n",
      "Epoch [18/25], Train Loss: 0.00014787144027650356, Validation Loss: 0.00014475052254662538\n",
      "Epoch [18/25], Train Loss: 0.00014680954336654395, Validation Loss: 0.0001448580927293127\n",
      "Epoch [18/25], Train Loss: 0.00010288348858011886, Validation Loss: 0.00014473600167548285\n",
      "Epoch [18/25], Train Loss: 0.00018677508342079818, Validation Loss: 0.00014447739619451264\n",
      "Epoch [18/25], Train Loss: 0.00016738830890972167, Validation Loss: 0.00014421382802538575\n",
      "Epoch [18/25], Train Loss: 0.00018999863823410124, Validation Loss: 0.0001438971892639529\n",
      "Epoch [18/25], Train Loss: 0.00020013355242554098, Validation Loss: 0.00014351949600192407\n",
      "Epoch [18/25], Train Loss: 0.00013985720579512417, Validation Loss: 0.00014338045366457663\n",
      "Epoch [18/25], Train Loss: 0.00015667815750930458, Validation Loss: 0.00014339391855173745\n",
      "Epoch [18/25], Train Loss: 0.0001613712083781138, Validation Loss: 0.00014345735908136704\n",
      "Epoch [18/25], Train Loss: 0.00014416070189327002, Validation Loss: 0.00014350849960464984\n",
      "Epoch [18/25], Train Loss: 0.00018759490922093391, Validation Loss: 0.00014361363403925982\n",
      "Epoch [18/25], Train Loss: 0.00015668707783333957, Validation Loss: 0.00014367469314796228\n",
      "Epoch [18/25], Train Loss: 0.00017798104090616107, Validation Loss: 0.00014375183624603475\n",
      "Epoch [18/25], Train Loss: 0.00011717961024260148, Validation Loss: 0.00014377427384412538\n",
      "Epoch [18/25], Train Loss: 0.00014977925457060337, Validation Loss: 0.00014388053799242091\n",
      "Epoch [18/25], Train Loss: 0.0001541447127237916, Validation Loss: 0.0001439337325185382\n",
      "Epoch [18/25], Train Loss: 0.00013902300270274282, Validation Loss: 0.0001439736741303932\n",
      "Epoch [18/25], Train Loss: 0.0001522138627478853, Validation Loss: 0.00014401001535588876\n",
      "Epoch [18/25], Train Loss: 0.00018560867465566844, Validation Loss: 0.00014418322025449014\n",
      "Epoch [18/25], Train Loss: 0.00012869310739915818, Validation Loss: 0.00014415650584851392\n",
      "Epoch [18/25], Train Loss: 0.0001598597300471738, Validation Loss: 0.00014418725356032762\n",
      "Epoch [18/25], Train Loss: 0.00015048618661239743, Validation Loss: 0.00014425116920998942\n",
      "Epoch [18/25], Train Loss: 0.00014884228585287929, Validation Loss: 0.00014422491609972592\n",
      "Epoch [18/25], Train Loss: 0.00013530343130696565, Validation Loss: 0.00014424472780471357\n",
      "Epoch [18/25], Train Loss: 0.00022148183779790998, Validation Loss: 0.00014396777842193842\n",
      "Epoch [18/25], Train Loss: 0.0001625238364795223, Validation Loss: 0.00014378725245478563\n",
      "Epoch [18/25], Train Loss: 0.00014790448767598718, Validation Loss: 0.0001436171080664887\n",
      "Epoch [18/25], Train Loss: 0.0001212675852002576, Validation Loss: 0.00014350305646075866\n",
      "Epoch [18/25], Train Loss: 0.00014950578042771667, Validation Loss: 0.00014340483903652057\n",
      "Epoch [18/25], Train Loss: 0.00012399282422848046, Validation Loss: 0.0001433695593732409\n",
      "Epoch [18/25], Train Loss: 0.0001380612375214696, Validation Loss: 0.00014342323289990116\n",
      "Epoch [18/25], Train Loss: 0.00015832166536711156, Validation Loss: 0.00014346706181337746\n",
      "Epoch [18/25], Train Loss: 0.00012715837510768324, Validation Loss: 0.0001435271454586958\n",
      "Epoch [18/25], Train Loss: 0.0001695381652098149, Validation Loss: 0.0001437215937282114\n",
      "Epoch [18/25], Train Loss: 0.00017616098921280354, Validation Loss: 0.0001438852528129549\n",
      "Epoch [18/25], Train Loss: 0.000203367555513978, Validation Loss: 0.00014425700768091096\n",
      "Epoch [18/25], Train Loss: 0.00011847294808831066, Validation Loss: 0.00014462736532247316\n",
      "Epoch [18/25], Train Loss: 0.00014064667630009353, Validation Loss: 0.00014517155213980004\n",
      "Epoch [18/25], Train Loss: 0.00014049679157324135, Validation Loss: 0.00014581593374411264\n",
      "Epoch [18/25], Train Loss: 0.0001963375834748149, Validation Loss: 0.00014690027067748208\n",
      "Epoch [18/25], Train Loss: 0.00017297492013312876, Validation Loss: 0.0001475877664537014\n",
      "Epoch [18/25], Train Loss: 0.00018459836428519338, Validation Loss: 0.0001479287069135656\n",
      "Epoch [18/25], Train Loss: 0.00018469271890353411, Validation Loss: 0.00014684458292322234\n",
      "Epoch [18/25], Train Loss: 0.00010844974895007908, Validation Loss: 0.00014523949915504393\n",
      "Epoch [18/25], Train Loss: 0.0002001784450840205, Validation Loss: 0.00014378847457313288\n",
      "Epoch [18/25], Train Loss: 0.00010105507681146264, Validation Loss: 0.0001436230340914335\n",
      "Epoch [18/25], Train Loss: 0.0001331840321654454, Validation Loss: 0.00014440257776489793\n",
      "Epoch [18/25], Train Loss: 0.00015813503705430776, Validation Loss: 0.00014516933176006813\n",
      "Epoch [18/25], Train Loss: 0.00014588882913812995, Validation Loss: 0.00014507313026115298\n",
      "Epoch [18/25], Train Loss: 0.0001632308412808925, Validation Loss: 0.00014426230942869248\n",
      "Epoch [18/25], Train Loss: 0.00013579883670900017, Validation Loss: 0.000143588894570712\n",
      "Epoch [18/25], Train Loss: 0.00015328019799198955, Validation Loss: 0.00014359164488269015\n",
      "Epoch [18/25], Train Loss: 0.00014008453581482172, Validation Loss: 0.00014409445890729936\n",
      "Epoch [18/25], Train Loss: 0.00012218202755320817, Validation Loss: 0.00014469244051724673\n",
      "Epoch [18/25], Train Loss: 0.0002116802061209455, Validation Loss: 0.00014461398265363338\n",
      "Epoch [18/25], Train Loss: 0.00013309068162925541, Validation Loss: 0.0001442359205005535\n",
      "Epoch [18/25], Train Loss: 0.00015920880832709372, Validation Loss: 0.00014371536405330214\n",
      "Epoch [18/25], Train Loss: 0.0001529411820229143, Validation Loss: 0.0001435009127211136\n",
      "Epoch [18/25], Train Loss: 0.00012990702816750854, Validation Loss: 0.00014346081152325497\n",
      "Epoch [18/25], Train Loss: 0.00015364789578597993, Validation Loss: 0.00014357742450859707\n",
      "Epoch [18/25], Train Loss: 0.00019745726604014635, Validation Loss: 0.00014387341846789544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], Train Loss: 0.00011287406232440844, Validation Loss: 0.00014425015178858301\n",
      "Epoch [18/25], Train Loss: 0.0001327680511167273, Validation Loss: 0.00014466005256205487\n",
      "Epoch [18/25], Train Loss: 0.0001970660377992317, Validation Loss: 0.00014475193796291327\n",
      "Epoch [18/25], Train Loss: 0.00018554281268734485, Validation Loss: 0.00014516697168195\n",
      "Epoch [18/25], Train Loss: 0.00017031232709996402, Validation Loss: 0.00014502794147119857\n",
      "Epoch [18/25], Train Loss: 0.0001474826131016016, Validation Loss: 0.00014460420376659992\n",
      "Epoch [18/25], Train Loss: 0.0001866278616944328, Validation Loss: 0.00014422142727804992\n",
      "Epoch [18/25], Train Loss: 0.0001192568670376204, Validation Loss: 0.00014398471321328544\n",
      "Epoch [18/25], Train Loss: 0.00015307695139199495, Validation Loss: 0.00014366572601526666\n",
      "Epoch [18/25], Train Loss: 0.00011619538418017328, Validation Loss: 0.00014344322286585036\n",
      "Epoch [18/25], Train Loss: 0.00015144547796808183, Validation Loss: 0.00014338253313326277\n",
      "Epoch [18/25], Train Loss: 0.00017293216660618782, Validation Loss: 0.00014349113844218663\n",
      "Epoch [18/25], Train Loss: 0.00012204569793539122, Validation Loss: 0.00014351542486110703\n",
      "Epoch [18/25], Train Loss: 0.0001558270596433431, Validation Loss: 0.0001436805110036706\n",
      "Epoch [18/25], Train Loss: 0.00018005070160143077, Validation Loss: 0.00014382263883211028\n",
      "Epoch [18/25], Train Loss: 0.0001301556039834395, Validation Loss: 0.00014388523559318855\n",
      "Epoch [18/25], Train Loss: 0.00012888530909549445, Validation Loss: 0.00014387941021899072\n",
      "Epoch [18/25], Train Loss: 0.0002510773192625493, Validation Loss: 0.00014382063988402176\n",
      "Epoch [18/25], Train Loss: 0.00012924731709063053, Validation Loss: 0.00014375375782644067\n",
      "Epoch [18/25], Train Loss: 0.00012856614193879068, Validation Loss: 0.00014372127140328909\n",
      "Epoch [18/25], Train Loss: 0.00010612328333081678, Validation Loss: 0.00014361496844988625\n",
      "Epoch [18/25], Train Loss: 0.0001827908999985084, Validation Loss: 0.0001435469576487473\n",
      "Epoch [18/25], Train Loss: 0.00014180784637574106, Validation Loss: 0.00014348517142934725\n",
      "Epoch [18/25], Train Loss: 0.0001533317263238132, Validation Loss: 0.00014342534535292847\n",
      "Epoch [18/25], Train Loss: 0.00016209136811085045, Validation Loss: 0.00014335686476745952\n",
      "Epoch [18/25], Train Loss: 0.00011288803943898529, Validation Loss: 0.00014339430393495907\n",
      "Epoch [18/25], Train Loss: 0.0001471802534069866, Validation Loss: 0.00014338547043735161\n",
      "Epoch [18/25], Train Loss: 0.0001211736089317128, Validation Loss: 0.00014340393875803177\n",
      "Epoch [18/25], Train Loss: 0.00019038820755667984, Validation Loss: 0.00014339039747331603\n",
      "Epoch [18/25], Train Loss: 0.00014851170999463648, Validation Loss: 0.0001434362551663071\n",
      "Epoch [18/25], Train Loss: 0.00016215183131862432, Validation Loss: 0.00014338916807901114\n",
      "Epoch [18/25], Train Loss: 0.00014671152166556567, Validation Loss: 0.0001434053796401713\n",
      "Epoch [18/25], Train Loss: 0.000124556987429969, Validation Loss: 0.0001434321539515319\n",
      "Epoch [18/25], Train Loss: 9.09421723918058e-05, Validation Loss: 0.00014337595421238803\n",
      "Epoch [18/25], Train Loss: 0.00017058190132956952, Validation Loss: 0.00014337179648767537\n",
      "Epoch [18/25], Train Loss: 0.00014465543790720403, Validation Loss: 0.00014337184960216593\n",
      "Epoch [18/25], Train Loss: 0.0002099040721077472, Validation Loss: 0.00014355384361503337\n",
      "Epoch [18/25], Train Loss: 0.00012780973338522017, Validation Loss: 0.00014395208345376886\n",
      "Epoch [18/25], Train Loss: 0.00018596768495626748, Validation Loss: 0.0001442451013038711\n",
      "Epoch [18/25], Train Loss: 0.00013241189299151301, Validation Loss: 0.00014467674312375797\n",
      "Epoch [18/25], Train Loss: 0.00018002232536673546, Validation Loss: 0.00014593566374969668\n",
      "Epoch [18/25], Train Loss: 0.00017815560568124056, Validation Loss: 0.00014885965938447044\n",
      "Epoch [18/25], Train Loss: 0.00019541032088454813, Validation Loss: 0.00015321901737479492\n",
      "Epoch [18/25], Train Loss: 0.0001673430815571919, Validation Loss: 0.0001612072754748321\n",
      "Epoch [18/25], Train Loss: 0.00014317024033516645, Validation Loss: 0.0001608298245021918\n",
      "Epoch [18/25], Train Loss: 0.00016685028094798326, Validation Loss: 0.0001514128807078426\n",
      "Epoch [18/25], Train Loss: 0.0001239590346813202, Validation Loss: 0.00015076531708473339\n",
      "Epoch [18/25], Train Loss: 0.00018553207337390631, Validation Loss: 0.00015542728676033827\n",
      "Epoch [18/25], Train Loss: 0.00012695822806563228, Validation Loss: 0.0001493957863810162\n",
      "Epoch [18/25], Train Loss: 0.0002410922315903008, Validation Loss: 0.0001483287604060024\n",
      "Epoch [18/25], Train Loss: 0.0001704610331216827, Validation Loss: 0.00014681853839041044\n",
      "Epoch [18/25], Train Loss: 0.00016285556193906814, Validation Loss: 0.0001459435916331131\n",
      "Epoch [18/25], Train Loss: 0.00015893211821094155, Validation Loss: 0.00014681254906463438\n",
      "Epoch [18/25], Train Loss: 0.0001603887212695554, Validation Loss: 0.00016713742467497166\n",
      "Epoch [18/25], Train Loss: 0.0001299957511946559, Validation Loss: 0.0003423425378665949\n",
      "Epoch [18/25], Train Loss: 0.00039144131005741656, Validation Loss: 0.0004767760343383998\n",
      "Epoch [18/25], Train Loss: 0.0005467497976496816, Validation Loss: 0.00040185767187116047\n",
      "Epoch [18/25], Train Loss: 0.00040673313196748495, Validation Loss: 0.0002869445495889522\n",
      "Epoch [18/25], Train Loss: 0.0002978729025926441, Validation Loss: 0.00017055164256210749\n",
      "Epoch [18/25], Train Loss: 0.00016820731980260462, Validation Loss: 0.0002256623983460789\n",
      "Epoch [18/25], Train Loss: 0.00016394811973441392, Validation Loss: 0.0002402997978303271\n",
      "Epoch [18/25], Train Loss: 0.0001711691584205255, Validation Loss: 0.00020502458210103214\n",
      "Epoch [18/25], Train Loss: 0.00023481542302761227, Validation Loss: 0.00019744988191329563\n",
      "Epoch [18/25], Train Loss: 0.00019850253011099994, Validation Loss: 0.0002220840649291252\n",
      "Epoch [18/25], Train Loss: 0.0003193721058778465, Validation Loss: 0.00023911290045361966\n",
      "Epoch [18/25], Train Loss: 0.00033761007944121957, Validation Loss: 0.0002313134182865421\n",
      "Epoch [18/25], Train Loss: 0.00020964295254088938, Validation Loss: 0.0002284606317213426\n",
      "Epoch [18/25], Train Loss: 0.00025612942408770323, Validation Loss: 0.00021247745704992363\n",
      "Epoch [18/25], Train Loss: 0.000242059730226174, Validation Loss: 0.00021631270210491492\n",
      "Epoch [18/25], Train Loss: 0.00019470849656499922, Validation Loss: 0.000214484149182681\n",
      "Epoch [18/25], Train Loss: 0.0002107106411131099, Validation Loss: 0.0002151773102620306\n",
      "Epoch [18/25], Train Loss: 0.0002716602757573128, Validation Loss: 0.00021708068476679424\n",
      "Epoch [18/25], Train Loss: 0.0002318868355359882, Validation Loss: 0.0002127866779725688\n",
      "Epoch [18/25], Train Loss: 0.0003745017165783793, Validation Loss: 0.00020701595931313932\n",
      "Epoch [18/25], Train Loss: 0.00034542521461844444, Validation Loss: 0.00020543923795533677\n",
      "Epoch [18/25], Train Loss: 0.00026789039839059114, Validation Loss: 0.0002068328360716502\n",
      "Epoch [18/25], Train Loss: 0.0001630848419154063, Validation Loss: 0.0002077372366329655\n",
      "Epoch [18/25], Train Loss: 0.00020536452939268202, Validation Loss: 0.0002069066365947947\n",
      "Epoch [18/25], Train Loss: 0.00016556947957724333, Validation Loss: 0.00020522736352480327\n",
      "Epoch [18/25], Train Loss: 0.00027230955311097205, Validation Loss: 0.00020346766638491923\n",
      "Epoch [18/25], Train Loss: 0.0002411209570709616, Validation Loss: 0.00020234579181609055\n",
      "Epoch [18/25], Train Loss: 0.0002231450635008514, Validation Loss: 0.00020184001477900891\n",
      "Epoch [18/25], Train Loss: 0.00017457483045291156, Validation Loss: 0.00020133554595910634\n",
      "Epoch [18/25], Train Loss: 0.00021401565754786134, Validation Loss: 0.00020019986210778978\n",
      "Epoch [18/25], Train Loss: 0.00020519434474408627, Validation Loss: 0.00019890282759054873\n",
      "Epoch [18/25], Train Loss: 0.00020182716252747923, Validation Loss: 0.00019821292711033796\n",
      "Epoch [18/25], Train Loss: 0.00023065808636602014, Validation Loss: 0.0001977094546115647\n",
      "Epoch [18/25], Train Loss: 0.00021175589063204825, Validation Loss: 0.00019693101251808305\n",
      "Epoch [18/25], Train Loss: 0.0001954538602149114, Validation Loss: 0.00019560321670724078\n",
      "Epoch [18/25], Train Loss: 0.00028653116896748543, Validation Loss: 0.0001938764403651779\n",
      "Epoch [18/25], Train Loss: 0.000173162596183829, Validation Loss: 0.0001929802048834972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], Train Loss: 0.00017408555140718818, Validation Loss: 0.00019084974725653108\n",
      "Epoch [18/25], Train Loss: 0.00027028031763620675, Validation Loss: 0.00018994872662005945\n",
      "Epoch [18/25], Train Loss: 0.00022376210836227983, Validation Loss: 0.0001883080811239779\n",
      "Epoch [18/25], Train Loss: 0.0001874501904239878, Validation Loss: 0.00018525616421053806\n",
      "Epoch [18/25], Train Loss: 0.0001680974237388, Validation Loss: 0.00018450803157368985\n",
      "Epoch [18/25], Train Loss: 0.0001521263038739562, Validation Loss: 0.00018346140035040056\n",
      "Epoch [18/25], Train Loss: 0.00021376690710894763, Validation Loss: 0.00018239876687099846\n",
      "Epoch [18/25], Train Loss: 0.00021078568533994257, Validation Loss: 0.00018667406014477212\n",
      "Epoch [18/25], Train Loss: 0.00025353586534038186, Validation Loss: 0.00018299893902925154\n",
      "Epoch [18/25], Train Loss: 0.00015972068649716675, Validation Loss: 0.00017735888832248747\n",
      "Epoch [18/25], Train Loss: 0.00017169902275782079, Validation Loss: 0.00017625347754801624\n",
      "Epoch [18/25], Train Loss: 0.00020081370894331485, Validation Loss: 0.00017470327220507896\n",
      "Epoch [18/25], Train Loss: 0.0002437810762785375, Validation Loss: 0.0001680751842892884\n",
      "Epoch [18/25], Train Loss: 0.00018535017443355173, Validation Loss: 0.00016744151604749884\n",
      "Epoch [18/25], Train Loss: 0.00020590776694007218, Validation Loss: 0.00016630179597996176\n",
      "Epoch [18/25], Train Loss: 0.00017698567535262555, Validation Loss: 0.00015895209532269897\n",
      "Epoch [18/25], Train Loss: 0.00014157945406623185, Validation Loss: 0.00015544760681223123\n",
      "Epoch [18/25], Train Loss: 0.00013307221524883062, Validation Loss: 0.00015991957091803972\n",
      "Epoch [18/25], Train Loss: 0.00013791759556625038, Validation Loss: 0.0001826145579495157\n",
      "Epoch [18/25], Train Loss: 0.0001807253429433331, Validation Loss: 0.0001560413545424429\n",
      "Epoch [18/25], Train Loss: 0.0002078287652693689, Validation Loss: 0.00015975217199108252\n",
      "Epoch [18/25], Train Loss: 0.00020707136718556285, Validation Loss: 0.00018658043845789506\n",
      "Epoch [18/25], Train Loss: 0.00019937509205192327, Validation Loss: 0.0001540705806595118\n",
      "Epoch [18/25], Train Loss: 0.0001424668007530272, Validation Loss: 0.00019254213063201557\n",
      "Epoch [18/25], Train Loss: 0.00023268358199857175, Validation Loss: 0.00019114972577275087\n",
      "Epoch [18/25], Train Loss: 0.0001692709920462221, Validation Loss: 0.00017553590441821143\n",
      "Epoch [18/25], Train Loss: 0.00018794828793033957, Validation Loss: 0.0001891427823769239\n",
      "Epoch [18/25], Train Loss: 0.00022301771969068795, Validation Loss: 0.00015820716216694564\n",
      "Epoch [18/25], Train Loss: 0.00016090090502984822, Validation Loss: 0.00017971431613356497\n",
      "Epoch [18/25], Train Loss: 0.000234979743254371, Validation Loss: 0.0001616748590701415\n",
      "Epoch [18/25], Train Loss: 0.00012544133642222732, Validation Loss: 0.00017334879570019742\n",
      "Epoch [18/25], Train Loss: 0.00012092072574887425, Validation Loss: 0.0001594524786924012\n",
      "Epoch [18/25], Train Loss: 0.0003116617735940963, Validation Loss: 0.00016511152401411285\n",
      "Epoch [18/25], Train Loss: 0.00019049194816034287, Validation Loss: 0.0001622873356003159\n",
      "Epoch [18/25], Train Loss: 0.0002106721221935004, Validation Loss: 0.00016040262028885384\n",
      "Epoch [18/25], Train Loss: 0.00012165964290034026, Validation Loss: 0.00016136276826728135\n",
      "Epoch [18/25], Train Loss: 0.00018734407785814255, Validation Loss: 0.00015743895831595486\n",
      "Epoch [18/25], Train Loss: 0.00016964554379228503, Validation Loss: 0.00016126218276137175\n",
      "Epoch [18/25], Train Loss: 0.00020311266416683793, Validation Loss: 0.00015517064772817927\n",
      "Epoch [18/25], Train Loss: 0.00015099698794074357, Validation Loss: 0.00015990176204165132\n",
      "Epoch [18/25], Train Loss: 0.00010848946840269491, Validation Loss: 0.0001542941616207827\n",
      "Epoch [18/25], Train Loss: 0.00023909298761282116, Validation Loss: 0.00015675105872408796\n",
      "Epoch [18/25], Train Loss: 0.00017769554688129574, Validation Loss: 0.00015329520077405807\n",
      "Epoch [18/25], Train Loss: 0.0001786292705219239, Validation Loss: 0.00015484360531748583\n",
      "Epoch [18/25], Train Loss: 0.00010797969298437238, Validation Loss: 0.0001530424878486277\n",
      "Epoch [18/25], Train Loss: 0.0001422351342625916, Validation Loss: 0.00015258311711174126\n",
      "Epoch [18/25], Train Loss: 0.00012271753803361207, Validation Loss: 0.00015314517731894738\n",
      "Epoch [18/25], Train Loss: 0.00015612358401995152, Validation Loss: 0.00015009469861979597\n",
      "Epoch [18/25], Train Loss: 0.00016755604883655906, Validation Loss: 0.00015268945984037903\n",
      "Epoch [18/25], Train Loss: 0.00017768806719686836, Validation Loss: 0.0001496049260216144\n",
      "Epoch [18/25], Train Loss: 0.0001331632665824145, Validation Loss: 0.00015100000309757887\n",
      "Epoch [18/25], Train Loss: 0.00019988314306829125, Validation Loss: 0.00015165985266018349\n",
      "Epoch [18/25], Train Loss: 0.00016512659203726798, Validation Loss: 0.00014878315948105107\n",
      "Epoch [18/25], Train Loss: 0.00019098588381893933, Validation Loss: 0.0001510240268544294\n",
      "Epoch [18/25], Train Loss: 0.00015416191308759153, Validation Loss: 0.00015066642663441598\n",
      "Epoch [18/25], Train Loss: 0.0001613320637261495, Validation Loss: 0.00014851265271621135\n",
      "Epoch [18/25], Train Loss: 0.00016343320021405816, Validation Loss: 0.000150370855408255\n",
      "Epoch [18/25], Train Loss: 0.00014811672735959291, Validation Loss: 0.00015061134011678707\n",
      "Epoch [18/25], Train Loss: 0.0001883984514279291, Validation Loss: 0.00014835042820777745\n",
      "Epoch [18/25], Train Loss: 0.0001556173083372414, Validation Loss: 0.00014946078226785176\n",
      "Epoch [18/25], Train Loss: 0.00012415318633429706, Validation Loss: 0.00014997528172292125\n",
      "Epoch [18/25], Train Loss: 0.0001272594672627747, Validation Loss: 0.00014836361685108084\n",
      "Epoch [18/25], Train Loss: 0.00020926077559124678, Validation Loss: 0.0001481025242052662\n",
      "Epoch [18/25], Train Loss: 0.00010707747424021363, Validation Loss: 0.00014843177486909554\n",
      "Epoch [18/25], Train Loss: 0.0002027928567258641, Validation Loss: 0.0001479757304574984\n",
      "Epoch [18/25], Train Loss: 0.00021198723698034883, Validation Loss: 0.00014779371049371547\n",
      "Epoch [19/25], Train Loss: 0.000126291299238801, Validation Loss: 0.00014781685822526925\n",
      "Epoch [19/25], Train Loss: 0.00014396301412489265, Validation Loss: 0.00014780911563624008\n",
      "Epoch [19/25], Train Loss: 0.0001563234400236979, Validation Loss: 0.0001477001727228829\n",
      "Epoch [19/25], Train Loss: 0.00014574154920410365, Validation Loss: 0.00014758679268804067\n",
      "Epoch [19/25], Train Loss: 0.0001520298537798226, Validation Loss: 0.00014761995577525038\n",
      "Epoch [19/25], Train Loss: 0.00016944615344982594, Validation Loss: 0.00014769530680496245\n",
      "Epoch [19/25], Train Loss: 0.0001778895384632051, Validation Loss: 0.00014763443226305147\n",
      "Epoch [19/25], Train Loss: 0.00017514138016849756, Validation Loss: 0.0001475036857300438\n",
      "Epoch [19/25], Train Loss: 0.00016649819735903293, Validation Loss: 0.0001473888994951267\n",
      "Epoch [19/25], Train Loss: 0.0001621639239601791, Validation Loss: 0.00014747701473728133\n",
      "Epoch [19/25], Train Loss: 0.00014909743913449347, Validation Loss: 0.00014754453198596214\n",
      "Epoch [19/25], Train Loss: 0.00019503348448779434, Validation Loss: 0.00014742823671743584\n",
      "Epoch [19/25], Train Loss: 0.00019979829085059464, Validation Loss: 0.0001472224641474895\n",
      "Epoch [19/25], Train Loss: 0.00016384763875976205, Validation Loss: 0.00014725610866056133\n",
      "Epoch [19/25], Train Loss: 0.0001609617902431637, Validation Loss: 0.0001473618375409084\n",
      "Epoch [19/25], Train Loss: 0.00018962338799610734, Validation Loss: 0.00014722529983070368\n",
      "Epoch [19/25], Train Loss: 0.00017186343029607087, Validation Loss: 0.00014706088938207056\n",
      "Epoch [19/25], Train Loss: 0.0002029497700277716, Validation Loss: 0.00014718018088994239\n",
      "Epoch [19/25], Train Loss: 0.0001755167031660676, Validation Loss: 0.00014731055222606907\n",
      "Epoch [19/25], Train Loss: 9.100390889216214e-05, Validation Loss: 0.00014715786455781198\n",
      "Epoch [19/25], Train Loss: 0.0001684032176854089, Validation Loss: 0.00014693534346103357\n",
      "Epoch [19/25], Train Loss: 0.00011414475011406466, Validation Loss: 0.00014694337514811195\n",
      "Epoch [19/25], Train Loss: 0.00011083352728746831, Validation Loss: 0.00014703243020145843\n",
      "Epoch [19/25], Train Loss: 0.00011159135465277359, Validation Loss: 0.00014693289340357297\n",
      "Epoch [19/25], Train Loss: 0.00018402545538265258, Validation Loss: 0.00014680515814688989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], Train Loss: 0.0001426589151378721, Validation Loss: 0.00014678710819377255\n",
      "Epoch [19/25], Train Loss: 0.00013225592556409538, Validation Loss: 0.00014687777608439015\n",
      "Epoch [19/25], Train Loss: 9.358415263704956e-05, Validation Loss: 0.00014686350768897683\n",
      "Epoch [19/25], Train Loss: 0.00017464542179368436, Validation Loss: 0.00014681222431439284\n",
      "Epoch [19/25], Train Loss: 0.0002220995374955237, Validation Loss: 0.00014669921171541015\n",
      "Epoch [19/25], Train Loss: 0.0001444742811145261, Validation Loss: 0.0001466663617369098\n",
      "Epoch [19/25], Train Loss: 0.00016253696230705827, Validation Loss: 0.00014668956476574142\n",
      "Epoch [19/25], Train Loss: 0.00017403702076990157, Validation Loss: 0.00014665905667546515\n",
      "Epoch [19/25], Train Loss: 0.00011935584188904613, Validation Loss: 0.00014657404196138183\n",
      "Epoch [19/25], Train Loss: 0.00021454706438817084, Validation Loss: 0.0001465839343533541\n",
      "Epoch [19/25], Train Loss: 0.00013434466382022947, Validation Loss: 0.000146727833392409\n",
      "Epoch [19/25], Train Loss: 0.00019848361262120306, Validation Loss: 0.0001466580052995899\n",
      "Epoch [19/25], Train Loss: 8.70437070261687e-05, Validation Loss: 0.0001465522841802643\n",
      "Epoch [19/25], Train Loss: 0.0001143444242188707, Validation Loss: 0.00014654837747608932\n",
      "Epoch [19/25], Train Loss: 0.0001492143201176077, Validation Loss: 0.0001465716651485612\n",
      "Epoch [19/25], Train Loss: 0.00015789235476404428, Validation Loss: 0.00014651562232756988\n",
      "Epoch [19/25], Train Loss: 0.00013179944653529674, Validation Loss: 0.00014653712593523476\n",
      "Epoch [19/25], Train Loss: 0.00013561031664721668, Validation Loss: 0.00014654298890188027\n",
      "Epoch [19/25], Train Loss: 0.00020036898786202073, Validation Loss: 0.00014642197347711772\n",
      "Epoch [19/25], Train Loss: 0.00017205829499289393, Validation Loss: 0.00014642979731434026\n",
      "Epoch [19/25], Train Loss: 0.00017926508735399693, Validation Loss: 0.0001465103036025539\n",
      "Epoch [19/25], Train Loss: 0.00018359487876296043, Validation Loss: 0.00014642895500098044\n",
      "Epoch [19/25], Train Loss: 9.395534289069474e-05, Validation Loss: 0.00014634471081080845\n",
      "Epoch [19/25], Train Loss: 0.00013927070540376008, Validation Loss: 0.00014639761429862118\n",
      "Epoch [19/25], Train Loss: 0.0002195873239543289, Validation Loss: 0.0001463921328346866\n",
      "Epoch [19/25], Train Loss: 0.00014774636656511575, Validation Loss: 0.00014637050068510386\n",
      "Epoch [19/25], Train Loss: 0.0001450350828235969, Validation Loss: 0.0001463754873839207\n",
      "Epoch [19/25], Train Loss: 0.00012126588262617588, Validation Loss: 0.00014632852835347876\n",
      "Epoch [19/25], Train Loss: 0.00015278853243216872, Validation Loss: 0.00014625325396385355\n",
      "Epoch [19/25], Train Loss: 0.00023229223734233528, Validation Loss: 0.0001462739198662651\n",
      "Epoch [19/25], Train Loss: 0.0001450542767997831, Validation Loss: 0.00014630136987155615\n",
      "Epoch [19/25], Train Loss: 0.00015758075460325927, Validation Loss: 0.0001462720882652017\n",
      "Epoch [19/25], Train Loss: 0.00011941097909584641, Validation Loss: 0.00014630504107723634\n",
      "Epoch [19/25], Train Loss: 0.00012846187746617943, Validation Loss: 0.00014631674202973953\n",
      "Epoch [19/25], Train Loss: 0.00013098232739139348, Validation Loss: 0.0001462186970456969\n",
      "Epoch [19/25], Train Loss: 0.0001244091836269945, Validation Loss: 0.00014621997227853473\n",
      "Epoch [19/25], Train Loss: 0.00014199648285284638, Validation Loss: 0.0001462794910670103\n",
      "Epoch [19/25], Train Loss: 0.00017660536104813218, Validation Loss: 0.0001462357761435366\n",
      "Epoch [19/25], Train Loss: 0.00020070889149792492, Validation Loss: 0.00014614737665397115\n",
      "Epoch [19/25], Train Loss: 0.00012090301606804132, Validation Loss: 0.0001461816173105035\n",
      "Epoch [19/25], Train Loss: 0.00022220025130081922, Validation Loss: 0.00014614006577176041\n",
      "Epoch [19/25], Train Loss: 0.00015393107605632395, Validation Loss: 0.0001461113662420151\n",
      "Epoch [19/25], Train Loss: 0.00016814962145872414, Validation Loss: 0.00014610565461528797\n",
      "Epoch [19/25], Train Loss: 9.12066170712933e-05, Validation Loss: 0.00014606048716814257\n",
      "Epoch [19/25], Train Loss: 0.0001826938969315961, Validation Loss: 0.00014606236266748358\n",
      "Epoch [19/25], Train Loss: 7.386192737612873e-05, Validation Loss: 0.0001460965533624403\n",
      "Epoch [19/25], Train Loss: 0.0001668585609877482, Validation Loss: 0.00014609741629101336\n",
      "Epoch [19/25], Train Loss: 0.0002028976596193388, Validation Loss: 0.00014602432614386392\n",
      "Epoch [19/25], Train Loss: 0.00015063058526720852, Validation Loss: 0.00014600217109546066\n",
      "Epoch [19/25], Train Loss: 0.0002023324923356995, Validation Loss: 0.00014606648474000395\n",
      "Epoch [19/25], Train Loss: 0.0002124525635736063, Validation Loss: 0.00014606386636539052\n",
      "Epoch [19/25], Train Loss: 0.00016082407091744244, Validation Loss: 0.00014600683959239784\n",
      "Epoch [19/25], Train Loss: 0.00014175442629493773, Validation Loss: 0.0001459731191668349\n",
      "Epoch [19/25], Train Loss: 0.00014487712178379297, Validation Loss: 0.00014599412946457354\n",
      "Epoch [19/25], Train Loss: 0.00014969620679039508, Validation Loss: 0.00014595383399864658\n",
      "Epoch [19/25], Train Loss: 0.00020792668510694057, Validation Loss: 0.00014591416232481909\n",
      "Epoch [19/25], Train Loss: 0.00014909240417182446, Validation Loss: 0.00014598333727917635\n",
      "Epoch [19/25], Train Loss: 0.0001940853107953444, Validation Loss: 0.00014599647377811683\n",
      "Epoch [19/25], Train Loss: 0.00014057994121685624, Validation Loss: 0.00014595417039042028\n",
      "Epoch [19/25], Train Loss: 0.00010776533599710092, Validation Loss: 0.00014596291002817453\n",
      "Epoch [19/25], Train Loss: 0.00012214394519105554, Validation Loss: 0.00014593091060911926\n",
      "Epoch [19/25], Train Loss: 0.00012660851643886417, Validation Loss: 0.00014588472452790787\n",
      "Epoch [19/25], Train Loss: 0.0001688723568804562, Validation Loss: 0.0001459089381872521\n",
      "Epoch [19/25], Train Loss: 0.00014403784007299691, Validation Loss: 0.00014587303885491565\n",
      "Epoch [19/25], Train Loss: 0.00016599378432147205, Validation Loss: 0.000145874977412556\n",
      "Epoch [19/25], Train Loss: 0.00010276179091306403, Validation Loss: 0.0001458884864405263\n",
      "Epoch [19/25], Train Loss: 0.00019627086294349283, Validation Loss: 0.00014591088305072238\n",
      "Epoch [19/25], Train Loss: 0.00020102335838600993, Validation Loss: 0.00014587106416001915\n",
      "Epoch [19/25], Train Loss: 0.00016871576372068375, Validation Loss: 0.00014583968950319104\n",
      "Epoch [19/25], Train Loss: 0.0001320373557973653, Validation Loss: 0.00014579161288565956\n",
      "Epoch [19/25], Train Loss: 0.00014360772911459208, Validation Loss: 0.00014576180207465466\n",
      "Epoch [19/25], Train Loss: 0.00015438522677868605, Validation Loss: 0.0001457886423546976\n",
      "Epoch [19/25], Train Loss: 0.00015519841690547764, Validation Loss: 0.0001457940158919276\n",
      "Epoch [19/25], Train Loss: 0.00017572286014910787, Validation Loss: 0.00014581794045322264\n",
      "Epoch [19/25], Train Loss: 0.0002073736977763474, Validation Loss: 0.000145866099774139\n",
      "Epoch [19/25], Train Loss: 0.00018247839761897922, Validation Loss: 0.00014590112599156176\n",
      "Epoch [19/25], Train Loss: 0.0001782781764632091, Validation Loss: 0.0001458024045859929\n",
      "Epoch [19/25], Train Loss: 0.00012171699199825525, Validation Loss: 0.0001457670672001162\n",
      "Epoch [19/25], Train Loss: 0.00011686944344546646, Validation Loss: 0.00014573443161983353\n",
      "Epoch [19/25], Train Loss: 0.00020516689983196557, Validation Loss: 0.0001456806525917879\n",
      "Epoch [19/25], Train Loss: 0.00016942041111178696, Validation Loss: 0.00014579209867709625\n",
      "Epoch [19/25], Train Loss: 0.00018197194731328636, Validation Loss: 0.00014582376315956934\n",
      "Epoch [19/25], Train Loss: 9.035092080011964e-05, Validation Loss: 0.000145690316761223\n",
      "Epoch [19/25], Train Loss: 0.0001869908592198044, Validation Loss: 0.0001457111029594671\n",
      "Epoch [19/25], Train Loss: 0.0001228193286806345, Validation Loss: 0.0001457146771523791\n",
      "Epoch [19/25], Train Loss: 0.00011829379945993423, Validation Loss: 0.00014566878890036604\n",
      "Epoch [19/25], Train Loss: 0.0001711793738650158, Validation Loss: 0.00014571943781144608\n",
      "Epoch [19/25], Train Loss: 0.00016739100101403892, Validation Loss: 0.00014572135720906468\n",
      "Epoch [19/25], Train Loss: 0.00021364634449128062, Validation Loss: 0.00014567258937555987\n",
      "Epoch [19/25], Train Loss: 0.0001655183732509613, Validation Loss: 0.00014568126352969556\n",
      "Epoch [19/25], Train Loss: 9.40641257329844e-05, Validation Loss: 0.00014563846392168974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], Train Loss: 0.00015538415755145252, Validation Loss: 0.00014562023546507892\n",
      "Epoch [19/25], Train Loss: 0.00015528485528193414, Validation Loss: 0.00014567953354950683\n",
      "Epoch [19/25], Train Loss: 0.00020189864153508097, Validation Loss: 0.00014568991731114995\n",
      "Epoch [19/25], Train Loss: 0.00015066836203914136, Validation Loss: 0.00014565994812680097\n",
      "Epoch [19/25], Train Loss: 0.0001470951538067311, Validation Loss: 0.00014567586816459274\n",
      "Epoch [19/25], Train Loss: 0.0002036412915913388, Validation Loss: 0.0001456454883737024\n",
      "Epoch [19/25], Train Loss: 0.00010944021050818264, Validation Loss: 0.00014564752321651517\n",
      "Epoch [19/25], Train Loss: 8.9774199295789e-05, Validation Loss: 0.000145675763633335\n",
      "Epoch [19/25], Train Loss: 0.0001849638792918995, Validation Loss: 0.00014560219206032342\n",
      "Epoch [19/25], Train Loss: 0.00011697533773258328, Validation Loss: 0.0001455751723066593\n",
      "Epoch [19/25], Train Loss: 0.00022175376943778247, Validation Loss: 0.00014559475772936518\n",
      "Epoch [19/25], Train Loss: 0.00013428500096779317, Validation Loss: 0.00014560782534924025\n",
      "Epoch [19/25], Train Loss: 0.00019036057346966118, Validation Loss: 0.0001456744779716246\n",
      "Epoch [19/25], Train Loss: 0.00021525607735384256, Validation Loss: 0.00014573636726709083\n",
      "Epoch [19/25], Train Loss: 0.00010601800750009716, Validation Loss: 0.0001457433907489758\n",
      "Epoch [19/25], Train Loss: 0.00015215284656733274, Validation Loss: 0.00014581533808571596\n",
      "Epoch [19/25], Train Loss: 0.00014345069939736277, Validation Loss: 0.0001458714638526241\n",
      "Epoch [19/25], Train Loss: 0.0001319473230978474, Validation Loss: 0.00014594550351224218\n",
      "Epoch [19/25], Train Loss: 0.00015837310638744384, Validation Loss: 0.0001461233594454825\n",
      "Epoch [19/25], Train Loss: 0.00013664447760675102, Validation Loss: 0.00014625777354619154\n",
      "Epoch [19/25], Train Loss: 0.00016468566900584847, Validation Loss: 0.00014651566477065595\n",
      "Epoch [19/25], Train Loss: 0.00014534287038259208, Validation Loss: 0.00014662604250285463\n",
      "Epoch [19/25], Train Loss: 0.00020224782929290086, Validation Loss: 0.0001470101451559458\n",
      "Epoch [19/25], Train Loss: 0.000128815823700279, Validation Loss: 0.0001472404316397539\n",
      "Epoch [19/25], Train Loss: 0.0001459983759559691, Validation Loss: 0.00014744277868885546\n",
      "Epoch [19/25], Train Loss: 0.0001435460289940238, Validation Loss: 0.00014752690015787568\n",
      "Epoch [19/25], Train Loss: 0.00016270512423943728, Validation Loss: 0.00014788881671847775\n",
      "Epoch [19/25], Train Loss: 0.00016086746472865343, Validation Loss: 0.00014771005323079104\n",
      "Epoch [19/25], Train Loss: 0.00016622836119495332, Validation Loss: 0.00014746765858338526\n",
      "Epoch [19/25], Train Loss: 0.0001126379647757858, Validation Loss: 0.0001471493934028937\n",
      "Epoch [19/25], Train Loss: 0.00013237145321909338, Validation Loss: 0.0001466370830409384\n",
      "Epoch [19/25], Train Loss: 0.00010493497393326834, Validation Loss: 0.0001460738620759609\n",
      "Epoch [19/25], Train Loss: 0.00018197158351540565, Validation Loss: 0.00014569056972201603\n",
      "Epoch [19/25], Train Loss: 0.0001324647746514529, Validation Loss: 0.0001456051060813479\n",
      "Epoch [19/25], Train Loss: 0.00015799350512679666, Validation Loss: 0.00014560177830086712\n",
      "Epoch [19/25], Train Loss: 0.00011986191384494305, Validation Loss: 0.00014567997010696369\n",
      "Epoch [19/25], Train Loss: 0.0001569693413330242, Validation Loss: 0.00014593677454589245\n",
      "Epoch [19/25], Train Loss: 0.00016351423982996494, Validation Loss: 0.0001462644104321953\n",
      "Epoch [19/25], Train Loss: 0.00019397508003748953, Validation Loss: 0.0001461933856868806\n",
      "Epoch [19/25], Train Loss: 0.00011420932423789054, Validation Loss: 0.0001460922789798739\n",
      "Epoch [19/25], Train Loss: 0.00016068243712652475, Validation Loss: 0.00014596968442977717\n",
      "Epoch [19/25], Train Loss: 0.00013612642942462116, Validation Loss: 0.00014576262571305658\n",
      "Epoch [19/25], Train Loss: 0.0002169460494769737, Validation Loss: 0.0001455672633407327\n",
      "Epoch [19/25], Train Loss: 0.00014548201579600573, Validation Loss: 0.00014539526018779724\n",
      "Epoch [19/25], Train Loss: 0.0001273577508982271, Validation Loss: 0.00014538299825896198\n",
      "Epoch [19/25], Train Loss: 0.00014073788770474494, Validation Loss: 0.00014541388130358732\n",
      "Epoch [19/25], Train Loss: 0.00010232997010461986, Validation Loss: 0.00014535940887678105\n",
      "Epoch [19/25], Train Loss: 0.00015517754945904016, Validation Loss: 0.00014533661354410772\n",
      "Epoch [19/25], Train Loss: 0.00010588079021545127, Validation Loss: 0.0001453922578851537\n",
      "Epoch [19/25], Train Loss: 0.0001120446395361796, Validation Loss: 0.0001454293999510507\n",
      "Epoch [19/25], Train Loss: 0.00017228885553777218, Validation Loss: 0.00014546668583837648\n",
      "Epoch [19/25], Train Loss: 0.00012479053111746907, Validation Loss: 0.00014553578366758302\n",
      "Epoch [19/25], Train Loss: 0.00014350317360367626, Validation Loss: 0.00014569832516523698\n",
      "Epoch [19/25], Train Loss: 0.0001904190139612183, Validation Loss: 0.00014579070654387275\n",
      "Epoch [19/25], Train Loss: 0.00016258527466561645, Validation Loss: 0.00014588832830971418\n",
      "Epoch [19/25], Train Loss: 0.00013861375919077545, Validation Loss: 0.00014599677742808126\n",
      "Epoch [19/25], Train Loss: 0.0001590453612152487, Validation Loss: 0.00014630490065125438\n",
      "Epoch [19/25], Train Loss: 0.00015824721776880324, Validation Loss: 0.00014667610448668711\n",
      "Epoch [19/25], Train Loss: 9.9453805887606e-05, Validation Loss: 0.00014722112925179924\n",
      "Epoch [19/25], Train Loss: 0.00012566002260427922, Validation Loss: 0.0001476577669867159\n",
      "Epoch [19/25], Train Loss: 0.00014384134556166828, Validation Loss: 0.00014856994821457192\n",
      "Epoch [19/25], Train Loss: 0.00018056228873319924, Validation Loss: 0.00014910251265973785\n",
      "Epoch [19/25], Train Loss: 0.00017307823873125017, Validation Loss: 0.00015010453280410728\n",
      "Epoch [19/25], Train Loss: 0.00018943303439300507, Validation Loss: 0.00015011658397270366\n",
      "Epoch [19/25], Train Loss: 0.0001456940663047135, Validation Loss: 0.00015053596265109567\n",
      "Epoch [19/25], Train Loss: 0.00016764557221904397, Validation Loss: 0.00014900331201109414\n",
      "Epoch [19/25], Train Loss: 0.0001619894610485062, Validation Loss: 0.00014748358880751767\n",
      "Epoch [19/25], Train Loss: 0.0001504217943875119, Validation Loss: 0.00014605463099239083\n",
      "Epoch [19/25], Train Loss: 0.0001464868983021006, Validation Loss: 0.00014536175027994128\n",
      "Epoch [19/25], Train Loss: 0.00014246349746827036, Validation Loss: 0.00014552489640967298\n",
      "Epoch [19/25], Train Loss: 0.00016590200539212674, Validation Loss: 0.00014620813502309223\n",
      "Epoch [19/25], Train Loss: 0.00018362711125519127, Validation Loss: 0.00014695209853622752\n",
      "Epoch [19/25], Train Loss: 0.0001818143209675327, Validation Loss: 0.00014688826607501445\n",
      "Epoch [19/25], Train Loss: 0.00017839521751739085, Validation Loss: 0.00014635410771006717\n",
      "Epoch [19/25], Train Loss: 0.0001770563976606354, Validation Loss: 0.00014564136751384164\n",
      "Epoch [19/25], Train Loss: 0.00016747842892073095, Validation Loss: 0.00014526656159432604\n",
      "Epoch [19/25], Train Loss: 0.0001469688577344641, Validation Loss: 0.0001452652756900837\n",
      "Epoch [19/25], Train Loss: 0.00016211610636673868, Validation Loss: 0.00014570609346264973\n",
      "Epoch [19/25], Train Loss: 0.00012263571261428297, Validation Loss: 0.00014613865860155784\n",
      "Epoch [19/25], Train Loss: 0.00014969728363212198, Validation Loss: 0.00014620429428759962\n",
      "Epoch [19/25], Train Loss: 0.00017009190923999995, Validation Loss: 0.0001462142368836794\n",
      "Epoch [19/25], Train Loss: 0.00011061172699555755, Validation Loss: 0.000146078521599217\n",
      "Epoch [19/25], Train Loss: 0.00013146561104804277, Validation Loss: 0.00014569352424587123\n",
      "Epoch [19/25], Train Loss: 0.0001230363268405199, Validation Loss: 0.00014533617795677855\n",
      "Epoch [19/25], Train Loss: 0.00019251058984082192, Validation Loss: 0.00014528280905021045\n",
      "Epoch [19/25], Train Loss: 0.0002354195312364027, Validation Loss: 0.00014540323754772544\n",
      "Epoch [19/25], Train Loss: 0.00018250047287438065, Validation Loss: 0.00014544113000738434\n",
      "Epoch [19/25], Train Loss: 0.00020838703494518995, Validation Loss: 0.00014565203406770403\n",
      "Epoch [19/25], Train Loss: 0.00016924190276768059, Validation Loss: 0.00014603856955848946\n",
      "Epoch [19/25], Train Loss: 0.00012442620936781168, Validation Loss: 0.0001459884011031439\n",
      "Epoch [19/25], Train Loss: 0.00011085416190326214, Validation Loss: 0.00014596591936424374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], Train Loss: 0.00016722861619200557, Validation Loss: 0.00014621285736211577\n",
      "Epoch [19/25], Train Loss: 0.00011337612522765994, Validation Loss: 0.00014617367972581026\n",
      "Epoch [19/25], Train Loss: 0.00017727921658661216, Validation Loss: 0.00014606410598692795\n",
      "Epoch [19/25], Train Loss: 0.0001725565962260589, Validation Loss: 0.00014615891923313028\n",
      "Epoch [19/25], Train Loss: 0.00015960572636686265, Validation Loss: 0.00014631409576395526\n",
      "Epoch [19/25], Train Loss: 0.00013463411596603692, Validation Loss: 0.00014600096110370942\n",
      "Epoch [19/25], Train Loss: 0.00021369206660892814, Validation Loss: 0.00014586715333280152\n",
      "Epoch [19/25], Train Loss: 0.00019360604346729815, Validation Loss: 0.0001459002887713723\n",
      "Epoch [19/25], Train Loss: 0.00016654428327456117, Validation Loss: 0.00014572551032567086\n",
      "Epoch [19/25], Train Loss: 0.0001792946713976562, Validation Loss: 0.00014546202946803534\n",
      "Epoch [19/25], Train Loss: 0.00021137925796210766, Validation Loss: 0.00014545931844622828\n",
      "Epoch [19/25], Train Loss: 0.00015784382412675768, Validation Loss: 0.00014551716449204833\n",
      "Epoch [19/25], Train Loss: 0.00012815679656341672, Validation Loss: 0.00014535390873788857\n",
      "Epoch [19/25], Train Loss: 0.0001495872129453346, Validation Loss: 0.00014518698590109124\n",
      "Epoch [19/25], Train Loss: 0.000193369371118024, Validation Loss: 0.00014541706962821385\n",
      "Epoch [19/25], Train Loss: 0.00020604547171387821, Validation Loss: 0.0001452066030954787\n",
      "Epoch [19/25], Train Loss: 0.00015519750013481826, Validation Loss: 0.00014517398813040927\n",
      "Epoch [19/25], Train Loss: 0.0001540514058433473, Validation Loss: 0.00014540366319124587\n",
      "Epoch [19/25], Train Loss: 0.00011528458708198741, Validation Loss: 0.000145357743895147\n",
      "Epoch [19/25], Train Loss: 0.00011186456686118618, Validation Loss: 0.0001452808811639746\n",
      "Epoch [19/25], Train Loss: 9.793030039872974e-05, Validation Loss: 0.0001453189312693818\n",
      "Epoch [19/25], Train Loss: 0.00012625943054445088, Validation Loss: 0.00014547945708424473\n",
      "Epoch [19/25], Train Loss: 0.00016679923282936215, Validation Loss: 0.00014546433036836484\n",
      "Epoch [19/25], Train Loss: 0.0002019127132371068, Validation Loss: 0.00014550060222973116\n",
      "Epoch [19/25], Train Loss: 0.00013396285066846758, Validation Loss: 0.00014565911988029256\n",
      "Epoch [19/25], Train Loss: 0.00016670083277858794, Validation Loss: 0.00014584184900741094\n",
      "Epoch [19/25], Train Loss: 0.00022147325216792524, Validation Loss: 0.00014610653670388274\n",
      "Epoch [19/25], Train Loss: 0.0001674973900662735, Validation Loss: 0.0001463921721248577\n",
      "Epoch [19/25], Train Loss: 0.00015351164620369673, Validation Loss: 0.00014666962185098478\n",
      "Epoch [19/25], Train Loss: 0.0001987519208341837, Validation Loss: 0.0001473219158166709\n",
      "Epoch [19/25], Train Loss: 0.00021591331460513175, Validation Loss: 0.00014813815093172404\n",
      "Epoch [19/25], Train Loss: 0.00017044245032593608, Validation Loss: 0.00014933857673895545\n",
      "Epoch [19/25], Train Loss: 8.129109482979402e-05, Validation Loss: 0.00014995144786856447\n",
      "Epoch [19/25], Train Loss: 0.00018471170915290713, Validation Loss: 0.00015136005143479754\n",
      "Epoch [19/25], Train Loss: 0.0001356463908450678, Validation Loss: 0.00015078023376797017\n",
      "Epoch [19/25], Train Loss: 0.00013605748245026916, Validation Loss: 0.00015018824754709688\n",
      "Epoch [19/25], Train Loss: 0.000159646529937163, Validation Loss: 0.000148234947603972\n",
      "Epoch [19/25], Train Loss: 0.00018347290460951626, Validation Loss: 0.00014623930401285178\n",
      "Epoch [19/25], Train Loss: 0.0001468047412345186, Validation Loss: 0.00014518417010549455\n",
      "Epoch [19/25], Train Loss: 0.0001474871824029833, Validation Loss: 0.00014532333298120648\n",
      "Epoch [19/25], Train Loss: 0.00012125654029659927, Validation Loss: 0.00014614438987337053\n",
      "Epoch [19/25], Train Loss: 0.0002211287064710632, Validation Loss: 0.00014686902480510374\n",
      "Epoch [19/25], Train Loss: 0.00014520181866828352, Validation Loss: 0.00014706490765092894\n",
      "Epoch [19/25], Train Loss: 0.00017546840535942465, Validation Loss: 0.00014638363378859746\n",
      "Epoch [19/25], Train Loss: 0.00010211429616902024, Validation Loss: 0.00014561394491465762\n",
      "Epoch [19/25], Train Loss: 0.0001007415703497827, Validation Loss: 0.00014513327623717488\n",
      "Epoch [19/25], Train Loss: 0.00016167844296433032, Validation Loss: 0.00014523099186286952\n",
      "Epoch [19/25], Train Loss: 0.00018307896971236914, Validation Loss: 0.00014560495158851458\n",
      "Epoch [19/25], Train Loss: 0.00012447527842596173, Validation Loss: 0.00014587451805709862\n",
      "Epoch [19/25], Train Loss: 0.00014724121137987822, Validation Loss: 0.000145959313037262\n",
      "Epoch [19/25], Train Loss: 0.00020643368770834059, Validation Loss: 0.00014568610883240277\n",
      "Epoch [19/25], Train Loss: 0.00016712583601474762, Validation Loss: 0.00014528878891724163\n",
      "Epoch [19/25], Train Loss: 0.00012762052938342094, Validation Loss: 0.0001451145578660847\n",
      "Epoch [19/25], Train Loss: 0.00020586323807947338, Validation Loss: 0.0001450783335409748\n",
      "Epoch [19/25], Train Loss: 0.00018437791732139885, Validation Loss: 0.00014515370785375125\n",
      "Epoch [19/25], Train Loss: 0.0001857741008279845, Validation Loss: 0.00014524527723551727\n",
      "Epoch [19/25], Train Loss: 0.00012793976929970086, Validation Loss: 0.000145300001410457\n",
      "Epoch [19/25], Train Loss: 0.00017151785141322762, Validation Loss: 0.00014529345183594462\n",
      "Epoch [19/25], Train Loss: 0.00015575977158732712, Validation Loss: 0.00014525753091826726\n",
      "Epoch [19/25], Train Loss: 0.00012924960174132138, Validation Loss: 0.00014516245670771847\n",
      "Epoch [19/25], Train Loss: 0.00013528112322092056, Validation Loss: 0.0001452893028423811\n",
      "Epoch [20/25], Train Loss: 0.00016106164548546076, Validation Loss: 0.0001452719511386628\n",
      "Epoch [20/25], Train Loss: 0.00016575823246967047, Validation Loss: 0.00014508003781277997\n",
      "Epoch [20/25], Train Loss: 0.00011899185483343899, Validation Loss: 0.0001450335165524545\n",
      "Epoch [20/25], Train Loss: 0.00018671787984203547, Validation Loss: 0.00014499807875836268\n",
      "Epoch [20/25], Train Loss: 0.0001497029879828915, Validation Loss: 0.00014498942958501477\n",
      "Epoch [20/25], Train Loss: 0.00020735918951686472, Validation Loss: 0.00014498379484090645\n",
      "Epoch [20/25], Train Loss: 0.00016390951350331306, Validation Loss: 0.0001450018047762569\n",
      "Epoch [20/25], Train Loss: 0.00011971565254498273, Validation Loss: 0.00014502766910785188\n",
      "Epoch [20/25], Train Loss: 0.00012586846423801035, Validation Loss: 0.00014501948268540825\n",
      "Epoch [20/25], Train Loss: 0.0001236426760442555, Validation Loss: 0.00014504553182632663\n",
      "Epoch [20/25], Train Loss: 0.00014661619206890464, Validation Loss: 0.0001451039907503097\n",
      "Epoch [20/25], Train Loss: 0.0001461040083086118, Validation Loss: 0.00014518682025178956\n",
      "Epoch [20/25], Train Loss: 0.00019578501814976335, Validation Loss: 0.00014530209155054762\n",
      "Epoch [20/25], Train Loss: 0.00015181936032604426, Validation Loss: 0.00014548581214815688\n",
      "Epoch [20/25], Train Loss: 0.00015931243251543492, Validation Loss: 0.0001458979349990841\n",
      "Epoch [20/25], Train Loss: 0.00021450816711876541, Validation Loss: 0.0001462925579592896\n",
      "Epoch [20/25], Train Loss: 0.00015636510215699673, Validation Loss: 0.00014706093352288008\n",
      "Epoch [20/25], Train Loss: 0.00018024383462034166, Validation Loss: 0.0001479191589169204\n",
      "Epoch [20/25], Train Loss: 0.00013942125951871276, Validation Loss: 0.00014933584728472245\n",
      "Epoch [20/25], Train Loss: 0.00017227770877070725, Validation Loss: 0.00015055238764034585\n",
      "Epoch [20/25], Train Loss: 0.00012061587040079758, Validation Loss: 0.00015298947764676995\n",
      "Epoch [20/25], Train Loss: 0.00016450330440420657, Validation Loss: 0.00015367267599989038\n",
      "Epoch [20/25], Train Loss: 0.00015510839875787497, Validation Loss: 0.00015482052840525283\n",
      "Epoch [20/25], Train Loss: 0.00017719611059874296, Validation Loss: 0.00015208111071842722\n",
      "Epoch [20/25], Train Loss: 0.0001662038848735392, Validation Loss: 0.00014936698814077925\n",
      "Epoch [20/25], Train Loss: 0.00016322550072800368, Validation Loss: 0.00014618937833195864\n",
      "Epoch [20/25], Train Loss: 0.00017773783474694937, Validation Loss: 0.00014498658711090684\n",
      "Epoch [20/25], Train Loss: 0.00018877297407016158, Validation Loss: 0.0001461407324920098\n",
      "Epoch [20/25], Train Loss: 0.00017242782632820308, Validation Loss: 0.0001477070722709565\n",
      "Epoch [20/25], Train Loss: 0.00014277482114266604, Validation Loss: 0.00014816398591695665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], Train Loss: 0.00012666836846619844, Validation Loss: 0.00014679053395714922\n",
      "Epoch [20/25], Train Loss: 0.00014883225958328694, Validation Loss: 0.00014546407255693338\n",
      "Epoch [20/25], Train Loss: 0.00014330711564980447, Validation Loss: 0.00014513509086100385\n",
      "Epoch [20/25], Train Loss: 0.00017839430074673146, Validation Loss: 0.00014596853022036765\n",
      "Epoch [20/25], Train Loss: 0.0001450981799280271, Validation Loss: 0.00014708401383055995\n",
      "Epoch [20/25], Train Loss: 0.0001863617217168212, Validation Loss: 0.00014672117516359624\n",
      "Epoch [20/25], Train Loss: 0.00014476665819529444, Validation Loss: 0.00014572771615348757\n",
      "Epoch [20/25], Train Loss: 0.00016506400424987078, Validation Loss: 0.00014530201830590764\n",
      "Epoch [20/25], Train Loss: 0.0001683371519902721, Validation Loss: 0.0001454080265830271\n",
      "Epoch [20/25], Train Loss: 0.00015995105786714703, Validation Loss: 0.0001457902464608196\n",
      "Epoch [20/25], Train Loss: 0.00019229642930440605, Validation Loss: 0.0001460929942065074\n",
      "Epoch [20/25], Train Loss: 0.00012960383901372552, Validation Loss: 0.00014599013860182216\n",
      "Epoch [20/25], Train Loss: 0.00016628419689368457, Validation Loss: 0.00014545174005130927\n",
      "Epoch [20/25], Train Loss: 0.00014505386934615672, Validation Loss: 0.0001450560654726966\n",
      "Epoch [20/25], Train Loss: 0.00015549981617368758, Validation Loss: 0.00014516055016429165\n",
      "Epoch [20/25], Train Loss: 0.00023179185518529266, Validation Loss: 0.0001453962819747782\n",
      "Epoch [20/25], Train Loss: 0.0001329458027612418, Validation Loss: 0.00014548318552745816\n",
      "Epoch [20/25], Train Loss: 0.0001470991555834189, Validation Loss: 0.00014542693582673868\n",
      "Epoch [20/25], Train Loss: 0.0002136464900104329, Validation Loss: 0.00014526662926073186\n",
      "Epoch [20/25], Train Loss: 0.00017218496941495687, Validation Loss: 0.00014508693226768324\n",
      "Epoch [20/25], Train Loss: 0.00019903942302335054, Validation Loss: 0.000145008315545662\n",
      "Epoch [20/25], Train Loss: 0.0001702752779237926, Validation Loss: 0.00014500421821139753\n",
      "Epoch [20/25], Train Loss: 0.0001797799050109461, Validation Loss: 0.00014508647606514084\n",
      "Epoch [20/25], Train Loss: 0.00017423578538000584, Validation Loss: 0.00014507866314185472\n",
      "Epoch [20/25], Train Loss: 0.00012839227565564215, Validation Loss: 0.00014510437540593557\n",
      "Epoch [20/25], Train Loss: 0.00017229467630386353, Validation Loss: 0.00014507443144490633\n",
      "Epoch [20/25], Train Loss: 0.00013810541713610291, Validation Loss: 0.0001449546723354918\n",
      "Epoch [20/25], Train Loss: 0.00013799576845485717, Validation Loss: 0.00014486541452545982\n",
      "Epoch [20/25], Train Loss: 0.00010675028170226142, Validation Loss: 0.00014489888514314467\n",
      "Epoch [20/25], Train Loss: 0.00016695434169378132, Validation Loss: 0.00014487591082191405\n",
      "Epoch [20/25], Train Loss: 0.00015763512055855244, Validation Loss: 0.000144900244292027\n",
      "Epoch [20/25], Train Loss: 0.00017779076006263494, Validation Loss: 0.00014498579936722916\n",
      "Epoch [20/25], Train Loss: 0.00016430021787527949, Validation Loss: 0.00014487761436612345\n",
      "Epoch [20/25], Train Loss: 0.00015380754484795034, Validation Loss: 0.0001450052744379112\n",
      "Epoch [20/25], Train Loss: 0.00019525701645761728, Validation Loss: 0.0001450517379756396\n",
      "Epoch [20/25], Train Loss: 0.00017029211448971182, Validation Loss: 0.00014494909046334215\n",
      "Epoch [20/25], Train Loss: 0.00015452071966137737, Validation Loss: 0.0001451546442694962\n",
      "Epoch [20/25], Train Loss: 0.00015959017036948353, Validation Loss: 0.00014508395373316791\n",
      "Epoch [20/25], Train Loss: 0.00013700428826268762, Validation Loss: 0.00014493563649011775\n",
      "Epoch [20/25], Train Loss: 0.0002516876847948879, Validation Loss: 0.00014505751848143216\n",
      "Epoch [20/25], Train Loss: 0.00014512335474137217, Validation Loss: 0.0001452006622760867\n",
      "Epoch [20/25], Train Loss: 0.00011404002725612372, Validation Loss: 0.00014488765276231182\n",
      "Epoch [20/25], Train Loss: 0.00011112799256807193, Validation Loss: 0.0001449872977294338\n",
      "Epoch [20/25], Train Loss: 0.00014305359218269587, Validation Loss: 0.00014498555610771292\n",
      "Epoch [20/25], Train Loss: 0.00011671876563923433, Validation Loss: 0.00014482516077502321\n",
      "Epoch [20/25], Train Loss: 0.00010439804464112967, Validation Loss: 0.00014490096024625623\n",
      "Epoch [20/25], Train Loss: 0.00014778398326598108, Validation Loss: 0.00014491002875729463\n",
      "Epoch [20/25], Train Loss: 0.00015995312423910946, Validation Loss: 0.00014488352074598271\n",
      "Epoch [20/25], Train Loss: 0.00018744371482171118, Validation Loss: 0.00014508596068480984\n",
      "Epoch [20/25], Train Loss: 0.00016424203931819648, Validation Loss: 0.0001449410638694341\n",
      "Epoch [20/25], Train Loss: 0.00022366461053024977, Validation Loss: 0.00014490563892953409\n",
      "Epoch [20/25], Train Loss: 0.00017860429943539202, Validation Loss: 0.0001449354330058365\n",
      "Epoch [20/25], Train Loss: 0.00019664857245516032, Validation Loss: 0.00014484259081655183\n",
      "Epoch [20/25], Train Loss: 0.00014795920287724584, Validation Loss: 0.00014489197662139\n",
      "Epoch [20/25], Train Loss: 0.00013638305244967341, Validation Loss: 0.00014492197005893103\n",
      "Epoch [20/25], Train Loss: 9.695946209831163e-05, Validation Loss: 0.0001448703815791911\n",
      "Epoch [20/25], Train Loss: 0.0001589663588674739, Validation Loss: 0.00014494436521393558\n",
      "Epoch [20/25], Train Loss: 0.00014753345749340951, Validation Loss: 0.00014498915552394465\n",
      "Epoch [20/25], Train Loss: 0.00017051363829523325, Validation Loss: 0.00014498535092570818\n",
      "Epoch [20/25], Train Loss: 0.0001752694370225072, Validation Loss: 0.0001449669662785406\n",
      "Epoch [20/25], Train Loss: 0.00013251506607048213, Validation Loss: 0.0001451608302886598\n",
      "Epoch [20/25], Train Loss: 0.00017930753529071808, Validation Loss: 0.00014516872761305422\n",
      "Epoch [20/25], Train Loss: 0.00013391434913501143, Validation Loss: 0.00014512295868674603\n",
      "Epoch [20/25], Train Loss: 0.00017123619909398258, Validation Loss: 0.00014543179471123344\n",
      "Epoch [20/25], Train Loss: 0.00017504015704616904, Validation Loss: 0.00014578159364949292\n",
      "Epoch [20/25], Train Loss: 0.00015331507893279195, Validation Loss: 0.0001458848940577203\n",
      "Epoch [20/25], Train Loss: 0.00017072242917492986, Validation Loss: 0.00014635167050679835\n",
      "Epoch [20/25], Train Loss: 0.00015749553858768195, Validation Loss: 0.00014701482117137252\n",
      "Epoch [20/25], Train Loss: 0.00016729334311094135, Validation Loss: 0.00014788672609332328\n",
      "Epoch [20/25], Train Loss: 0.0001917136542033404, Validation Loss: 0.00014802330818686945\n",
      "Epoch [20/25], Train Loss: 8.17759646452032e-05, Validation Loss: 0.00014870514860376716\n",
      "Epoch [20/25], Train Loss: 0.0001406850351486355, Validation Loss: 0.0001488825835015935\n",
      "Epoch [20/25], Train Loss: 0.00016490092093590647, Validation Loss: 0.0001490204202127643\n",
      "Epoch [20/25], Train Loss: 0.0001949707802850753, Validation Loss: 0.00014816364855505527\n",
      "Epoch [20/25], Train Loss: 0.00015628055552951992, Validation Loss: 0.00014743834156737042\n",
      "Epoch [20/25], Train Loss: 0.00015037896810099483, Validation Loss: 0.0001463718777813483\n",
      "Epoch [20/25], Train Loss: 0.00014734613068867475, Validation Loss: 0.0001455044989900974\n",
      "Epoch [20/25], Train Loss: 0.0001304732431890443, Validation Loss: 0.00014506569180715209\n",
      "Epoch [20/25], Train Loss: 0.00018266728147864342, Validation Loss: 0.0001450261338807953\n",
      "Epoch [20/25], Train Loss: 0.00018553635163698345, Validation Loss: 0.00014534933143295348\n",
      "Epoch [20/25], Train Loss: 0.00011107650789199397, Validation Loss: 0.0001457292911557791\n",
      "Epoch [20/25], Train Loss: 0.00016725363093428314, Validation Loss: 0.0001461469932110049\n",
      "Epoch [20/25], Train Loss: 0.0001559868542244658, Validation Loss: 0.00014627678247052244\n",
      "Epoch [20/25], Train Loss: 0.00011818495840998366, Validation Loss: 0.00014632261081715114\n",
      "Epoch [20/25], Train Loss: 0.00012979093298781663, Validation Loss: 0.00014604298145665478\n",
      "Epoch [20/25], Train Loss: 0.00014028391160536557, Validation Loss: 0.00014573967904046487\n",
      "Epoch [20/25], Train Loss: 0.00013936316827312112, Validation Loss: 0.00014530021362588741\n",
      "Epoch [20/25], Train Loss: 0.00017435984045732766, Validation Loss: 0.000145051672734553\n",
      "Epoch [20/25], Train Loss: 0.0001282988378079608, Validation Loss: 0.00014488767604537618\n",
      "Epoch [20/25], Train Loss: 0.00016270720516331494, Validation Loss: 0.0001449117980276545\n",
      "Epoch [20/25], Train Loss: 0.00021280019427649677, Validation Loss: 0.00014491613231560526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], Train Loss: 0.00018201518105342984, Validation Loss: 0.0001452002295991406\n",
      "Epoch [20/25], Train Loss: 0.00013774234685115516, Validation Loss: 0.000145323621109128\n",
      "Epoch [20/25], Train Loss: 0.00014715723227709532, Validation Loss: 0.00014540384969829271\n",
      "Epoch [20/25], Train Loss: 0.00017485937860328704, Validation Loss: 0.0001456276583970369\n",
      "Epoch [20/25], Train Loss: 0.0001634134678170085, Validation Loss: 0.0001457539425852398\n",
      "Epoch [20/25], Train Loss: 0.00011799558706115931, Validation Loss: 0.00014594775299580457\n",
      "Epoch [20/25], Train Loss: 0.00013566661800723523, Validation Loss: 0.0001460393495411457\n",
      "Epoch [20/25], Train Loss: 0.00013259754632599652, Validation Loss: 0.00014615300509224956\n",
      "Epoch [20/25], Train Loss: 0.00022837150027044117, Validation Loss: 0.00014657845022156835\n",
      "Epoch [20/25], Train Loss: 0.00011518981045810506, Validation Loss: 0.00014656591544432256\n",
      "Epoch [20/25], Train Loss: 0.00013166212011128664, Validation Loss: 0.00014675113658692377\n",
      "Epoch [20/25], Train Loss: 0.00016983835666906089, Validation Loss: 0.00014710403096008425\n",
      "Epoch [20/25], Train Loss: 0.00012333835184108466, Validation Loss: 0.00014710890682181343\n",
      "Epoch [20/25], Train Loss: 0.00015991556574590504, Validation Loss: 0.00014762614421973315\n",
      "Epoch [20/25], Train Loss: 0.00015161311603151262, Validation Loss: 0.00014763293656869792\n",
      "Epoch [20/25], Train Loss: 0.0001542992831673473, Validation Loss: 0.00014783903194863038\n",
      "Epoch [20/25], Train Loss: 9.716856584418565e-05, Validation Loss: 0.000147687254017607\n",
      "Epoch [20/25], Train Loss: 0.0001386920193908736, Validation Loss: 0.00014747759754148623\n",
      "Epoch [20/25], Train Loss: 0.00011675864516291767, Validation Loss: 0.00014697573618226064\n",
      "Epoch [20/25], Train Loss: 0.00012289808364585042, Validation Loss: 0.00014629165040484318\n",
      "Epoch [20/25], Train Loss: 0.0001165721332654357, Validation Loss: 0.00014560221558591971\n",
      "Epoch [20/25], Train Loss: 0.00015313115727622062, Validation Loss: 0.00014526013037539088\n",
      "Epoch [20/25], Train Loss: 0.00013917573960497975, Validation Loss: 0.00014486003395480413\n",
      "Epoch [20/25], Train Loss: 0.00014553898654412478, Validation Loss: 0.00014484775747405365\n",
      "Epoch [20/25], Train Loss: 0.000148169812746346, Validation Loss: 0.0001451885703621277\n",
      "Epoch [20/25], Train Loss: 0.0001062791925505735, Validation Loss: 0.00014533035428030417\n",
      "Epoch [20/25], Train Loss: 0.00012259981303941458, Validation Loss: 0.00014554987865267321\n",
      "Epoch [20/25], Train Loss: 0.00016049350961111486, Validation Loss: 0.00014557890681317077\n",
      "Epoch [20/25], Train Loss: 0.00015476006956305355, Validation Loss: 0.00014545057444289948\n",
      "Epoch [20/25], Train Loss: 0.00016261881683021784, Validation Loss: 0.00014528643174950654\n",
      "Epoch [20/25], Train Loss: 9.78878015303053e-05, Validation Loss: 0.00014501110053970477\n",
      "Epoch [20/25], Train Loss: 0.0001761654275469482, Validation Loss: 0.00014483243188199898\n",
      "Epoch [20/25], Train Loss: 0.00011876904318341985, Validation Loss: 0.0001447776429510365\n",
      "Epoch [20/25], Train Loss: 9.561381011735648e-05, Validation Loss: 0.00014473223782260903\n",
      "Epoch [20/25], Train Loss: 0.00012081688328180462, Validation Loss: 0.00014484316901265022\n",
      "Epoch [20/25], Train Loss: 0.00013391024549491704, Validation Loss: 0.00014497504016617314\n",
      "Epoch [20/25], Train Loss: 0.00012759795936290175, Validation Loss: 0.0001450658011890482\n",
      "Epoch [20/25], Train Loss: 0.0001719972788123414, Validation Loss: 0.0001451435059910485\n",
      "Epoch [20/25], Train Loss: 0.00013947692059446126, Validation Loss: 0.00014521286818004834\n",
      "Epoch [20/25], Train Loss: 0.00010206777369603515, Validation Loss: 0.0001451437203892662\n",
      "Epoch [20/25], Train Loss: 0.00012461205187719315, Validation Loss: 0.00014509439594500388\n",
      "Epoch [20/25], Train Loss: 0.0001883747027022764, Validation Loss: 0.00014513727510347962\n",
      "Epoch [20/25], Train Loss: 0.00014266018115449697, Validation Loss: 0.0001451101971421546\n",
      "Epoch [20/25], Train Loss: 0.00020976060477551073, Validation Loss: 0.00014504200832258599\n",
      "Epoch [20/25], Train Loss: 0.0001520205696579069, Validation Loss: 0.00014509834533479686\n",
      "Epoch [20/25], Train Loss: 0.00011847352288896218, Validation Loss: 0.00014496865963640934\n",
      "Epoch [20/25], Train Loss: 0.00016473614959977567, Validation Loss: 0.00014488427210987236\n",
      "Epoch [20/25], Train Loss: 0.000201268499949947, Validation Loss: 0.0001448804937050833\n",
      "Epoch [20/25], Train Loss: 0.00016212421178352088, Validation Loss: 0.0001447790134989191\n",
      "Epoch [20/25], Train Loss: 0.00016770722868386656, Validation Loss: 0.00014474306711539006\n",
      "Epoch [20/25], Train Loss: 0.0001740599691402167, Validation Loss: 0.00014478797020274216\n",
      "Epoch [20/25], Train Loss: 0.0001771093375282362, Validation Loss: 0.00014470920456612172\n",
      "Epoch [20/25], Train Loss: 0.00014696035941597074, Validation Loss: 0.00014465884014498443\n",
      "Epoch [20/25], Train Loss: 0.00017239907174371183, Validation Loss: 0.00014465417965160062\n",
      "Epoch [20/25], Train Loss: 0.00015174367581494153, Validation Loss: 0.00014467531970391672\n",
      "Epoch [20/25], Train Loss: 0.00015259167412295938, Validation Loss: 0.0001446923740635005\n",
      "Epoch [20/25], Train Loss: 0.00014311722770798951, Validation Loss: 0.0001446666208115251\n",
      "Epoch [20/25], Train Loss: 8.132674702210352e-05, Validation Loss: 0.00014468402829758513\n",
      "Epoch [20/25], Train Loss: 0.0001530333247501403, Validation Loss: 0.00014467294919692601\n",
      "Epoch [20/25], Train Loss: 0.00015245888789650053, Validation Loss: 0.00014466060480723778\n",
      "Epoch [20/25], Train Loss: 0.00012413921649567783, Validation Loss: 0.00014474788743730946\n",
      "Epoch [20/25], Train Loss: 0.0001294505927944556, Validation Loss: 0.0001448085354544067\n",
      "Epoch [20/25], Train Loss: 0.00011787143012043089, Validation Loss: 0.00014473151701774138\n",
      "Epoch [20/25], Train Loss: 0.00017506441508885473, Validation Loss: 0.00014482023107120767\n",
      "Epoch [20/25], Train Loss: 0.00021415058290585876, Validation Loss: 0.0001449003058951348\n",
      "Epoch [20/25], Train Loss: 0.0001593366905581206, Validation Loss: 0.00014503632240424243\n",
      "Epoch [20/25], Train Loss: 0.00010867581295315176, Validation Loss: 0.0001452180564228911\n",
      "Epoch [20/25], Train Loss: 0.00018424654263071716, Validation Loss: 0.00014556382448063231\n",
      "Epoch [20/25], Train Loss: 0.00017786498938221484, Validation Loss: 0.00014622132415145945\n",
      "Epoch [20/25], Train Loss: 0.00014014831685926765, Validation Loss: 0.00014733988500665874\n",
      "Epoch [20/25], Train Loss: 0.00011409360013203695, Validation Loss: 0.00014878884661205424\n",
      "Epoch [20/25], Train Loss: 0.000138118484755978, Validation Loss: 0.000151725040874832\n",
      "Epoch [20/25], Train Loss: 0.00019131605222355574, Validation Loss: 0.00015455681253418637\n",
      "Epoch [20/25], Train Loss: 0.00017806346295401454, Validation Loss: 0.00016031777607471062\n",
      "Epoch [20/25], Train Loss: 0.00013935586321167648, Validation Loss: 0.00016170654198504052\n",
      "Epoch [20/25], Train Loss: 0.00023263950424734503, Validation Loss: 0.00016403409002426392\n",
      "Epoch [20/25], Train Loss: 0.00013672717614099383, Validation Loss: 0.0001560224212880712\n",
      "Epoch [20/25], Train Loss: 0.00012694139149971306, Validation Loss: 0.00014873160083273736\n",
      "Epoch [20/25], Train Loss: 0.00019119097851216793, Validation Loss: 0.0001447976911246466\n",
      "Epoch [20/25], Train Loss: 0.0001736505510052666, Validation Loss: 0.00014751457686846455\n",
      "Epoch [20/25], Train Loss: 0.00014363131776917726, Validation Loss: 0.00015195324886008166\n",
      "Epoch [20/25], Train Loss: 0.0002377190685365349, Validation Loss: 0.00015032095446561775\n",
      "Epoch [20/25], Train Loss: 0.00016419959138147533, Validation Loss: 0.0001462626205466222\n",
      "Epoch [20/25], Train Loss: 0.0001240281853824854, Validation Loss: 0.00014516836672555654\n",
      "Epoch [20/25], Train Loss: 0.00013152546307537705, Validation Loss: 0.0001479527258197777\n",
      "Epoch [20/25], Train Loss: 0.00011507644376251847, Validation Loss: 0.00014940245382604188\n",
      "Epoch [20/25], Train Loss: 0.00021006165479775518, Validation Loss: 0.00014655762885619575\n",
      "Epoch [20/25], Train Loss: 0.00015067034109961241, Validation Loss: 0.00014487517012942892\n",
      "Epoch [20/25], Train Loss: 0.00016481277998536825, Validation Loss: 0.00014650809680460952\n",
      "Epoch [20/25], Train Loss: 0.00020664898329414427, Validation Loss: 0.00014776803097144391\n",
      "Epoch [20/25], Train Loss: 0.00012815951777156442, Validation Loss: 0.0001472018033382483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], Train Loss: 0.00017204933101311326, Validation Loss: 0.00014549882374315833\n",
      "Epoch [20/25], Train Loss: 0.00022785071632824838, Validation Loss: 0.000145135999143046\n",
      "Epoch [20/25], Train Loss: 0.00020122859859839082, Validation Loss: 0.00014551033794608277\n",
      "Epoch [20/25], Train Loss: 0.00011286180233582854, Validation Loss: 0.00014571935729084846\n",
      "Epoch [20/25], Train Loss: 0.0001540695084258914, Validation Loss: 0.00014563851145794616\n",
      "Epoch [20/25], Train Loss: 0.0001437240862287581, Validation Loss: 0.0001450182465002096\n",
      "Epoch [20/25], Train Loss: 0.00016681267879903316, Validation Loss: 0.00014508823078358547\n",
      "Epoch [20/25], Train Loss: 0.00019393190450500697, Validation Loss: 0.00014540185487324682\n",
      "Epoch [20/25], Train Loss: 0.00014839100185781717, Validation Loss: 0.00014526146987918763\n",
      "Epoch [20/25], Train Loss: 0.00012091192184016109, Validation Loss: 0.00014525245496770366\n",
      "Epoch [20/25], Train Loss: 0.00021205240045674145, Validation Loss: 0.00014482385959126986\n",
      "Epoch [20/25], Train Loss: 0.000145368481753394, Validation Loss: 0.00014478592590118448\n",
      "Epoch [20/25], Train Loss: 0.00012400276318658143, Validation Loss: 0.00014507125645953542\n",
      "Epoch [20/25], Train Loss: 0.0001529491419205442, Validation Loss: 0.00014508590635765964\n",
      "Epoch [20/25], Train Loss: 0.00012505603081081063, Validation Loss: 0.0001451351730793249\n",
      "Epoch [20/25], Train Loss: 0.00018593785353004932, Validation Loss: 0.00014502264578671506\n",
      "Epoch [20/25], Train Loss: 0.0001363654009765014, Validation Loss: 0.0001448214383951078\n",
      "Epoch [20/25], Train Loss: 0.00018071931845042855, Validation Loss: 0.00014478186446164424\n",
      "Epoch [20/25], Train Loss: 0.00017347470566164702, Validation Loss: 0.0001447660688427277\n",
      "Epoch [20/25], Train Loss: 0.00013283621228765696, Validation Loss: 0.00014480501898409177\n",
      "Epoch [20/25], Train Loss: 0.0001534610491944477, Validation Loss: 0.00014488862410265332\n",
      "Epoch [20/25], Train Loss: 0.0001586201397003606, Validation Loss: 0.00014498156985306803\n",
      "Epoch [20/25], Train Loss: 0.00016490003326907754, Validation Loss: 0.00014509224420180544\n",
      "Epoch [20/25], Train Loss: 0.00016232897178269923, Validation Loss: 0.00014509904309913205\n",
      "Epoch [20/25], Train Loss: 0.00011502499546622857, Validation Loss: 0.0001451133117370773\n",
      "Epoch [20/25], Train Loss: 0.00013887336535844952, Validation Loss: 0.00014504628585806738\n",
      "Epoch [20/25], Train Loss: 0.0001788951485650614, Validation Loss: 0.00014496251751552335\n",
      "Epoch [20/25], Train Loss: 0.0001560985838295892, Validation Loss: 0.00014504897262668237\n",
      "Epoch [20/25], Train Loss: 0.00019291277567390352, Validation Loss: 0.00014486203945125454\n",
      "Epoch [20/25], Train Loss: 0.00013221552944742143, Validation Loss: 0.00014478043716129225\n",
      "Epoch [20/25], Train Loss: 0.00021875747188460082, Validation Loss: 0.0001448025194501194\n",
      "Epoch [20/25], Train Loss: 0.00016588358266744763, Validation Loss: 0.00014494623998568084\n",
      "Epoch [20/25], Train Loss: 0.00016705786401871592, Validation Loss: 0.00014516242833148378\n",
      "Epoch [20/25], Train Loss: 0.0001141646716860123, Validation Loss: 0.00014516538261280705\n",
      "Epoch [20/25], Train Loss: 0.0001649181212997064, Validation Loss: 0.000145389503207601\n",
      "Epoch [20/25], Train Loss: 0.00015226130199152976, Validation Loss: 0.00014532683077656355\n",
      "Epoch [20/25], Train Loss: 0.00014927162555977702, Validation Loss: 0.0001450674084480852\n",
      "Epoch [20/25], Train Loss: 0.00019596070342231542, Validation Loss: 0.00014488374242015804\n",
      "Epoch [20/25], Train Loss: 0.0001839772885432467, Validation Loss: 0.00014464354438435597\n",
      "Epoch [20/25], Train Loss: 0.00013942709483671933, Validation Loss: 0.00014462644394370728\n",
      "Epoch [20/25], Train Loss: 0.0001651672355365008, Validation Loss: 0.00014473131377599203\n",
      "Epoch [20/25], Train Loss: 0.00013376481365412474, Validation Loss: 0.00014461260652751662\n",
      "Epoch [20/25], Train Loss: 0.00015252846060320735, Validation Loss: 0.00014464500394145336\n",
      "Epoch [20/25], Train Loss: 0.00010680007835617289, Validation Loss: 0.0001447270355129149\n",
      "Epoch [20/25], Train Loss: 0.00013006350491195917, Validation Loss: 0.00014457816214417108\n",
      "Epoch [20/25], Train Loss: 0.0001747415226418525, Validation Loss: 0.00014470760240025508\n",
      "Epoch [20/25], Train Loss: 0.00019255302322562784, Validation Loss: 0.00014466426994961997\n",
      "Epoch [20/25], Train Loss: 0.00018538911535870284, Validation Loss: 0.00014455910422839224\n",
      "Epoch [20/25], Train Loss: 0.00018546212231740355, Validation Loss: 0.00014461153211110892\n",
      "Epoch [20/25], Train Loss: 0.00018716449267230928, Validation Loss: 0.00014454364475871746\n",
      "Epoch [20/25], Train Loss: 0.00010619199019856751, Validation Loss: 0.00014449072211088302\n",
      "Epoch [20/25], Train Loss: 0.00012377132952678949, Validation Loss: 0.000144558992178645\n",
      "Epoch [20/25], Train Loss: 0.00017267580551560968, Validation Loss: 0.00014452635005000048\n",
      "Epoch [20/25], Train Loss: 0.00017014503828249872, Validation Loss: 0.00014457451567674676\n",
      "Epoch [20/25], Train Loss: 0.0001600022951606661, Validation Loss: 0.00014447456560446882\n",
      "Epoch [20/25], Train Loss: 0.00018445578461978585, Validation Loss: 0.0001445900694913386\n",
      "Epoch [21/25], Train Loss: 0.0001653145591262728, Validation Loss: 0.00014460073701532867\n",
      "Epoch [21/25], Train Loss: 0.0001468130067223683, Validation Loss: 0.0001445020553849948\n",
      "Epoch [21/25], Train Loss: 0.00020028544531669468, Validation Loss: 0.00014468287239045215\n",
      "Epoch [21/25], Train Loss: 0.0001398970780428499, Validation Loss: 0.00014452008569302657\n",
      "Epoch [21/25], Train Loss: 0.00022766381152905524, Validation Loss: 0.0001452206083437583\n",
      "Epoch [21/25], Train Loss: 0.00014742794155608863, Validation Loss: 0.00014504282080451958\n",
      "Epoch [21/25], Train Loss: 0.00012758380034938455, Validation Loss: 0.00014491489273495972\n",
      "Epoch [21/25], Train Loss: 0.000141531418194063, Validation Loss: 0.00014614390456699767\n",
      "Epoch [21/25], Train Loss: 0.0001574747439008206, Validation Loss: 0.00014660965437845636\n",
      "Epoch [21/25], Train Loss: 0.00016981683438643813, Validation Loss: 0.000149536584164404\n",
      "Epoch [21/25], Train Loss: 0.00016614363994449377, Validation Loss: 0.00015148936484668714\n",
      "Epoch [21/25], Train Loss: 0.00017326527449768037, Validation Loss: 0.00015112416876945646\n",
      "Epoch [21/25], Train Loss: 0.00012777278607245535, Validation Loss: 0.00014862997171197396\n",
      "Epoch [21/25], Train Loss: 0.00016450723342131823, Validation Loss: 0.00014601687241035203\n",
      "Epoch [21/25], Train Loss: 0.00017658456636127084, Validation Loss: 0.0001448247591421629\n",
      "Epoch [21/25], Train Loss: 0.00021005482994951308, Validation Loss: 0.00014613340050952198\n",
      "Epoch [21/25], Train Loss: 0.00019267287279944867, Validation Loss: 0.00014763983005347352\n",
      "Epoch [21/25], Train Loss: 0.00018086111231241375, Validation Loss: 0.00014794633170822635\n",
      "Epoch [21/25], Train Loss: 0.00015606623492203653, Validation Loss: 0.00014684104341237494\n",
      "Epoch [21/25], Train Loss: 0.00011559636186575517, Validation Loss: 0.00014513770147459582\n",
      "Epoch [21/25], Train Loss: 0.0001578376250108704, Validation Loss: 0.00014459279967316737\n",
      "Epoch [21/25], Train Loss: 0.00021593918791040778, Validation Loss: 0.00014508063613902778\n",
      "Epoch [21/25], Train Loss: 0.0001301514421356842, Validation Loss: 0.00014623263195971957\n",
      "Epoch [21/25], Train Loss: 0.00017605250468477607, Validation Loss: 0.00014825830779348811\n",
      "Epoch [21/25], Train Loss: 0.00020643534662667662, Validation Loss: 0.0001497264878707938\n",
      "Epoch [21/25], Train Loss: 0.0001187121742987074, Validation Loss: 0.00015224017649112891\n",
      "Epoch [21/25], Train Loss: 0.00019643994164653122, Validation Loss: 0.00015280834389462447\n",
      "Epoch [21/25], Train Loss: 0.00016539559874217957, Validation Loss: 0.0001531808569173639\n",
      "Epoch [21/25], Train Loss: 0.00019566138507798314, Validation Loss: 0.00014981957889782885\n",
      "Epoch [21/25], Train Loss: 0.00019045612134505063, Validation Loss: 0.00014628533099312336\n",
      "Epoch [21/25], Train Loss: 0.00019160506781190634, Validation Loss: 0.00014437638067950806\n",
      "Epoch [21/25], Train Loss: 0.00017582987493369728, Validation Loss: 0.00014572541428303036\n",
      "Epoch [21/25], Train Loss: 0.00014806055696681142, Validation Loss: 0.00014802309694156673\n",
      "Epoch [21/25], Train Loss: 0.00011191999510629103, Validation Loss: 0.00014758290538641934\n",
      "Epoch [21/25], Train Loss: 0.00020591368956957012, Validation Loss: 0.00014544530010122494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], Train Loss: 0.0001576370996190235, Validation Loss: 0.00014462310258143892\n",
      "Epoch [21/25], Train Loss: 0.0001389700046274811, Validation Loss: 0.0001454931819656243\n",
      "Epoch [21/25], Train Loss: 0.00016925497038755566, Validation Loss: 0.0001465487456395446\n",
      "Epoch [21/25], Train Loss: 0.00011023685510735959, Validation Loss: 0.00014600817521568388\n",
      "Epoch [21/25], Train Loss: 0.00019678474927786738, Validation Loss: 0.00014468574348332672\n",
      "Epoch [21/25], Train Loss: 0.00015980994794517756, Validation Loss: 0.0001445346524027021\n",
      "Epoch [21/25], Train Loss: 0.0001587679871590808, Validation Loss: 0.00014531488147137377\n",
      "Epoch [21/25], Train Loss: 0.0001574748457642272, Validation Loss: 0.00014571762148989363\n",
      "Epoch [21/25], Train Loss: 0.00014760802150703967, Validation Loss: 0.00014534592895264117\n",
      "Epoch [21/25], Train Loss: 0.0002172357781091705, Validation Loss: 0.00014475070420303383\n",
      "Epoch [21/25], Train Loss: 0.0002000209060497582, Validation Loss: 0.00014454102177599755\n",
      "Epoch [21/25], Train Loss: 0.00014211439702194184, Validation Loss: 0.0001445417069286729\n",
      "Epoch [21/25], Train Loss: 0.00013741936709266156, Validation Loss: 0.00014522761727372806\n",
      "Epoch [21/25], Train Loss: 0.00016056063759606332, Validation Loss: 0.0001451612663610528\n",
      "Epoch [21/25], Train Loss: 0.00014538035611622036, Validation Loss: 0.00014469987121022617\n",
      "Epoch [21/25], Train Loss: 0.00014412167365662754, Validation Loss: 0.00014470381502178497\n",
      "Epoch [21/25], Train Loss: 0.00014046025171410292, Validation Loss: 0.0001444122528482694\n",
      "Epoch [21/25], Train Loss: 0.00012396056263241917, Validation Loss: 0.00014435808601168294\n",
      "Epoch [21/25], Train Loss: 0.00016094134480226785, Validation Loss: 0.00014464518729558526\n",
      "Epoch [21/25], Train Loss: 0.00017069185560103506, Validation Loss: 0.00014468852386926301\n",
      "Epoch [21/25], Train Loss: 0.00016928058175835758, Validation Loss: 0.00014488372835330665\n",
      "Epoch [21/25], Train Loss: 0.0001170951290987432, Validation Loss: 0.00014498288073809816\n",
      "Epoch [21/25], Train Loss: 0.0001785180502338335, Validation Loss: 0.00014474945831655835\n",
      "Epoch [21/25], Train Loss: 0.0001662565191509202, Validation Loss: 0.00014452835336366358\n",
      "Epoch [21/25], Train Loss: 0.000157444053911604, Validation Loss: 0.00014448401877113307\n",
      "Epoch [21/25], Train Loss: 0.000192783132661134, Validation Loss: 0.00014444706418241063\n",
      "Epoch [21/25], Train Loss: 0.00012002078437944874, Validation Loss: 0.0001441930961542918\n",
      "Epoch [21/25], Train Loss: 0.00018878957780543715, Validation Loss: 0.00014432736643357202\n",
      "Epoch [21/25], Train Loss: 0.0001075559266610071, Validation Loss: 0.00014439018171591062\n",
      "Epoch [21/25], Train Loss: 0.00017585592286195606, Validation Loss: 0.0001443219203792978\n",
      "Epoch [21/25], Train Loss: 0.00013534820755012333, Validation Loss: 0.00014449134299259942\n",
      "Epoch [21/25], Train Loss: 0.0001673567749094218, Validation Loss: 0.00014448510846705177\n",
      "Epoch [21/25], Train Loss: 0.0002274436701554805, Validation Loss: 0.00014431070934127396\n",
      "Epoch [21/25], Train Loss: 0.00011373821325832978, Validation Loss: 0.0001443334753275849\n",
      "Epoch [21/25], Train Loss: 0.00014600138820242137, Validation Loss: 0.0001442201617464889\n",
      "Epoch [21/25], Train Loss: 0.00015835597878322005, Validation Loss: 0.0001441418954830927\n",
      "Epoch [21/25], Train Loss: 0.00016117197810672224, Validation Loss: 0.00014427377294244555\n",
      "Epoch [21/25], Train Loss: 0.00016671862977091223, Validation Loss: 0.00014414270723743053\n",
      "Epoch [21/25], Train Loss: 0.00017703103367239237, Validation Loss: 0.00014414641724821802\n",
      "Epoch [21/25], Train Loss: 0.00012948723451700062, Validation Loss: 0.00014428457313139613\n",
      "Epoch [21/25], Train Loss: 0.00013640610268339515, Validation Loss: 0.00014423950633499772\n",
      "Epoch [21/25], Train Loss: 0.0001713434758130461, Validation Loss: 0.00014431331413409983\n",
      "Epoch [21/25], Train Loss: 0.00017453249893151224, Validation Loss: 0.0001444204186554998\n",
      "Epoch [21/25], Train Loss: 0.00015922810416668653, Validation Loss: 0.00014434286107037526\n",
      "Epoch [21/25], Train Loss: 0.00017142137221526355, Validation Loss: 0.00014433066389756276\n",
      "Epoch [21/25], Train Loss: 0.00010245558951282874, Validation Loss: 0.00014424263329904837\n",
      "Epoch [21/25], Train Loss: 0.00013116924674250185, Validation Loss: 0.00014427775410392012\n",
      "Epoch [21/25], Train Loss: 0.00013533877790905535, Validation Loss: 0.00014431160598178395\n",
      "Epoch [21/25], Train Loss: 0.00013866201334167272, Validation Loss: 0.0001441895622216786\n",
      "Epoch [21/25], Train Loss: 0.0001252418733201921, Validation Loss: 0.0001442468700891671\n",
      "Epoch [21/25], Train Loss: 0.00016128418792504817, Validation Loss: 0.0001442104713836064\n",
      "Epoch [21/25], Train Loss: 0.0001285014150198549, Validation Loss: 0.00014424217879422941\n",
      "Epoch [21/25], Train Loss: 0.00015121004253160208, Validation Loss: 0.00014422374782346498\n",
      "Epoch [21/25], Train Loss: 0.000120500051707495, Validation Loss: 0.0001443108022309995\n",
      "Epoch [21/25], Train Loss: 0.00015034539683256298, Validation Loss: 0.00014422912063309922\n",
      "Epoch [21/25], Train Loss: 9.49304667301476e-05, Validation Loss: 0.00014413315099470007\n",
      "Epoch [21/25], Train Loss: 0.00017080495308618993, Validation Loss: 0.00014411223576947426\n",
      "Epoch [21/25], Train Loss: 0.00014593762170989066, Validation Loss: 0.00014410811660733696\n",
      "Epoch [21/25], Train Loss: 0.0002045732398983091, Validation Loss: 0.00015525750908030507\n",
      "Epoch [21/25], Train Loss: 0.00017967539315577596, Validation Loss: 0.0001645885179944647\n",
      "Epoch [21/25], Train Loss: 0.00020935485372319818, Validation Loss: 0.00018487945053493604\n",
      "Epoch [21/25], Train Loss: 0.00021649489644914865, Validation Loss: 0.00020491341322970887\n",
      "Epoch [21/25], Train Loss: 0.00019203050760552287, Validation Loss: 0.00018407148697103065\n",
      "Epoch [21/25], Train Loss: 0.00019553919264581054, Validation Loss: 0.00015624346342519858\n",
      "Epoch [21/25], Train Loss: 0.00012324149429332465, Validation Loss: 0.0001501281646293743\n",
      "Epoch [21/25], Train Loss: 0.00010553633910603821, Validation Loss: 0.00016930589699768462\n",
      "Epoch [21/25], Train Loss: 0.0002123047161148861, Validation Loss: 0.00016979589248270106\n",
      "Epoch [21/25], Train Loss: 0.00017988316540140659, Validation Loss: 0.00014829817470551158\n",
      "Epoch [21/25], Train Loss: 0.00012923721806146204, Validation Loss: 0.00015804773114117172\n",
      "Epoch [21/25], Train Loss: 0.0001398743479512632, Validation Loss: 0.00016778212617888735\n",
      "Epoch [21/25], Train Loss: 0.0001913915912155062, Validation Loss: 0.00014916679841311029\n",
      "Epoch [21/25], Train Loss: 0.0001422521163476631, Validation Loss: 0.00015554897909169084\n",
      "Epoch [21/25], Train Loss: 0.00014524640573654324, Validation Loss: 0.00016327384316052\n",
      "Epoch [21/25], Train Loss: 0.0002002437977353111, Validation Loss: 0.00014801915143228446\n",
      "Epoch [21/25], Train Loss: 0.0001380072790198028, Validation Loss: 0.00015570431326826414\n",
      "Epoch [21/25], Train Loss: 0.00022312341025099158, Validation Loss: 0.00015832442901834534\n",
      "Epoch [21/25], Train Loss: 0.00017434566689189523, Validation Loss: 0.00014706174139670716\n",
      "Epoch [21/25], Train Loss: 0.00014247908256947994, Validation Loss: 0.00015597575402352958\n",
      "Epoch [21/25], Train Loss: 0.00014649699733126909, Validation Loss: 0.00015525593238029008\n",
      "Epoch [21/25], Train Loss: 0.00019861359032802284, Validation Loss: 0.0001467526970373001\n",
      "Epoch [21/25], Train Loss: 0.00020401811343617737, Validation Loss: 0.0001556640653385936\n",
      "Epoch [21/25], Train Loss: 0.000207180404686369, Validation Loss: 0.00015342986638036866\n",
      "Epoch [21/25], Train Loss: 0.00016065215459093451, Validation Loss: 0.00014673759117916536\n",
      "Epoch [21/25], Train Loss: 0.00014833170280326158, Validation Loss: 0.00015498125479401398\n",
      "Epoch [21/25], Train Loss: 0.00010579654917819425, Validation Loss: 0.00015203897661801116\n",
      "Epoch [21/25], Train Loss: 0.00014472485054284334, Validation Loss: 0.0001462515749153681\n",
      "Epoch [21/25], Train Loss: 0.00017043473781086504, Validation Loss: 0.00015403541013559637\n",
      "Epoch [21/25], Train Loss: 0.00014829174324404448, Validation Loss: 0.00015160402447994176\n",
      "Epoch [21/25], Train Loss: 0.00018668246048036963, Validation Loss: 0.00014544722289429047\n",
      "Epoch [21/25], Train Loss: 0.0002451596374157816, Validation Loss: 0.00015241572643086935\n",
      "Epoch [21/25], Train Loss: 0.00014386406110133976, Validation Loss: 0.00015191074417089112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], Train Loss: 0.00014340273628477007, Validation Loss: 0.00014516641458612868\n",
      "Epoch [21/25], Train Loss: 0.000163257573149167, Validation Loss: 0.0001506706525105983\n",
      "Epoch [21/25], Train Loss: 0.00013196977670304477, Validation Loss: 0.00015191507579099078\n",
      "Epoch [21/25], Train Loss: 0.000154467998072505, Validation Loss: 0.00014529818314864922\n",
      "Epoch [21/25], Train Loss: 0.00011500323307700455, Validation Loss: 0.00014878656850972523\n",
      "Epoch [21/25], Train Loss: 0.00013968507118988782, Validation Loss: 0.00015137533143085117\n",
      "Epoch [21/25], Train Loss: 0.00012478728604037315, Validation Loss: 0.0001456500812006804\n",
      "Epoch [21/25], Train Loss: 0.0001535647315904498, Validation Loss: 0.00014689940289827063\n",
      "Epoch [21/25], Train Loss: 0.00017481351096648723, Validation Loss: 0.00015050361980684102\n",
      "Epoch [21/25], Train Loss: 0.00018211045244242996, Validation Loss: 0.00014622219678130932\n",
      "Epoch [21/25], Train Loss: 0.00012006497127003968, Validation Loss: 0.0001457307019639605\n",
      "Epoch [21/25], Train Loss: 0.00016506436804775149, Validation Loss: 0.00014892941374758568\n",
      "Epoch [21/25], Train Loss: 0.0001458492042729631, Validation Loss: 0.0001465468078095\n",
      "Epoch [21/25], Train Loss: 0.00012061579764122143, Validation Loss: 0.0001450708715613776\n",
      "Epoch [21/25], Train Loss: 0.00016211434558499604, Validation Loss: 0.00014742891095617476\n",
      "Epoch [21/25], Train Loss: 0.00015890052600298077, Validation Loss: 0.0001466705684530704\n",
      "Epoch [21/25], Train Loss: 0.00016490084817633033, Validation Loss: 0.00014498233213089406\n",
      "Epoch [21/25], Train Loss: 0.0001399976317770779, Validation Loss: 0.0001460604447250565\n",
      "Epoch [21/25], Train Loss: 0.0001348176592728123, Validation Loss: 0.0001463952770185036\n",
      "Epoch [21/25], Train Loss: 0.00011900171375600621, Validation Loss: 0.00014506040582394538\n",
      "Epoch [21/25], Train Loss: 0.0001485112152295187, Validation Loss: 0.0001452000200515613\n",
      "Epoch [21/25], Train Loss: 0.0001217819590237923, Validation Loss: 0.0001460504560479118\n",
      "Epoch [21/25], Train Loss: 0.00012910614896100014, Validation Loss: 0.0001453387643171785\n",
      "Epoch [21/25], Train Loss: 0.00012077781138941646, Validation Loss: 0.0001448845701816026\n",
      "Epoch [21/25], Train Loss: 9.988547390094027e-05, Validation Loss: 0.00014562322467099876\n",
      "Epoch [21/25], Train Loss: 0.00014699564781039953, Validation Loss: 0.0001454510379213995\n",
      "Epoch [21/25], Train Loss: 0.00020239401783328503, Validation Loss: 0.0001448187635105569\n",
      "Epoch [21/25], Train Loss: 0.00015724632248748094, Validation Loss: 0.00014525237274938264\n",
      "Epoch [21/25], Train Loss: 0.00014723828644491732, Validation Loss: 0.00014544771547662095\n",
      "Epoch [21/25], Train Loss: 0.0001134422782342881, Validation Loss: 0.0001448590048918656\n",
      "Epoch [21/25], Train Loss: 0.00017103490245062858, Validation Loss: 0.00014491547141612198\n",
      "Epoch [21/25], Train Loss: 0.00016273939399980009, Validation Loss: 0.00014532138399469356\n",
      "Epoch [21/25], Train Loss: 9.61279874900356e-05, Validation Loss: 0.0001449877747897214\n",
      "Epoch [21/25], Train Loss: 0.00019507053366396576, Validation Loss: 0.00014466170493202904\n",
      "Epoch [21/25], Train Loss: 0.00020803337974939495, Validation Loss: 0.0001449487487358662\n",
      "Epoch [21/25], Train Loss: 0.00016851360851433128, Validation Loss: 0.00014500757000253847\n",
      "Epoch [21/25], Train Loss: 0.00014795950846746564, Validation Loss: 0.00014467867246518532\n",
      "Epoch [21/25], Train Loss: 0.00010940533684333786, Validation Loss: 0.0001446304862232258\n",
      "Epoch [21/25], Train Loss: 0.00014920711691956967, Validation Loss: 0.00014480287791229784\n",
      "Epoch [21/25], Train Loss: 0.0001473928423365578, Validation Loss: 0.0001446673741156701\n",
      "Epoch [21/25], Train Loss: 0.00011593789531616494, Validation Loss: 0.0001444670917408075\n",
      "Epoch [21/25], Train Loss: 0.00019989623979199678, Validation Loss: 0.00014457792179503788\n",
      "Epoch [21/25], Train Loss: 0.00015979926683939993, Validation Loss: 0.0001449192153813783\n",
      "Epoch [21/25], Train Loss: 0.00012296656495891511, Validation Loss: 0.0001451389037053256\n",
      "Epoch [21/25], Train Loss: 9.505105117568746e-05, Validation Loss: 0.0001451562849979382\n",
      "Epoch [21/25], Train Loss: 0.00013972519082017243, Validation Loss: 0.00014469568098623615\n",
      "Epoch [21/25], Train Loss: 0.00013135171320755035, Validation Loss: 0.00014438664899595703\n",
      "Epoch [21/25], Train Loss: 0.00016148120630532503, Validation Loss: 0.0001445532798243221\n",
      "Epoch [21/25], Train Loss: 0.00019532459555193782, Validation Loss: 0.0001448449479842869\n",
      "Epoch [21/25], Train Loss: 0.00014403274690266699, Validation Loss: 0.00014496568765025585\n",
      "Epoch [21/25], Train Loss: 0.00018502457533031702, Validation Loss: 0.00014456197823164985\n",
      "Epoch [21/25], Train Loss: 0.00016419052553828806, Validation Loss: 0.0001443203720555175\n",
      "Epoch [21/25], Train Loss: 0.00012687355047091842, Validation Loss: 0.00014438200450967998\n",
      "Epoch [21/25], Train Loss: 0.00016469958063680679, Validation Loss: 0.00014450208667161253\n",
      "Epoch [21/25], Train Loss: 0.0001485361426603049, Validation Loss: 0.00014443299272291673\n",
      "Epoch [21/25], Train Loss: 0.00017400231445208192, Validation Loss: 0.0001442578989857187\n",
      "Epoch [21/25], Train Loss: 0.00015620223712176085, Validation Loss: 0.0001442886491228516\n",
      "Epoch [21/25], Train Loss: 0.00011427636491134763, Validation Loss: 0.000144356295641046\n",
      "Epoch [21/25], Train Loss: 0.0001920373470056802, Validation Loss: 0.0001442922965604036\n",
      "Epoch [21/25], Train Loss: 0.00014601722068618983, Validation Loss: 0.00014419597331046438\n",
      "Epoch [21/25], Train Loss: 0.0001873052679002285, Validation Loss: 0.00014426902683529382\n",
      "Epoch [21/25], Train Loss: 0.00012512838293332607, Validation Loss: 0.0001443030509108212\n",
      "Epoch [21/25], Train Loss: 0.0001698940177448094, Validation Loss: 0.00014423727479879745\n",
      "Epoch [21/25], Train Loss: 0.00013172681792639196, Validation Loss: 0.0001441978466270181\n",
      "Epoch [21/25], Train Loss: 9.670693543739617e-05, Validation Loss: 0.00014426404013647697\n",
      "Epoch [21/25], Train Loss: 0.00013850469258613884, Validation Loss: 0.0001442764592259967\n",
      "Epoch [21/25], Train Loss: 0.0001358826702926308, Validation Loss: 0.0001441824853827711\n",
      "Epoch [21/25], Train Loss: 0.00022112038277555257, Validation Loss: 0.00014419745857594535\n",
      "Epoch [21/25], Train Loss: 0.0001604435674380511, Validation Loss: 0.0001442323921461745\n",
      "Epoch [21/25], Train Loss: 0.00015817744133528322, Validation Loss: 0.00014419107562086236\n",
      "Epoch [21/25], Train Loss: 0.00014138719416223466, Validation Loss: 0.0001441858757364874\n",
      "Epoch [21/25], Train Loss: 0.00015886897745076567, Validation Loss: 0.0001441514294128865\n",
      "Epoch [21/25], Train Loss: 0.00011898111552000046, Validation Loss: 0.00014417712154681794\n",
      "Epoch [21/25], Train Loss: 0.0001272238150704652, Validation Loss: 0.0001441597232769709\n",
      "Epoch [21/25], Train Loss: 0.00015666920808143914, Validation Loss: 0.00014416685565568816\n",
      "Epoch [21/25], Train Loss: 0.00014323087816592306, Validation Loss: 0.0001441715151789443\n",
      "Epoch [21/25], Train Loss: 0.0001519546494819224, Validation Loss: 0.00014414200729030805\n",
      "Epoch [21/25], Train Loss: 0.00012919071014039218, Validation Loss: 0.00014413528406294064\n",
      "Epoch [21/25], Train Loss: 0.00020230869995430112, Validation Loss: 0.000144130571182662\n",
      "Epoch [21/25], Train Loss: 0.000157490125275217, Validation Loss: 0.0001441362396387073\n",
      "Epoch [21/25], Train Loss: 0.0001366139476886019, Validation Loss: 0.00014413527605938725\n",
      "Epoch [21/25], Train Loss: 0.00016594631597399712, Validation Loss: 0.00014416363216393318\n",
      "Epoch [21/25], Train Loss: 0.00013576641504187137, Validation Loss: 0.00014416981260486258\n",
      "Epoch [21/25], Train Loss: 0.00018359786190558225, Validation Loss: 0.00014413117848259086\n",
      "Epoch [21/25], Train Loss: 0.00016296168905682862, Validation Loss: 0.00014419688886846416\n",
      "Epoch [21/25], Train Loss: 0.00017832851153798401, Validation Loss: 0.00014430503457939873\n",
      "Epoch [21/25], Train Loss: 0.00017318289610557258, Validation Loss: 0.00014420395988660553\n",
      "Epoch [21/25], Train Loss: 0.00016002437041606754, Validation Loss: 0.00014411577212740666\n",
      "Epoch [21/25], Train Loss: 0.00016288980259560049, Validation Loss: 0.0001442194453071958\n",
      "Epoch [21/25], Train Loss: 8.690768299857154e-05, Validation Loss: 0.00014427005614076432\n",
      "Epoch [21/25], Train Loss: 0.00023885049449745566, Validation Loss: 0.00014415661717066542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], Train Loss: 0.0001796217547962442, Validation Loss: 0.0001442782682715915\n",
      "Epoch [21/25], Train Loss: 0.0001509779685875401, Validation Loss: 0.0001443299394547163\n",
      "Epoch [21/25], Train Loss: 0.00015288955182768404, Validation Loss: 0.0001441687338228803\n",
      "Epoch [21/25], Train Loss: 0.0001729268697090447, Validation Loss: 0.00014431488186043376\n",
      "Epoch [21/25], Train Loss: 0.00014831923181191087, Validation Loss: 0.00014430204707120234\n",
      "Epoch [21/25], Train Loss: 0.00013388012303039432, Validation Loss: 0.00014421824453165756\n",
      "Epoch [21/25], Train Loss: 0.00012041953596053645, Validation Loss: 0.0001443794021421733\n",
      "Epoch [21/25], Train Loss: 0.00014639689470641315, Validation Loss: 0.0001443470976179621\n",
      "Epoch [21/25], Train Loss: 0.00017525900329928845, Validation Loss: 0.00014422046006075108\n",
      "Epoch [21/25], Train Loss: 0.00015818803512956947, Validation Loss: 0.00014424492183024996\n",
      "Epoch [21/25], Train Loss: 0.00014390864816959947, Validation Loss: 0.00014418941912784552\n",
      "Epoch [21/25], Train Loss: 0.00014044661656953394, Validation Loss: 0.00014414513449689063\n",
      "Epoch [21/25], Train Loss: 0.00020625877368729562, Validation Loss: 0.00014428404732219253\n",
      "Epoch [21/25], Train Loss: 0.00012450468784663826, Validation Loss: 0.00014429084500685956\n",
      "Epoch [21/25], Train Loss: 0.00017032188770826906, Validation Loss: 0.00014414720111138497\n",
      "Epoch [21/25], Train Loss: 0.00013499095803126693, Validation Loss: 0.00014418692371691576\n",
      "Epoch [21/25], Train Loss: 0.0001491575822001323, Validation Loss: 0.00014416449339478278\n",
      "Epoch [21/25], Train Loss: 0.0001886606914922595, Validation Loss: 0.00014411487233398172\n",
      "Epoch [21/25], Train Loss: 0.00016738876001909375, Validation Loss: 0.00014412535941422296\n",
      "Epoch [21/25], Train Loss: 0.00015302877000067383, Validation Loss: 0.00014412389282369986\n",
      "Epoch [21/25], Train Loss: 0.00019137041817884892, Validation Loss: 0.00014409378588122005\n",
      "Epoch [21/25], Train Loss: 0.00013674462388735265, Validation Loss: 0.00014409052673727273\n",
      "Epoch [21/25], Train Loss: 0.0001327080390183255, Validation Loss: 0.00014411363954422996\n",
      "Epoch [21/25], Train Loss: 0.00013647647574543953, Validation Loss: 0.0001440946395935801\n",
      "Epoch [21/25], Train Loss: 0.00014965930313337594, Validation Loss: 0.00014415504095571425\n",
      "Epoch [21/25], Train Loss: 0.00013200675311964005, Validation Loss: 0.00014417675168563923\n",
      "Epoch [21/25], Train Loss: 0.0001736385893309489, Validation Loss: 0.000144095274299616\n",
      "Epoch [21/25], Train Loss: 0.0001644284784561023, Validation Loss: 0.0001441501452063676\n",
      "Epoch [21/25], Train Loss: 0.00012064517068210989, Validation Loss: 0.00014415956029552036\n",
      "Epoch [21/25], Train Loss: 0.0001959206274477765, Validation Loss: 0.00014418270608681876\n",
      "Epoch [21/25], Train Loss: 0.00021286691480781883, Validation Loss: 0.00014417151032830589\n",
      "Epoch [21/25], Train Loss: 0.00015864321903791279, Validation Loss: 0.00014411579322768376\n",
      "Epoch [21/25], Train Loss: 0.00023698229051660746, Validation Loss: 0.00014418666614801623\n",
      "Epoch [21/25], Train Loss: 0.00014281364565249532, Validation Loss: 0.00014408933081237288\n",
      "Epoch [21/25], Train Loss: 0.00012541573960334063, Validation Loss: 0.0001441155839226364\n",
      "Epoch [21/25], Train Loss: 0.00018353220366407186, Validation Loss: 0.00014414356677055668\n",
      "Epoch [21/25], Train Loss: 0.0001601735275471583, Validation Loss: 0.00014409837046211276\n",
      "Epoch [21/25], Train Loss: 0.00013057401520200074, Validation Loss: 0.0001441021811236472\n",
      "Epoch [21/25], Train Loss: 0.00013951904838904738, Validation Loss: 0.0001441269698261749\n",
      "Epoch [21/25], Train Loss: 0.00011656185961328447, Validation Loss: 0.00014408577529441877\n",
      "Epoch [21/25], Train Loss: 0.00014129212650004774, Validation Loss: 0.00014410149863882302\n",
      "Epoch [21/25], Train Loss: 0.00012959477317053825, Validation Loss: 0.00014418608916457742\n",
      "Epoch [21/25], Train Loss: 0.0001494886091677472, Validation Loss: 0.00014416765479836614\n",
      "Epoch [21/25], Train Loss: 0.0001471890864195302, Validation Loss: 0.00014410426156246102\n",
      "Epoch [21/25], Train Loss: 0.00017255170678254217, Validation Loss: 0.00014440113300224767\n",
      "Epoch [21/25], Train Loss: 0.0001767996873240918, Validation Loss: 0.00014444637927226722\n",
      "Epoch [21/25], Train Loss: 0.00015302558313123882, Validation Loss: 0.00014418551242367054\n",
      "Epoch [21/25], Train Loss: 0.000133752022520639, Validation Loss: 0.00014431430123901617\n",
      "Epoch [21/25], Train Loss: 0.0001442133798263967, Validation Loss: 0.0001444047595820545\n",
      "Epoch [21/25], Train Loss: 0.00010522804223001003, Validation Loss: 0.0001441804061566169\n",
      "Epoch [21/25], Train Loss: 0.00018535004346631467, Validation Loss: 0.00014435679840971716\n",
      "Epoch [22/25], Train Loss: 0.000151482701767236, Validation Loss: 0.00014443058801892524\n",
      "Epoch [22/25], Train Loss: 0.00015835525118745863, Validation Loss: 0.00014422566091525367\n",
      "Epoch [22/25], Train Loss: 0.0001852039131335914, Validation Loss: 0.0001442944669785599\n",
      "Epoch [22/25], Train Loss: 8.243750198744237e-05, Validation Loss: 0.00014432475703263966\n",
      "Epoch [22/25], Train Loss: 0.00014931666373740882, Validation Loss: 0.00014423655787444052\n",
      "Epoch [22/25], Train Loss: 0.00014969284529797733, Validation Loss: 0.00014428928940712164\n",
      "Epoch [22/25], Train Loss: 0.00015826251183170825, Validation Loss: 0.00014418339778785594\n",
      "Epoch [22/25], Train Loss: 0.00012654814054258168, Validation Loss: 0.00014420974111999384\n",
      "Epoch [22/25], Train Loss: 0.00012749791494570673, Validation Loss: 0.00014431011198515382\n",
      "Epoch [22/25], Train Loss: 0.00014260111493058503, Validation Loss: 0.00014422353245511962\n",
      "Epoch [22/25], Train Loss: 0.00019635766511783004, Validation Loss: 0.00014423233345344972\n",
      "Epoch [22/25], Train Loss: 0.0001479039347032085, Validation Loss: 0.00014421035860626337\n",
      "Epoch [22/25], Train Loss: 0.000165687088156119, Validation Loss: 0.0001441905265285944\n",
      "Epoch [22/25], Train Loss: 0.00011519930558279157, Validation Loss: 0.0001441918320779223\n",
      "Epoch [22/25], Train Loss: 0.00014328218821901828, Validation Loss: 0.00014421930366855426\n",
      "Epoch [22/25], Train Loss: 0.0001477967161918059, Validation Loss: 0.0001441709015731855\n",
      "Epoch [22/25], Train Loss: 0.00014198430289980024, Validation Loss: 0.0001441800304746721\n",
      "Epoch [22/25], Train Loss: 0.00012012340448563918, Validation Loss: 0.00014426924875200105\n",
      "Epoch [22/25], Train Loss: 0.0001912772422656417, Validation Loss: 0.00014416221868790064\n",
      "Epoch [22/25], Train Loss: 0.00017579297127667814, Validation Loss: 0.00014410048849337423\n",
      "Epoch [22/25], Train Loss: 0.00021073664538562298, Validation Loss: 0.00014426559209823608\n",
      "Epoch [22/25], Train Loss: 0.00010964318789774552, Validation Loss: 0.00014422181654178228\n",
      "Epoch [22/25], Train Loss: 0.00021545084018725902, Validation Loss: 0.0001441401279104563\n",
      "Epoch [22/25], Train Loss: 0.00014081786503084004, Validation Loss: 0.000144397926730259\n",
      "Epoch [22/25], Train Loss: 0.00013374185073189437, Validation Loss: 0.0001442303109797649\n",
      "Epoch [22/25], Train Loss: 0.00015880767023190856, Validation Loss: 0.00014422950383353357\n",
      "Epoch [22/25], Train Loss: 0.00014473946066573262, Validation Loss: 0.00014438891181877504\n",
      "Epoch [22/25], Train Loss: 0.00012707809219136834, Validation Loss: 0.00014431140783320492\n",
      "Epoch [22/25], Train Loss: 0.0001588075392646715, Validation Loss: 0.0001441682279012942\n",
      "Epoch [22/25], Train Loss: 0.00017280940664932132, Validation Loss: 0.0001442208524773984\n",
      "Epoch [22/25], Train Loss: 0.00015576051373500377, Validation Loss: 0.00014420476339485806\n",
      "Epoch [22/25], Train Loss: 0.0001762875181157142, Validation Loss: 0.00014416098153257432\n",
      "Epoch [22/25], Train Loss: 0.00011484259448479861, Validation Loss: 0.00014423875327338465\n",
      "Epoch [22/25], Train Loss: 0.00013074162416160107, Validation Loss: 0.00014423507406415107\n",
      "Epoch [22/25], Train Loss: 0.00013249095354694873, Validation Loss: 0.00014411551286078368\n",
      "Epoch [22/25], Train Loss: 0.0002018181694438681, Validation Loss: 0.00014411487063625827\n",
      "Epoch [22/25], Train Loss: 0.00017006209236569703, Validation Loss: 0.00014423559259739703\n",
      "Epoch [22/25], Train Loss: 0.00014019526133779436, Validation Loss: 0.00014420031390424508\n",
      "Epoch [22/25], Train Loss: 0.00014814910537097603, Validation Loss: 0.00014420917359529993\n",
      "Epoch [22/25], Train Loss: 0.00014766931417398155, Validation Loss: 0.000144184644886991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], Train Loss: 0.00015301405801437795, Validation Loss: 0.00014411597585421986\n",
      "Epoch [22/25], Train Loss: 0.00010386513167759404, Validation Loss: 0.00014410965911035116\n",
      "Epoch [22/25], Train Loss: 0.00011017010547220707, Validation Loss: 0.00014407657023790915\n",
      "Epoch [22/25], Train Loss: 0.0001415337173966691, Validation Loss: 0.00014407582542238137\n",
      "Epoch [22/25], Train Loss: 0.0001354848500341177, Validation Loss: 0.0001440915582255305\n",
      "Epoch [22/25], Train Loss: 0.0001767056091921404, Validation Loss: 0.00014408912878328313\n",
      "Epoch [22/25], Train Loss: 0.00011064670252380893, Validation Loss: 0.00014408859497052617\n",
      "Epoch [22/25], Train Loss: 0.00014822349476162344, Validation Loss: 0.00014405745459953324\n",
      "Epoch [22/25], Train Loss: 0.0001509407302364707, Validation Loss: 0.00014402904780581595\n",
      "Epoch [22/25], Train Loss: 0.00017424691759515554, Validation Loss: 0.00014406527698156423\n",
      "Epoch [22/25], Train Loss: 0.0001419564214302227, Validation Loss: 0.0001440538673098975\n",
      "Epoch [22/25], Train Loss: 0.00015060784062370658, Validation Loss: 0.00014417452402994967\n",
      "Epoch [22/25], Train Loss: 0.00017809518612921238, Validation Loss: 0.0001448208805716907\n",
      "Epoch [22/25], Train Loss: 0.00014433558681048453, Validation Loss: 0.000147161963347268\n",
      "Epoch [22/25], Train Loss: 0.00014948996249586344, Validation Loss: 0.000149045757765028\n",
      "Epoch [22/25], Train Loss: 0.0001743994653224945, Validation Loss: 0.0001474629583147665\n",
      "Epoch [22/25], Train Loss: 0.00016694336954969913, Validation Loss: 0.00014450619977045184\n",
      "Epoch [22/25], Train Loss: 0.00010123348329216242, Validation Loss: 0.0001445501038688235\n",
      "Epoch [22/25], Train Loss: 0.0001592557819094509, Validation Loss: 0.00014656190372382602\n",
      "Epoch [22/25], Train Loss: 0.00018041959265246987, Validation Loss: 0.00014671162449909995\n",
      "Epoch [22/25], Train Loss: 0.00015858204278629273, Validation Loss: 0.0001450760268198792\n",
      "Epoch [22/25], Train Loss: 0.0001843631180236116, Validation Loss: 0.00014402116309308137\n",
      "Epoch [22/25], Train Loss: 0.00016627561126369983, Validation Loss: 0.00014501563976712835\n",
      "Epoch [22/25], Train Loss: 0.0001753167889546603, Validation Loss: 0.00014604070505204922\n",
      "Epoch [22/25], Train Loss: 0.00016483146464452147, Validation Loss: 0.00014526497567809808\n",
      "Epoch [22/25], Train Loss: 0.0001801296166377142, Validation Loss: 0.00014417828448737662\n",
      "Epoch [22/25], Train Loss: 0.0001446846144972369, Validation Loss: 0.0001443184752133675\n",
      "Epoch [22/25], Train Loss: 0.00011794649617513642, Validation Loss: 0.0001451285344955977\n",
      "Epoch [22/25], Train Loss: 0.0001784143823897466, Validation Loss: 0.0001452804426662624\n",
      "Epoch [22/25], Train Loss: 0.00014938450476620346, Validation Loss: 0.0001445699822700893\n",
      "Epoch [22/25], Train Loss: 0.00015922320017125458, Validation Loss: 0.00014405756373889745\n",
      "Epoch [22/25], Train Loss: 0.00013711812789551914, Validation Loss: 0.0001443921845445099\n",
      "Epoch [22/25], Train Loss: 0.00013283039152156562, Validation Loss: 0.00014493769558612257\n",
      "Epoch [22/25], Train Loss: 0.00016848384984768927, Validation Loss: 0.00014473026045986142\n",
      "Epoch [22/25], Train Loss: 0.00014351615391205996, Validation Loss: 0.00014425031719535278\n",
      "Epoch [22/25], Train Loss: 9.912884706864133e-05, Validation Loss: 0.00014412922561556722\n",
      "Epoch [22/25], Train Loss: 7.97735556261614e-05, Validation Loss: 0.00014431473149064308\n",
      "Epoch [22/25], Train Loss: 0.00017546079470776021, Validation Loss: 0.00014460679279485096\n",
      "Epoch [22/25], Train Loss: 0.00010947951523121446, Validation Loss: 0.0001446152848075144\n",
      "Epoch [22/25], Train Loss: 0.00017515636864118278, Validation Loss: 0.00014420199440792202\n",
      "Epoch [22/25], Train Loss: 0.00019007452647201717, Validation Loss: 0.00014402451464169038\n",
      "Epoch [22/25], Train Loss: 0.00012733596668113023, Validation Loss: 0.00014426690759137273\n",
      "Epoch [22/25], Train Loss: 0.0001701857545413077, Validation Loss: 0.00014448805450228974\n",
      "Epoch [22/25], Train Loss: 0.00018455206009093672, Validation Loss: 0.00014448507718043402\n",
      "Epoch [22/25], Train Loss: 0.00013164087431505322, Validation Loss: 0.0001442797193400717\n",
      "Epoch [22/25], Train Loss: 0.00016189541202038527, Validation Loss: 0.00014403210589080117\n",
      "Epoch [22/25], Train Loss: 0.00023704749764874578, Validation Loss: 0.00014428730404082065\n",
      "Epoch [22/25], Train Loss: 0.00015638087643310428, Validation Loss: 0.00014433368560275994\n",
      "Epoch [22/25], Train Loss: 0.00021002974244765937, Validation Loss: 0.0001442993052478414\n",
      "Epoch [22/25], Train Loss: 0.00014598471170756966, Validation Loss: 0.00014438133051347297\n",
      "Epoch [22/25], Train Loss: 0.00017503637354820967, Validation Loss: 0.00014408669594558887\n",
      "Epoch [22/25], Train Loss: 0.00010434065916342661, Validation Loss: 0.00014416255774752547\n",
      "Epoch [22/25], Train Loss: 0.0001500406360719353, Validation Loss: 0.0001442598906578496\n",
      "Epoch [22/25], Train Loss: 0.00016939995111897588, Validation Loss: 0.0001442302030530603\n",
      "Epoch [22/25], Train Loss: 0.00014930653560440987, Validation Loss: 0.00014423869045761725\n",
      "Epoch [22/25], Train Loss: 0.0001791188697097823, Validation Loss: 0.0001441675216483418\n",
      "Epoch [22/25], Train Loss: 0.00012921725283376873, Validation Loss: 0.00014415446882291386\n",
      "Epoch [22/25], Train Loss: 0.0001320938317803666, Validation Loss: 0.00014412070013349875\n",
      "Epoch [22/25], Train Loss: 0.00013924945960752666, Validation Loss: 0.00014417560208433619\n",
      "Epoch [22/25], Train Loss: 0.0002137329865945503, Validation Loss: 0.00014426410149705285\n",
      "Epoch [22/25], Train Loss: 0.0002435454516671598, Validation Loss: 0.00014417336909294438\n",
      "Epoch [22/25], Train Loss: 0.000140378309879452, Validation Loss: 0.0001441339057540366\n",
      "Epoch [22/25], Train Loss: 0.00013130574370734394, Validation Loss: 0.00014424451073864476\n",
      "Epoch [22/25], Train Loss: 0.00014188150817062706, Validation Loss: 0.00014415624658189094\n",
      "Epoch [22/25], Train Loss: 0.00016474771837238222, Validation Loss: 0.00014424669352592901\n",
      "Epoch [22/25], Train Loss: 0.0001510738511569798, Validation Loss: 0.00014422122403630057\n",
      "Epoch [22/25], Train Loss: 0.00012348585005383939, Validation Loss: 0.00014414276665775105\n",
      "Epoch [22/25], Train Loss: 0.00016073659935500473, Validation Loss: 0.0001441256773735707\n",
      "Epoch [22/25], Train Loss: 0.00019560620421543717, Validation Loss: 0.00014422715588201148\n",
      "Epoch [22/25], Train Loss: 0.00018366510630585253, Validation Loss: 0.00014420195439015515\n",
      "Epoch [22/25], Train Loss: 0.00012916610285174102, Validation Loss: 0.00014411026010445008\n",
      "Epoch [22/25], Train Loss: 0.00021522252063732594, Validation Loss: 0.0001441901379924578\n",
      "Epoch [22/25], Train Loss: 0.00014593725791200995, Validation Loss: 0.00014413671888178215\n",
      "Epoch [22/25], Train Loss: 0.00016490880807396024, Validation Loss: 0.0001440402139754345\n",
      "Epoch [22/25], Train Loss: 0.00012348205200396478, Validation Loss: 0.0001440892551424137\n",
      "Epoch [22/25], Train Loss: 0.00016275855887215585, Validation Loss: 0.00014413094395422378\n",
      "Epoch [22/25], Train Loss: 0.00012136596342315897, Validation Loss: 0.00014402533148919852\n",
      "Epoch [22/25], Train Loss: 0.00014885967539157718, Validation Loss: 0.0001440523876226507\n",
      "Epoch [22/25], Train Loss: 9.803356078919023e-05, Validation Loss: 0.0001441244370653294\n",
      "Epoch [22/25], Train Loss: 0.00011991677456535399, Validation Loss: 0.0001440731781864694\n",
      "Epoch [22/25], Train Loss: 0.0001091403464670293, Validation Loss: 0.00014404214816750028\n",
      "Epoch [22/25], Train Loss: 0.00016790830704849213, Validation Loss: 0.00014415670060164605\n",
      "Epoch [22/25], Train Loss: 0.00013106454571243376, Validation Loss: 0.00014415231247160893\n",
      "Epoch [22/25], Train Loss: 0.0001410722325090319, Validation Loss: 0.00014412734793343892\n",
      "Epoch [22/25], Train Loss: 0.000139513416797854, Validation Loss: 0.00014418756206093045\n",
      "Epoch [22/25], Train Loss: 0.00017170206410810351, Validation Loss: 0.00014416050762520172\n",
      "Epoch [22/25], Train Loss: 0.00013150586164556444, Validation Loss: 0.0001441113359760493\n",
      "Epoch [22/25], Train Loss: 0.00016927154501900077, Validation Loss: 0.0001442483080609236\n",
      "Epoch [22/25], Train Loss: 0.00011819383507827297, Validation Loss: 0.00014420024259986046\n",
      "Epoch [22/25], Train Loss: 0.00015097518917173147, Validation Loss: 0.0001441321585540815\n",
      "Epoch [22/25], Train Loss: 0.00016479296027682722, Validation Loss: 0.00014406978783275312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], Train Loss: 0.00018370871839579195, Validation Loss: 0.00014419157426649084\n",
      "Epoch [22/25], Train Loss: 0.00017907272558659315, Validation Loss: 0.00014411298446551275\n",
      "Epoch [22/25], Train Loss: 0.00015549974341411144, Validation Loss: 0.00014401208221291503\n",
      "Epoch [22/25], Train Loss: 0.00019120294018648565, Validation Loss: 0.00014432309205100562\n",
      "Epoch [22/25], Train Loss: 0.00017247286450583488, Validation Loss: 0.00014414039615076035\n",
      "Epoch [22/25], Train Loss: 0.0001458135520806536, Validation Loss: 0.00014411308050815325\n",
      "Epoch [22/25], Train Loss: 0.0001711963559500873, Validation Loss: 0.00014445157648879103\n",
      "Epoch [22/25], Train Loss: 0.00013031542766839266, Validation Loss: 0.00014414868176875945\n",
      "Epoch [22/25], Train Loss: 0.00015316544158849865, Validation Loss: 0.00014406356179582266\n",
      "Epoch [22/25], Train Loss: 0.00020130295888520777, Validation Loss: 0.00014408455729911414\n",
      "Epoch [22/25], Train Loss: 0.00015723667456768453, Validation Loss: 0.0001441950742446352\n",
      "Epoch [22/25], Train Loss: 0.0001682526053627953, Validation Loss: 0.00014406677558630084\n",
      "Epoch [22/25], Train Loss: 0.00014913025370333344, Validation Loss: 0.0001441427766015598\n",
      "Epoch [22/25], Train Loss: 0.00013714948727283627, Validation Loss: 0.00014416503545362503\n",
      "Epoch [22/25], Train Loss: 0.00019282425637356937, Validation Loss: 0.000144063216187836\n",
      "Epoch [22/25], Train Loss: 0.0001603986311238259, Validation Loss: 0.00014415420785856742\n",
      "Epoch [22/25], Train Loss: 0.0001855822920333594, Validation Loss: 0.00014417131775796104\n",
      "Epoch [22/25], Train Loss: 0.0001795598800526932, Validation Loss: 0.00014398110943147913\n",
      "Epoch [22/25], Train Loss: 0.0001404861395712942, Validation Loss: 0.0001440808215799431\n",
      "Epoch [22/25], Train Loss: 0.00014337066386360675, Validation Loss: 0.00014405666418800442\n",
      "Epoch [22/25], Train Loss: 0.00012945386697538197, Validation Loss: 0.0001439887499145698\n",
      "Epoch [22/25], Train Loss: 0.00014058762462809682, Validation Loss: 0.00014407493484516938\n",
      "Epoch [22/25], Train Loss: 0.0001327101344941184, Validation Loss: 0.00014401528702971214\n",
      "Epoch [22/25], Train Loss: 0.00012080474698450416, Validation Loss: 0.00014411276521665665\n",
      "Epoch [22/25], Train Loss: 0.0001789252128219232, Validation Loss: 0.00014410992783571905\n",
      "Epoch [22/25], Train Loss: 0.0001714692043606192, Validation Loss: 0.00014398779943197343\n",
      "Epoch [22/25], Train Loss: 0.00015073335089255124, Validation Loss: 0.00014417181688865336\n",
      "Epoch [22/25], Train Loss: 0.00020218428107909858, Validation Loss: 0.00014415117911994457\n",
      "Epoch [22/25], Train Loss: 0.000197677465621382, Validation Loss: 0.00014409786114507976\n",
      "Epoch [22/25], Train Loss: 0.00016000530740711838, Validation Loss: 0.00014422062304220162\n",
      "Epoch [22/25], Train Loss: 0.00017633388051763177, Validation Loss: 0.0001441359221644234\n",
      "Epoch [22/25], Train Loss: 9.408512414665893e-05, Validation Loss: 0.0001442034415958915\n",
      "Epoch [22/25], Train Loss: 0.00019324198365211487, Validation Loss: 0.00014423827863841628\n",
      "Epoch [22/25], Train Loss: 0.00014795568131376058, Validation Loss: 0.00014419128807882469\n",
      "Epoch [22/25], Train Loss: 0.00014527850726153702, Validation Loss: 0.000144207776611438\n",
      "Epoch [22/25], Train Loss: 0.00019829804659821093, Validation Loss: 0.00014417362011348207\n",
      "Epoch [22/25], Train Loss: 0.0001794095733202994, Validation Loss: 0.0001442857161843373\n",
      "Epoch [22/25], Train Loss: 0.00010358459985582158, Validation Loss: 0.0001441684304154478\n",
      "Epoch [22/25], Train Loss: 0.0001747425994835794, Validation Loss: 0.00014405060016239684\n",
      "Epoch [22/25], Train Loss: 0.00014986086171120405, Validation Loss: 0.00014417000663039894\n",
      "Epoch [22/25], Train Loss: 0.00016758065612521023, Validation Loss: 0.00014414374260619903\n",
      "Epoch [22/25], Train Loss: 7.640954572707415e-05, Validation Loss: 0.00014405185744787257\n",
      "Epoch [22/25], Train Loss: 0.00013713407679460943, Validation Loss: 0.0001441336937811381\n",
      "Epoch [22/25], Train Loss: 0.00011759428889490664, Validation Loss: 0.00014411840214355228\n",
      "Epoch [22/25], Train Loss: 0.00019521785725373775, Validation Loss: 0.00014403545816700595\n",
      "Epoch [22/25], Train Loss: 0.00014671665849164128, Validation Loss: 0.00014414675606531092\n",
      "Epoch [22/25], Train Loss: 0.0001512542658019811, Validation Loss: 0.00014415471620547274\n",
      "Epoch [22/25], Train Loss: 0.0001774479023879394, Validation Loss: 0.0001440737243683543\n",
      "Epoch [22/25], Train Loss: 0.0001785416534403339, Validation Loss: 0.00014411247660367127\n",
      "Epoch [22/25], Train Loss: 0.00015672747395001352, Validation Loss: 0.00014406816165622635\n",
      "Epoch [22/25], Train Loss: 0.000161685558850877, Validation Loss: 0.00014404671237571164\n",
      "Epoch [22/25], Train Loss: 0.0002003789268201217, Validation Loss: 0.00014414195878392395\n",
      "Epoch [22/25], Train Loss: 0.0001596654037712142, Validation Loss: 0.00014414232985776227\n",
      "Epoch [22/25], Train Loss: 0.00013878574827685952, Validation Loss: 0.00014407452666394724\n",
      "Epoch [22/25], Train Loss: 0.00015706225531175733, Validation Loss: 0.00014416738704312593\n",
      "Epoch [22/25], Train Loss: 0.00014062268019188195, Validation Loss: 0.00014413613462238573\n",
      "Epoch [22/25], Train Loss: 0.00010007405217038468, Validation Loss: 0.00014406423191151891\n",
      "Epoch [22/25], Train Loss: 0.0001258061092812568, Validation Loss: 0.0001440844833268784\n",
      "Epoch [22/25], Train Loss: 0.00012172735296189785, Validation Loss: 0.00014413627553343152\n",
      "Epoch [22/25], Train Loss: 0.0001139765081461519, Validation Loss: 0.00014406156672824483\n",
      "Epoch [22/25], Train Loss: 0.00017635402036830783, Validation Loss: 0.0001440694856379802\n",
      "Epoch [22/25], Train Loss: 0.0001903929078252986, Validation Loss: 0.00014411610997437188\n",
      "Epoch [22/25], Train Loss: 0.0001271254732273519, Validation Loss: 0.00014402901845945356\n",
      "Epoch [22/25], Train Loss: 0.0001280126307392493, Validation Loss: 0.00014403972163563595\n",
      "Epoch [22/25], Train Loss: 0.00013002839114051312, Validation Loss: 0.00014411128746966522\n",
      "Epoch [22/25], Train Loss: 0.00020020347437821329, Validation Loss: 0.00014403698830089222\n",
      "Epoch [22/25], Train Loss: 0.00017200427828356624, Validation Loss: 0.0001439911618945189\n",
      "Epoch [22/25], Train Loss: 0.00015334045747295022, Validation Loss: 0.00014406811096705495\n",
      "Epoch [22/25], Train Loss: 0.00016027198580559343, Validation Loss: 0.00014399319576720396\n",
      "Epoch [22/25], Train Loss: 0.00021348513837438077, Validation Loss: 0.0001439425080510167\n",
      "Epoch [22/25], Train Loss: 0.00013652574853040278, Validation Loss: 0.00014396648305895117\n",
      "Epoch [22/25], Train Loss: 0.00013681281416211277, Validation Loss: 0.00014397209597518667\n",
      "Epoch [22/25], Train Loss: 0.00016460152983199805, Validation Loss: 0.00014401105387757223\n",
      "Epoch [22/25], Train Loss: 0.00016639578097965568, Validation Loss: 0.0001439568606050064\n",
      "Epoch [22/25], Train Loss: 0.0001554079499328509, Validation Loss: 0.00014397156167736587\n",
      "Epoch [22/25], Train Loss: 0.00015099842858035117, Validation Loss: 0.00014407073855788138\n",
      "Epoch [22/25], Train Loss: 0.0001054895983543247, Validation Loss: 0.00014394431806673917\n",
      "Epoch [22/25], Train Loss: 0.0001610233448445797, Validation Loss: 0.00014395055768545717\n",
      "Epoch [22/25], Train Loss: 0.00016883542411960661, Validation Loss: 0.00014399969659280033\n",
      "Epoch [22/25], Train Loss: 0.00018204555090051144, Validation Loss: 0.0001440245917668411\n",
      "Epoch [22/25], Train Loss: 0.00012486634659580886, Validation Loss: 0.00014396746628335676\n",
      "Epoch [22/25], Train Loss: 0.00010189588647335768, Validation Loss: 0.00014395941907423547\n",
      "Epoch [22/25], Train Loss: 0.0001691979996394366, Validation Loss: 0.00014407383399278235\n",
      "Epoch [22/25], Train Loss: 0.0001037622059811838, Validation Loss: 0.00014399257149004066\n",
      "Epoch [22/25], Train Loss: 0.00015409610932692885, Validation Loss: 0.00014393655122451793\n",
      "Epoch [22/25], Train Loss: 0.00012767974112648517, Validation Loss: 0.00014418461723835206\n",
      "Epoch [22/25], Train Loss: 0.00018207606626674533, Validation Loss: 0.0001440600640004656\n",
      "Epoch [22/25], Train Loss: 0.00015206061652861536, Validation Loss: 0.00014392770390259102\n",
      "Epoch [22/25], Train Loss: 0.0001601692201802507, Validation Loss: 0.0001440791784261819\n",
      "Epoch [22/25], Train Loss: 0.00012886333570349962, Validation Loss: 0.00014395831215855044\n",
      "Epoch [22/25], Train Loss: 0.00018489526701159775, Validation Loss: 0.0001439409694285132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], Train Loss: 0.0001503292005509138, Validation Loss: 0.0001439978174554805\n",
      "Epoch [22/25], Train Loss: 0.00015444385644514114, Validation Loss: 0.000143972753721755\n",
      "Epoch [22/25], Train Loss: 0.00012070775119354948, Validation Loss: 0.00014392935360471408\n",
      "Epoch [22/25], Train Loss: 0.00017402514640707523, Validation Loss: 0.0001439908587296183\n",
      "Epoch [22/25], Train Loss: 0.00016331177903339267, Validation Loss: 0.00014401993418384034\n",
      "Epoch [22/25], Train Loss: 0.0001599126699147746, Validation Loss: 0.00014398379498743453\n",
      "Epoch [22/25], Train Loss: 0.00016905223310459405, Validation Loss: 0.00014420025787937146\n",
      "Epoch [22/25], Train Loss: 0.00017291682888753712, Validation Loss: 0.0001444619478813062\n",
      "Epoch [22/25], Train Loss: 0.00013471826969180256, Validation Loss: 0.00014431960759490417\n",
      "Epoch [22/25], Train Loss: 0.0001612564956303686, Validation Loss: 0.0001445089406236851\n",
      "Epoch [22/25], Train Loss: 0.00019651444745250046, Validation Loss: 0.00014484996839504068\n",
      "Epoch [22/25], Train Loss: 0.00016552374290768057, Validation Loss: 0.00014501754218751255\n",
      "Epoch [22/25], Train Loss: 0.00014082371490076184, Validation Loss: 0.00014546939006928976\n",
      "Epoch [22/25], Train Loss: 0.00021662030485458672, Validation Loss: 0.00014601296691883666\n",
      "Epoch [22/25], Train Loss: 0.00014626032498199493, Validation Loss: 0.0001467131296521984\n",
      "Epoch [22/25], Train Loss: 0.00017453076725360006, Validation Loss: 0.00014744156311887006\n",
      "Epoch [22/25], Train Loss: 9.837024117587134e-05, Validation Loss: 0.00014848548526060767\n",
      "Epoch [22/25], Train Loss: 0.00014449201989918947, Validation Loss: 0.00014870873565087095\n",
      "Epoch [22/25], Train Loss: 0.00015631043061148375, Validation Loss: 0.00014902042700365808\n",
      "Epoch [22/25], Train Loss: 0.00019367437926121056, Validation Loss: 0.00014826696836583626\n",
      "Epoch [22/25], Train Loss: 0.0001259082491742447, Validation Loss: 0.0001470873847817226\n",
      "Epoch [22/25], Train Loss: 0.00016042593051679432, Validation Loss: 0.00014541647906298749\n",
      "Epoch [22/25], Train Loss: 0.00011789610289270058, Validation Loss: 0.00014444632373245743\n",
      "Epoch [22/25], Train Loss: 0.0002021286345552653, Validation Loss: 0.0001440868434049965\n",
      "Epoch [22/25], Train Loss: 0.00013494175800587982, Validation Loss: 0.00014420077447236206\n",
      "Epoch [22/25], Train Loss: 0.00014743211795575917, Validation Loss: 0.0001447903259152857\n",
      "Epoch [22/25], Train Loss: 0.00016297059482894838, Validation Loss: 0.0001454053754666044\n",
      "Epoch [22/25], Train Loss: 0.00012114363198634237, Validation Loss: 0.00014541516769289348\n",
      "Epoch [22/25], Train Loss: 0.00017763077630661428, Validation Loss: 0.00014514395346244176\n",
      "Epoch [22/25], Train Loss: 0.00016168456932064146, Validation Loss: 0.00014475221129638764\n",
      "Epoch [22/25], Train Loss: 0.0001184799984912388, Validation Loss: 0.00014419744911720045\n",
      "Epoch [22/25], Train Loss: 0.00017810772988013923, Validation Loss: 0.00014394082051391404\n",
      "Epoch [22/25], Train Loss: 0.000144904843182303, Validation Loss: 0.0001440893713152036\n",
      "Epoch [22/25], Train Loss: 0.00013150514860171825, Validation Loss: 0.00014438055974702973\n",
      "Epoch [22/25], Train Loss: 0.00016958877677097917, Validation Loss: 0.00014460148377111183\n",
      "Epoch [22/25], Train Loss: 0.0001473020383855328, Validation Loss: 0.00014475037363202622\n",
      "Epoch [22/25], Train Loss: 0.00018242814985569566, Validation Loss: 0.00014505294845245468\n",
      "Epoch [22/25], Train Loss: 0.00014771481801290065, Validation Loss: 0.00014470542810158805\n",
      "Epoch [22/25], Train Loss: 0.0002342978841625154, Validation Loss: 0.000144359098339919\n",
      "Epoch [22/25], Train Loss: 0.00014370540156960487, Validation Loss: 0.0001442469411510198\n",
      "Epoch [22/25], Train Loss: 9.901509474730119e-05, Validation Loss: 0.00014398539254519468\n",
      "Epoch [22/25], Train Loss: 0.00018057455599773675, Validation Loss: 0.00014405503679881804\n",
      "Epoch [22/25], Train Loss: 0.0001824136561481282, Validation Loss: 0.00014420131240816165\n",
      "Epoch [22/25], Train Loss: 0.00019243510905653238, Validation Loss: 0.00014398718267329968\n",
      "Epoch [22/25], Train Loss: 0.00011953339708270505, Validation Loss: 0.00014423342800000681\n",
      "Epoch [22/25], Train Loss: 0.00028881840989924967, Validation Loss: 0.00014531865041741792\n",
      "Epoch [23/25], Train Loss: 0.0001454287994420156, Validation Loss: 0.0001453089537487055\n",
      "Epoch [23/25], Train Loss: 0.00012627081014215946, Validation Loss: 0.0001441762443088616\n",
      "Epoch [23/25], Train Loss: 0.00016091442375909537, Validation Loss: 0.00014500828183372505\n",
      "Epoch [23/25], Train Loss: 0.00015459043788723648, Validation Loss: 0.00014485732220540133\n",
      "Epoch [23/25], Train Loss: 7.073115557432175e-05, Validation Loss: 0.0001445372229985272\n",
      "Epoch [23/25], Train Loss: 0.00020541626145131886, Validation Loss: 0.0001449535217640611\n",
      "Epoch [23/25], Train Loss: 0.00015816959785297513, Validation Loss: 0.00014515342918457462\n",
      "Epoch [23/25], Train Loss: 0.00020194311218801886, Validation Loss: 0.0001448784229675463\n",
      "Epoch [23/25], Train Loss: 0.00019517957116477191, Validation Loss: 0.000144913290811625\n",
      "Epoch [23/25], Train Loss: 0.00016968099225778133, Validation Loss: 0.00014553941558309209\n",
      "Epoch [23/25], Train Loss: 0.00015850852651055902, Validation Loss: 0.00014530468009373484\n",
      "Epoch [23/25], Train Loss: 0.0002064064028672874, Validation Loss: 0.00014536985181621276\n",
      "Epoch [23/25], Train Loss: 0.00012601249909494072, Validation Loss: 0.00014594448924375077\n",
      "Epoch [23/25], Train Loss: 0.00013426359510049224, Validation Loss: 0.0001460547396466912\n",
      "Epoch [23/25], Train Loss: 0.00019304620218463242, Validation Loss: 0.0001460160997036534\n",
      "Epoch [23/25], Train Loss: 0.00014315216685645282, Validation Loss: 0.0001463274818282419\n",
      "Epoch [23/25], Train Loss: 0.00012875520042143762, Validation Loss: 0.0001461384427481486\n",
      "Epoch [23/25], Train Loss: 0.00016716631944291294, Validation Loss: 0.00014612018276238814\n",
      "Epoch [23/25], Train Loss: 0.00011658387666102499, Validation Loss: 0.0001459556515328586\n",
      "Epoch [23/25], Train Loss: 0.00016908918041735888, Validation Loss: 0.00014571154291237082\n",
      "Epoch [23/25], Train Loss: 0.00015541281027253717, Validation Loss: 0.00014523580684908666\n",
      "Epoch [23/25], Train Loss: 0.00010920165368588641, Validation Loss: 0.00014482610082874696\n",
      "Epoch [23/25], Train Loss: 0.00016769100329838693, Validation Loss: 0.00014458653192074659\n",
      "Epoch [23/25], Train Loss: 0.00011678854207275435, Validation Loss: 0.00014423500057697918\n",
      "Epoch [23/25], Train Loss: 0.00014269244275055826, Validation Loss: 0.00014404380757090016\n",
      "Epoch [23/25], Train Loss: 0.00017465314886067063, Validation Loss: 0.00014413685688244489\n",
      "Epoch [23/25], Train Loss: 0.00016017895541153848, Validation Loss: 0.0001440205259617263\n",
      "Epoch [23/25], Train Loss: 0.00017071441106963903, Validation Loss: 0.0001441238198215918\n",
      "Epoch [23/25], Train Loss: 0.00015193258877843618, Validation Loss: 0.00014424804491378987\n",
      "Epoch [23/25], Train Loss: 0.00017656736599747092, Validation Loss: 0.0001444215248435891\n",
      "Epoch [23/25], Train Loss: 0.00010411824041511863, Validation Loss: 0.00014447341842848497\n",
      "Epoch [23/25], Train Loss: 0.00016363014583475888, Validation Loss: 0.00014445776881378454\n",
      "Epoch [23/25], Train Loss: 0.00022297345276456326, Validation Loss: 0.00014458666361557941\n",
      "Epoch [23/25], Train Loss: 0.00017297441081609577, Validation Loss: 0.00014458012107449275\n",
      "Epoch [23/25], Train Loss: 0.00011629058280959725, Validation Loss: 0.00014440207863420558\n",
      "Epoch [23/25], Train Loss: 0.00016693092766217887, Validation Loss: 0.00014444425784555884\n",
      "Epoch [23/25], Train Loss: 0.00018004006415139884, Validation Loss: 0.00014441388484556227\n",
      "Epoch [23/25], Train Loss: 0.0001336379355052486, Validation Loss: 0.0001444629216469669\n",
      "Epoch [23/25], Train Loss: 0.00020426845003385097, Validation Loss: 0.0001443298492328419\n",
      "Epoch [23/25], Train Loss: 0.00017024879343807697, Validation Loss: 0.00014430756176201005\n",
      "Epoch [23/25], Train Loss: 0.00014658801956102252, Validation Loss: 0.000144287415848036\n",
      "Epoch [23/25], Train Loss: 0.00018159234605263919, Validation Loss: 0.00014431329400395044\n",
      "Epoch [23/25], Train Loss: 0.00015296520723495632, Validation Loss: 0.00014445534519230325\n",
      "Epoch [23/25], Train Loss: 8.279539179056883e-05, Validation Loss: 0.00014444726354364927\n",
      "Epoch [23/25], Train Loss: 8.09983248473145e-05, Validation Loss: 0.000144576097712464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], Train Loss: 0.00013933285663370043, Validation Loss: 0.00014466444141968774\n",
      "Epoch [23/25], Train Loss: 0.0001303034950979054, Validation Loss: 0.00014492299960693343\n",
      "Epoch [23/25], Train Loss: 0.00017784697411116213, Validation Loss: 0.00014511820966921126\n",
      "Epoch [23/25], Train Loss: 0.00011900652316398919, Validation Loss: 0.00014544610166922212\n",
      "Epoch [23/25], Train Loss: 0.00016361207235604525, Validation Loss: 0.0001458855163946282\n",
      "Epoch [23/25], Train Loss: 0.00017472801846452057, Validation Loss: 0.0001464646920794621\n",
      "Epoch [23/25], Train Loss: 0.00011217834253329784, Validation Loss: 0.0001468322596338112\n",
      "Epoch [23/25], Train Loss: 0.00014542667486239225, Validation Loss: 0.00014740589782983685\n",
      "Epoch [23/25], Train Loss: 0.000124146492453292, Validation Loss: 0.00014757613292507206\n",
      "Epoch [23/25], Train Loss: 0.00012810048065148294, Validation Loss: 0.00014773266472426863\n",
      "Epoch [23/25], Train Loss: 0.00015066486957948655, Validation Loss: 0.00014727554177322114\n",
      "Epoch [23/25], Train Loss: 0.00016524398233741522, Validation Loss: 0.00014661565825614767\n",
      "Epoch [23/25], Train Loss: 0.00017908948939293623, Validation Loss: 0.00014551908291953925\n",
      "Epoch [23/25], Train Loss: 0.00013137012138031423, Validation Loss: 0.00014473034122299092\n",
      "Epoch [23/25], Train Loss: 0.00014297448797151446, Validation Loss: 0.00014416947815334424\n",
      "Epoch [23/25], Train Loss: 0.00015462991723325104, Validation Loss: 0.000144095015032993\n",
      "Epoch [23/25], Train Loss: 0.0001638133398955688, Validation Loss: 0.00014450162683109132\n",
      "Epoch [23/25], Train Loss: 0.00016953468730207533, Validation Loss: 0.00014490590256173165\n",
      "Epoch [23/25], Train Loss: 0.0001273494417546317, Validation Loss: 0.00014527639820395658\n",
      "Epoch [23/25], Train Loss: 0.00013001258776057512, Validation Loss: 0.00014531343719378735\n",
      "Epoch [23/25], Train Loss: 0.00016222039994318038, Validation Loss: 0.00014510040928144007\n",
      "Epoch [23/25], Train Loss: 0.00011896465730387717, Validation Loss: 0.00014456442368100398\n",
      "Epoch [23/25], Train Loss: 0.00010769588698167354, Validation Loss: 0.00014420694618214232\n",
      "Epoch [23/25], Train Loss: 0.0001864946389105171, Validation Loss: 0.00014411019437829964\n",
      "Epoch [23/25], Train Loss: 0.0001400545734213665, Validation Loss: 0.00014391698302157845\n",
      "Epoch [23/25], Train Loss: 0.00012476225674618036, Validation Loss: 0.00014410878017467136\n",
      "Epoch [23/25], Train Loss: 0.0002173861430492252, Validation Loss: 0.00014430758989571283\n",
      "Epoch [23/25], Train Loss: 0.0001745961926644668, Validation Loss: 0.0001444896544853691\n",
      "Epoch [23/25], Train Loss: 0.00015813687059562653, Validation Loss: 0.00014470973571102755\n",
      "Epoch [23/25], Train Loss: 0.00015213467122521251, Validation Loss: 0.00014481576314816872\n",
      "Epoch [23/25], Train Loss: 0.00017383196973241866, Validation Loss: 0.00014481579928542486\n",
      "Epoch [23/25], Train Loss: 0.0001453249715268612, Validation Loss: 0.00014478749023207153\n",
      "Epoch [23/25], Train Loss: 0.00017125063459388912, Validation Loss: 0.00014466001230175606\n",
      "Epoch [23/25], Train Loss: 0.00016788508219178766, Validation Loss: 0.00014462010246158268\n",
      "Epoch [23/25], Train Loss: 0.00015487553901039064, Validation Loss: 0.00014446877830778248\n",
      "Epoch [23/25], Train Loss: 0.00017686313367448747, Validation Loss: 0.00014437307763728313\n",
      "Epoch [23/25], Train Loss: 0.00011496669321786612, Validation Loss: 0.0001443577074193551\n",
      "Epoch [23/25], Train Loss: 0.0001669583871262148, Validation Loss: 0.00014433916027580077\n",
      "Epoch [23/25], Train Loss: 0.00015662185614928603, Validation Loss: 0.000144177694649746\n",
      "Epoch [23/25], Train Loss: 0.00016304911696352065, Validation Loss: 0.00014423601363281098\n",
      "Epoch [23/25], Train Loss: 0.00015453682863153517, Validation Loss: 0.00014427292505085157\n",
      "Epoch [23/25], Train Loss: 0.00011827693379018456, Validation Loss: 0.00014408642188451875\n",
      "Epoch [23/25], Train Loss: 0.0001309470389969647, Validation Loss: 0.00014427621329862934\n",
      "Epoch [23/25], Train Loss: 0.00016683047579135746, Validation Loss: 0.0001442665151747254\n",
      "Epoch [23/25], Train Loss: 0.000174950051587075, Validation Loss: 0.00014418666663308007\n",
      "Epoch [23/25], Train Loss: 0.00017292730626650155, Validation Loss: 0.00014442935983727996\n",
      "Epoch [23/25], Train Loss: 0.00014839135110378265, Validation Loss: 0.00014465644417214208\n",
      "Epoch [23/25], Train Loss: 0.0001772475807229057, Validation Loss: 0.00014465216299868188\n",
      "Epoch [23/25], Train Loss: 0.0001530377339804545, Validation Loss: 0.00014493328247529766\n",
      "Epoch [23/25], Train Loss: 0.00012271379819139838, Validation Loss: 0.00014539609328494406\n",
      "Epoch [23/25], Train Loss: 0.00014566868776455522, Validation Loss: 0.000145656864818496\n",
      "Epoch [23/25], Train Loss: 0.00014576596731785685, Validation Loss: 0.00014620560153465098\n",
      "Epoch [23/25], Train Loss: 0.0001788817608030513, Validation Loss: 0.00014667945894567916\n",
      "Epoch [23/25], Train Loss: 0.00014290759281720966, Validation Loss: 0.0001472882916762804\n",
      "Epoch [23/25], Train Loss: 0.00016875243454705924, Validation Loss: 0.00014735029156630238\n",
      "Epoch [23/25], Train Loss: 0.00012964641791768372, Validation Loss: 0.00014765379770930546\n",
      "Epoch [23/25], Train Loss: 0.00013854609278496355, Validation Loss: 0.00014710775794810616\n",
      "Epoch [23/25], Train Loss: 0.00012278713984414935, Validation Loss: 0.00014659142179880292\n",
      "Epoch [23/25], Train Loss: 0.00013960002979729325, Validation Loss: 0.00014559098902585296\n",
      "Epoch [23/25], Train Loss: 0.00013608795416075736, Validation Loss: 0.0001448010138119571\n",
      "Epoch [23/25], Train Loss: 0.00017123183351941407, Validation Loss: 0.00014417276009529207\n",
      "Epoch [23/25], Train Loss: 0.00020550290355458856, Validation Loss: 0.00014388930964438865\n",
      "Epoch [23/25], Train Loss: 0.0001507563574705273, Validation Loss: 0.00014412045614638677\n",
      "Epoch [23/25], Train Loss: 0.00019231940677855164, Validation Loss: 0.00014444105763686821\n",
      "Epoch [23/25], Train Loss: 0.00013515564205590636, Validation Loss: 0.00014482571229261035\n",
      "Epoch [23/25], Train Loss: 0.00015707459533587098, Validation Loss: 0.0001450336412138616\n",
      "Epoch [23/25], Train Loss: 0.00016874259745236486, Validation Loss: 0.00014499197883803086\n",
      "Epoch [23/25], Train Loss: 0.0001395536382915452, Validation Loss: 0.00014469388212698202\n",
      "Epoch [23/25], Train Loss: 0.000192966777831316, Validation Loss: 0.00014427859326436495\n",
      "Epoch [23/25], Train Loss: 0.00017894493066705763, Validation Loss: 0.000144013015475745\n",
      "Epoch [23/25], Train Loss: 0.00015123229240998626, Validation Loss: 0.00014394721195761426\n",
      "Epoch [23/25], Train Loss: 0.00015480104775633663, Validation Loss: 0.00014397878500555333\n",
      "Epoch [23/25], Train Loss: 0.00012756868090946227, Validation Loss: 0.00014419196571301048\n",
      "Epoch [23/25], Train Loss: 0.00020388152915984392, Validation Loss: 0.00014443663725008566\n",
      "Epoch [23/25], Train Loss: 0.00016850839892867953, Validation Loss: 0.00014463004772551357\n",
      "Epoch [23/25], Train Loss: 0.00016298575792461634, Validation Loss: 0.000144765500590438\n",
      "Epoch [23/25], Train Loss: 0.00015653356967959553, Validation Loss: 0.00014483886746650872\n",
      "Epoch [23/25], Train Loss: 0.0001629462349228561, Validation Loss: 0.0001447738250135444\n",
      "Epoch [23/25], Train Loss: 0.0001596315996721387, Validation Loss: 0.00014467258491398147\n",
      "Epoch [23/25], Train Loss: 0.00016374370898120105, Validation Loss: 0.00014463954115247665\n",
      "Epoch [23/25], Train Loss: 0.00019718706607818604, Validation Loss: 0.00014446359612823773\n",
      "Epoch [23/25], Train Loss: 0.00016492522263433784, Validation Loss: 0.0001443780420231633\n",
      "Epoch [23/25], Train Loss: 0.0001384427450830117, Validation Loss: 0.0001442098223681872\n",
      "Epoch [23/25], Train Loss: 0.0002015196660067886, Validation Loss: 0.00014408678374214408\n",
      "Epoch [23/25], Train Loss: 0.000178807124029845, Validation Loss: 0.0001440353854074298\n",
      "Epoch [23/25], Train Loss: 0.0001773942494764924, Validation Loss: 0.0001439637160122705\n",
      "Epoch [23/25], Train Loss: 0.0001363433402730152, Validation Loss: 0.0001439107514064138\n",
      "Epoch [23/25], Train Loss: 0.00013112004671711475, Validation Loss: 0.0001439752450096421\n",
      "Epoch [23/25], Train Loss: 0.00013734640378970653, Validation Loss: 0.00014406119953491725\n",
      "Epoch [23/25], Train Loss: 9.390239574713632e-05, Validation Loss: 0.000143946245952975\n",
      "Epoch [23/25], Train Loss: 0.00017343313083983958, Validation Loss: 0.0001439297368051484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], Train Loss: 0.00011363771045580506, Validation Loss: 0.0001439501254935749\n",
      "Epoch [23/25], Train Loss: 0.00019914732547476888, Validation Loss: 0.00014405206044709\n",
      "Epoch [23/25], Train Loss: 0.00011821128282463178, Validation Loss: 0.00014404239530752723\n",
      "Epoch [23/25], Train Loss: 0.0001371586840832606, Validation Loss: 0.0001442070444075701\n",
      "Epoch [23/25], Train Loss: 0.00014794326853007078, Validation Loss: 0.0001439777018579965\n",
      "Epoch [23/25], Train Loss: 0.00015396643721032888, Validation Loss: 0.00014411770025617442\n",
      "Epoch [23/25], Train Loss: 0.00020628781931009144, Validation Loss: 0.00014424925296528576\n",
      "Epoch [23/25], Train Loss: 0.00011979676492046565, Validation Loss: 0.00014410687266111684\n",
      "Epoch [23/25], Train Loss: 0.00016180402599275112, Validation Loss: 0.00014414317629416474\n",
      "Epoch [23/25], Train Loss: 0.00014813424786552787, Validation Loss: 0.000144308126133789\n",
      "Epoch [23/25], Train Loss: 0.00019146999693475664, Validation Loss: 0.00014447104137313242\n",
      "Epoch [23/25], Train Loss: 0.00012900310684926808, Validation Loss: 0.00014470642975841959\n",
      "Epoch [23/25], Train Loss: 0.00018713506869971752, Validation Loss: 0.00014516666463653866\n",
      "Epoch [23/25], Train Loss: 0.00020613493688870221, Validation Loss: 0.0001459941454716803\n",
      "Epoch [23/25], Train Loss: 0.00010167393338633701, Validation Loss: 0.0001469455496893109\n",
      "Epoch [23/25], Train Loss: 0.00017179132555611432, Validation Loss: 0.00014920244066161104\n",
      "Epoch [23/25], Train Loss: 0.0001328158687101677, Validation Loss: 0.00015120745811145753\n",
      "Epoch [23/25], Train Loss: 0.00017796225438360125, Validation Loss: 0.00015501848295874273\n",
      "Epoch [23/25], Train Loss: 0.00015914706455077976, Validation Loss: 0.0001565397695230786\n",
      "Epoch [23/25], Train Loss: 0.00015090238593984395, Validation Loss: 0.00015899368590908126\n",
      "Epoch [23/25], Train Loss: 0.00014143678708933294, Validation Loss: 0.00015458593940517555\n",
      "Epoch [23/25], Train Loss: 0.00013310906069818884, Validation Loss: 0.00014972327844589017\n",
      "Epoch [23/25], Train Loss: 0.00020385569951031357, Validation Loss: 0.00014514253925881348\n",
      "Epoch [23/25], Train Loss: 0.00015382473065983504, Validation Loss: 0.00014442271494772286\n",
      "Epoch [23/25], Train Loss: 0.00020648643840104342, Validation Loss: 0.0001469420438904005\n",
      "Epoch [23/25], Train Loss: 0.00011868926230818033, Validation Loss: 0.00014871481059041495\n",
      "Epoch [23/25], Train Loss: 0.0001393711136188358, Validation Loss: 0.0001481900110472149\n",
      "Epoch [23/25], Train Loss: 0.00016924490046221763, Validation Loss: 0.0001455022766701101\n",
      "Epoch [23/25], Train Loss: 0.0001397063460899517, Validation Loss: 0.0001440729827057415\n",
      "Epoch [23/25], Train Loss: 0.0001279933494515717, Validation Loss: 0.0001451764124794863\n",
      "Epoch [23/25], Train Loss: 0.00016424737987108529, Validation Loss: 0.0001466402592389689\n",
      "Epoch [23/25], Train Loss: 0.0001384612696710974, Validation Loss: 0.00014668906417985756\n",
      "Epoch [23/25], Train Loss: 0.00015605019871145487, Validation Loss: 0.00014494644055957907\n",
      "Epoch [23/25], Train Loss: 0.00011231676762690768, Validation Loss: 0.0001441104116869004\n",
      "Epoch [23/25], Train Loss: 0.0001456305617466569, Validation Loss: 0.00014459215065774819\n",
      "Epoch [23/25], Train Loss: 0.0001573290937812999, Validation Loss: 0.00014554085185712514\n",
      "Epoch [23/25], Train Loss: 0.00016664862050674856, Validation Loss: 0.00014589561396860518\n",
      "Epoch [23/25], Train Loss: 0.00012096118007320911, Validation Loss: 0.00014505081780953334\n",
      "Epoch [23/25], Train Loss: 0.00017797919281292707, Validation Loss: 0.00014412883756449447\n",
      "Epoch [23/25], Train Loss: 0.0001391782279824838, Validation Loss: 0.00014401560474652797\n",
      "Epoch [23/25], Train Loss: 0.00012830813648179173, Validation Loss: 0.00014451763612062982\n",
      "Epoch [23/25], Train Loss: 0.00012746785068884492, Validation Loss: 0.00014503489752920967\n",
      "Epoch [23/25], Train Loss: 0.00012891906953882426, Validation Loss: 0.00014497781982451366\n",
      "Epoch [23/25], Train Loss: 0.00017317729361820966, Validation Loss: 0.00014467058184285028\n",
      "Epoch [23/25], Train Loss: 0.00021013504010625184, Validation Loss: 0.0001441743537725415\n",
      "Epoch [23/25], Train Loss: 0.0002024176937993616, Validation Loss: 0.00014393300710556408\n",
      "Epoch [23/25], Train Loss: 0.0001575911301188171, Validation Loss: 0.00014411245113781963\n",
      "Epoch [23/25], Train Loss: 0.00014038762310519814, Validation Loss: 0.00014429614748223685\n",
      "Epoch [23/25], Train Loss: 0.000195201879250817, Validation Loss: 0.00014462518205012506\n",
      "Epoch [23/25], Train Loss: 0.00015524547779932618, Validation Loss: 0.00014459626860722588\n",
      "Epoch [23/25], Train Loss: 9.928146027959883e-05, Validation Loss: 0.0001445343766439085\n",
      "Epoch [23/25], Train Loss: 0.00015963563055265695, Validation Loss: 0.0001441540167434141\n",
      "Epoch [23/25], Train Loss: 0.0001701310247881338, Validation Loss: 0.00014399633461531874\n",
      "Epoch [23/25], Train Loss: 0.00012490972585510463, Validation Loss: 0.00014393659730558284\n",
      "Epoch [23/25], Train Loss: 0.0001538431242806837, Validation Loss: 0.00014386657834014233\n",
      "Epoch [23/25], Train Loss: 0.00013041244528722018, Validation Loss: 0.00014405210094992071\n",
      "Epoch [23/25], Train Loss: 0.00017771120474208146, Validation Loss: 0.00014419717311587495\n",
      "Epoch [23/25], Train Loss: 0.00017202517483383417, Validation Loss: 0.00014417692364077084\n",
      "Epoch [23/25], Train Loss: 0.0001342580799246207, Validation Loss: 0.00014436736673815176\n",
      "Epoch [23/25], Train Loss: 0.0001417733874404803, Validation Loss: 0.00014425124390982091\n",
      "Epoch [23/25], Train Loss: 0.00012047997006447986, Validation Loss: 0.00014411985393962822\n",
      "Epoch [23/25], Train Loss: 0.00021751933672931045, Validation Loss: 0.00014423391282131586\n",
      "Epoch [23/25], Train Loss: 0.00015973608242347836, Validation Loss: 0.00014415013647521845\n",
      "Epoch [23/25], Train Loss: 0.0001441758213331923, Validation Loss: 0.00014394718212618803\n",
      "Epoch [23/25], Train Loss: 0.00015041012375149876, Validation Loss: 0.0001439761823955147\n",
      "Epoch [23/25], Train Loss: 0.0001845587248681113, Validation Loss: 0.00014398304677645986\n",
      "Epoch [23/25], Train Loss: 0.00014353595906868577, Validation Loss: 0.0001439001711939151\n",
      "Epoch [23/25], Train Loss: 0.00018115167040377855, Validation Loss: 0.00014395716692282198\n",
      "Epoch [23/25], Train Loss: 0.0001394964347127825, Validation Loss: 0.00014397996080030376\n",
      "Epoch [23/25], Train Loss: 0.00019057124154642224, Validation Loss: 0.00014389577117981389\n",
      "Epoch [23/25], Train Loss: 0.0001324314362136647, Validation Loss: 0.00014399267335344726\n",
      "Epoch [23/25], Train Loss: 0.00015298963990062475, Validation Loss: 0.00014394597213443678\n",
      "Epoch [23/25], Train Loss: 0.00019640385289676487, Validation Loss: 0.00014398497854320642\n",
      "Epoch [23/25], Train Loss: 0.00018463487504050136, Validation Loss: 0.0001440646177798044\n",
      "Epoch [23/25], Train Loss: 0.00013365052291192114, Validation Loss: 0.00014420137910443979\n",
      "Epoch [23/25], Train Loss: 0.00015221411013044417, Validation Loss: 0.00014414506003959104\n",
      "Epoch [23/25], Train Loss: 0.0001137614090112038, Validation Loss: 0.00014401067237486133\n",
      "Epoch [23/25], Train Loss: 0.00019033084390684962, Validation Loss: 0.00014436547013853366\n",
      "Epoch [23/25], Train Loss: 0.00013748049968853593, Validation Loss: 0.00014421142526164962\n",
      "Epoch [23/25], Train Loss: 0.0001585564314154908, Validation Loss: 0.00014402024074418782\n",
      "Epoch [23/25], Train Loss: 0.0001312436070293188, Validation Loss: 0.00014406779300770722\n",
      "Epoch [23/25], Train Loss: 0.0001673189108259976, Validation Loss: 0.0001441347199336936\n",
      "Epoch [23/25], Train Loss: 0.00016544452228117734, Validation Loss: 0.00014405276330459554\n",
      "Epoch [23/25], Train Loss: 0.0001757430873112753, Validation Loss: 0.0001439674391197817\n",
      "Epoch [23/25], Train Loss: 0.00015365613216999918, Validation Loss: 0.0001440885896348239\n",
      "Epoch [23/25], Train Loss: 0.00017645575280766934, Validation Loss: 0.0001441083339159377\n",
      "Epoch [23/25], Train Loss: 0.00013524550013244152, Validation Loss: 0.0001440514376251182\n",
      "Epoch [23/25], Train Loss: 0.0001294809189857915, Validation Loss: 0.0001440182621687806\n",
      "Epoch [23/25], Train Loss: 0.0001313212123932317, Validation Loss: 0.00014402316640674446\n",
      "Epoch [23/25], Train Loss: 0.00013948454579804093, Validation Loss: 0.00014400596167736998\n",
      "Epoch [23/25], Train Loss: 0.00013416085857897997, Validation Loss: 0.00014396657400842134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], Train Loss: 0.0002197533322032541, Validation Loss: 0.00014391303193406202\n",
      "Epoch [23/25], Train Loss: 0.00014418415958061814, Validation Loss: 0.00014394333848031238\n",
      "Epoch [23/25], Train Loss: 0.0001293605746468529, Validation Loss: 0.0001440340015202916\n",
      "Epoch [23/25], Train Loss: 0.00017126234888564795, Validation Loss: 0.00014404154377795447\n",
      "Epoch [23/25], Train Loss: 0.00016747313202358782, Validation Loss: 0.00014392122344967598\n",
      "Epoch [23/25], Train Loss: 0.00011357069888617843, Validation Loss: 0.00014402135711861775\n",
      "Epoch [23/25], Train Loss: 0.00014099387044552714, Validation Loss: 0.0001439062141192456\n",
      "Epoch [23/25], Train Loss: 0.00013151777966413647, Validation Loss: 0.00014390891349952046\n",
      "Epoch [23/25], Train Loss: 0.00014070076576899737, Validation Loss: 0.00014403035723565457\n",
      "Epoch [23/25], Train Loss: 0.00013847145601175725, Validation Loss: 0.00014391944472057124\n",
      "Epoch [23/25], Train Loss: 0.0001046707620844245, Validation Loss: 0.00014392638452894366\n",
      "Epoch [23/25], Train Loss: 0.00015261284715961665, Validation Loss: 0.0001440273721527774\n",
      "Epoch [23/25], Train Loss: 0.00014843071403447539, Validation Loss: 0.00014388402632903308\n",
      "Epoch [23/25], Train Loss: 0.00012176605378044769, Validation Loss: 0.00014385513859451748\n",
      "Epoch [23/25], Train Loss: 0.0002287125971633941, Validation Loss: 0.0001440784748410806\n",
      "Epoch [23/25], Train Loss: 0.0001723296445561573, Validation Loss: 0.00014395850545649105\n",
      "Epoch [23/25], Train Loss: 0.00013729841157328337, Validation Loss: 0.000143912983185146\n",
      "Epoch [23/25], Train Loss: 0.00018836090748663992, Validation Loss: 0.00014404681981735243\n",
      "Epoch [23/25], Train Loss: 0.00011432456813054159, Validation Loss: 0.00014401819620009823\n",
      "Epoch [23/25], Train Loss: 0.00022695241204928607, Validation Loss: 0.00014395547623280436\n",
      "Epoch [23/25], Train Loss: 0.00015331385657191277, Validation Loss: 0.00014404248577193357\n",
      "Epoch [23/25], Train Loss: 0.00015329169400501996, Validation Loss: 0.00014409778062448217\n",
      "Epoch [23/25], Train Loss: 0.00014866737183183432, Validation Loss: 0.00014408972735206285\n",
      "Epoch [23/25], Train Loss: 0.00016487028915435076, Validation Loss: 0.00014415048475105625\n",
      "Epoch [23/25], Train Loss: 0.00017479393864050508, Validation Loss: 0.000144259560086842\n",
      "Epoch [23/25], Train Loss: 0.00015098435687832534, Validation Loss: 0.00014425689272078065\n",
      "Epoch [23/25], Train Loss: 0.00016047076496761292, Validation Loss: 0.00014449913675586382\n",
      "Epoch [23/25], Train Loss: 0.000178752132342197, Validation Loss: 0.00014449010122916662\n",
      "Epoch [23/25], Train Loss: 0.00020371140271890908, Validation Loss: 0.00014462317267316394\n",
      "Epoch [23/25], Train Loss: 9.017389675136656e-05, Validation Loss: 0.00014474717172561214\n",
      "Epoch [23/25], Train Loss: 0.00017002482491079718, Validation Loss: 0.00014487648149952292\n",
      "Epoch [23/25], Train Loss: 0.00015359769167844206, Validation Loss: 0.00014517362336240088\n",
      "Epoch [23/25], Train Loss: 0.00017101338016800582, Validation Loss: 0.00014545453935473537\n",
      "Epoch [23/25], Train Loss: 0.00014017090143170208, Validation Loss: 0.00014587455540701437\n",
      "Epoch [23/25], Train Loss: 0.0001787677319953218, Validation Loss: 0.00014637659551226535\n",
      "Epoch [23/25], Train Loss: 0.0001823888742364943, Validation Loss: 0.00014671349514780256\n",
      "Epoch [23/25], Train Loss: 0.0002000374806812033, Validation Loss: 0.00014746334442558389\n",
      "Epoch [23/25], Train Loss: 0.0001480646460549906, Validation Loss: 0.0001479592734540347\n",
      "Epoch [23/25], Train Loss: 0.00015039133722893894, Validation Loss: 0.00014872472141481315\n",
      "Epoch [23/25], Train Loss: 0.00012908628559671342, Validation Loss: 0.00014872625736946551\n",
      "Epoch [23/25], Train Loss: 0.00013864868378732353, Validation Loss: 0.0001490378218780582\n",
      "Epoch [24/25], Train Loss: 0.00018053996609523892, Validation Loss: 0.00014808858371300932\n",
      "Epoch [24/25], Train Loss: 0.00015639168850611895, Validation Loss: 0.0001471197091935513\n",
      "Epoch [24/25], Train Loss: 0.0001527487183921039, Validation Loss: 0.00014559667688445187\n",
      "Epoch [24/25], Train Loss: 0.00013795658014714718, Validation Loss: 0.00014447989087784663\n",
      "Epoch [24/25], Train Loss: 0.00016808972577564418, Validation Loss: 0.0001439170181887069\n",
      "Epoch [24/25], Train Loss: 0.00017934892093762755, Validation Loss: 0.00014403375438026462\n",
      "Epoch [24/25], Train Loss: 0.00022081063070800155, Validation Loss: 0.0001445081764056037\n",
      "Epoch [24/25], Train Loss: 0.00012463872553780675, Validation Loss: 0.0001448296507684669\n",
      "Epoch [24/25], Train Loss: 0.000188915801118128, Validation Loss: 0.00014494830829789863\n",
      "Epoch [24/25], Train Loss: 0.00019456134759820998, Validation Loss: 0.0001447629901425292\n",
      "Epoch [24/25], Train Loss: 0.00015737533976789564, Validation Loss: 0.00014444848735971997\n",
      "Epoch [24/25], Train Loss: 0.00013962188677396625, Validation Loss: 0.00014416804745754537\n",
      "Epoch [24/25], Train Loss: 0.00012364928261376917, Validation Loss: 0.00014397447933636915\n",
      "Epoch [24/25], Train Loss: 0.00020708759257104248, Validation Loss: 0.00014398810138421442\n",
      "Epoch [24/25], Train Loss: 0.00011927017476409674, Validation Loss: 0.00014416708533341685\n",
      "Epoch [24/25], Train Loss: 0.00013924248924013227, Validation Loss: 0.00014416498476445364\n",
      "Epoch [24/25], Train Loss: 0.00013606296852231026, Validation Loss: 0.0001442225169739686\n",
      "Epoch [24/25], Train Loss: 0.00010898302571149543, Validation Loss: 0.00014433384809914667\n",
      "Epoch [24/25], Train Loss: 0.00010971772280754521, Validation Loss: 0.00014421665837289767\n",
      "Epoch [24/25], Train Loss: 0.00013199244858697057, Validation Loss: 0.00014418395367101766\n",
      "Epoch [24/25], Train Loss: 0.00014503822603728622, Validation Loss: 0.00014425248931123255\n",
      "Epoch [24/25], Train Loss: 0.00011694816203089431, Validation Loss: 0.00014406137294524038\n",
      "Epoch [24/25], Train Loss: 0.00015417445683851838, Validation Loss: 0.0001439754501916468\n",
      "Epoch [24/25], Train Loss: 0.00016606187273282558, Validation Loss: 0.00014400758445844985\n",
      "Epoch [24/25], Train Loss: 0.00014772635768167675, Validation Loss: 0.0001439683825689523\n",
      "Epoch [24/25], Train Loss: 0.00018181522318627685, Validation Loss: 0.00014395363129248532\n",
      "Epoch [24/25], Train Loss: 0.00013455293083097786, Validation Loss: 0.00014396937670729432\n",
      "Epoch [24/25], Train Loss: 0.0001617517409613356, Validation Loss: 0.00014398808489204384\n",
      "Epoch [24/25], Train Loss: 0.0002083762374240905, Validation Loss: 0.00014398345980832044\n",
      "Epoch [24/25], Train Loss: 0.00012656499166041613, Validation Loss: 0.00014409646755666471\n",
      "Epoch [24/25], Train Loss: 0.00015927884669508785, Validation Loss: 0.00014403585422163208\n",
      "Epoch [24/25], Train Loss: 0.00011246999201830477, Validation Loss: 0.00014418673623974124\n",
      "Epoch [24/25], Train Loss: 0.000172570493305102, Validation Loss: 0.0001441141588050717\n",
      "Epoch [24/25], Train Loss: 0.00016318869893439114, Validation Loss: 0.00014422882862466697\n",
      "Epoch [24/25], Train Loss: 0.0001330472296103835, Validation Loss: 0.00014422818955305653\n",
      "Epoch [24/25], Train Loss: 0.00013263353321235627, Validation Loss: 0.00014427127316594124\n",
      "Epoch [24/25], Train Loss: 0.00012968922965228558, Validation Loss: 0.00014429219760738002\n",
      "Epoch [24/25], Train Loss: 0.0001630987972021103, Validation Loss: 0.00014444836609375973\n",
      "Epoch [24/25], Train Loss: 0.00018110821838490665, Validation Loss: 0.00014461959023416663\n",
      "Epoch [24/25], Train Loss: 0.00018366007134318352, Validation Loss: 0.00014505578025515812\n",
      "Epoch [24/25], Train Loss: 0.00014779505727346987, Validation Loss: 0.00014532366282461832\n",
      "Epoch [24/25], Train Loss: 0.00018475936667528003, Validation Loss: 0.0001454143692778113\n",
      "Epoch [24/25], Train Loss: 0.00018404345610179007, Validation Loss: 0.00014600997431746994\n",
      "Epoch [24/25], Train Loss: 0.0001343235926469788, Validation Loss: 0.00014642491660197266\n",
      "Epoch [24/25], Train Loss: 0.00015491675003431737, Validation Loss: 0.00014696926373289899\n",
      "Epoch [24/25], Train Loss: 0.00017826254770625383, Validation Loss: 0.00014723431231686846\n",
      "Epoch [24/25], Train Loss: 0.00014345442468766123, Validation Loss: 0.0001476520672440529\n",
      "Epoch [24/25], Train Loss: 0.0001571771572344005, Validation Loss: 0.00014736375402814398\n",
      "Epoch [24/25], Train Loss: 0.00013256135571282357, Validation Loss: 0.00014724290231242775\n",
      "Epoch [24/25], Train Loss: 0.0001769154769135639, Validation Loss: 0.00014638982320320794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], Train Loss: 0.00015172779967542738, Validation Loss: 0.00014559883566107602\n",
      "Epoch [24/25], Train Loss: 0.00014669934171251953, Validation Loss: 0.00014472640517245358\n",
      "Epoch [24/25], Train Loss: 0.0001260055578313768, Validation Loss: 0.0001440956972752853\n",
      "Epoch [24/25], Train Loss: 0.0001811298861866817, Validation Loss: 0.00014393724438074666\n",
      "Epoch [24/25], Train Loss: 0.00013170199235901237, Validation Loss: 0.00014408095933807392\n",
      "Epoch [24/25], Train Loss: 0.00015168801473919302, Validation Loss: 0.00014426515408558772\n",
      "Epoch [24/25], Train Loss: 0.00015815658844076097, Validation Loss: 0.0001445988263488592\n",
      "Epoch [24/25], Train Loss: 0.00018885046301875263, Validation Loss: 0.00014471288668573833\n",
      "Epoch [24/25], Train Loss: 0.00016564455290790647, Validation Loss: 0.00014465729933969365\n",
      "Epoch [24/25], Train Loss: 0.00012043242895742878, Validation Loss: 0.0001446199571849623\n",
      "Epoch [24/25], Train Loss: 0.0001780488237272948, Validation Loss: 0.0001443138860243683\n",
      "Epoch [24/25], Train Loss: 0.00015879723650868982, Validation Loss: 0.00014411060910788365\n",
      "Epoch [24/25], Train Loss: 0.00015031834482215345, Validation Loss: 0.0001440177965074933\n",
      "Epoch [24/25], Train Loss: 0.00017921703692991287, Validation Loss: 0.0001439236640483917\n",
      "Epoch [24/25], Train Loss: 0.00015560947940684855, Validation Loss: 0.00014390861227487524\n",
      "Epoch [24/25], Train Loss: 0.00013422558549791574, Validation Loss: 0.00014392234224942514\n",
      "Epoch [24/25], Train Loss: 0.00019012457050848752, Validation Loss: 0.00014394418661443827\n",
      "Epoch [24/25], Train Loss: 0.00015231592988129705, Validation Loss: 0.00014402884577672617\n",
      "Epoch [24/25], Train Loss: 9.967479127226397e-05, Validation Loss: 0.00014413427537268337\n",
      "Epoch [24/25], Train Loss: 0.00018996998551301658, Validation Loss: 0.0001447712413209956\n",
      "Epoch [24/25], Train Loss: 0.00014955864753574133, Validation Loss: 0.00014461091074432868\n",
      "Epoch [24/25], Train Loss: 0.00010939721687464043, Validation Loss: 0.00014464202322415077\n",
      "Epoch [24/25], Train Loss: 0.00014606081822421402, Validation Loss: 0.00014521882718933435\n",
      "Epoch [24/25], Train Loss: 0.00012454696116037667, Validation Loss: 0.0001457461616761672\n",
      "Epoch [24/25], Train Loss: 0.00011943555000470951, Validation Loss: 0.0001459798811993096\n",
      "Epoch [24/25], Train Loss: 0.00014817784540355206, Validation Loss: 0.00014680578654709583\n",
      "Epoch [24/25], Train Loss: 0.00016446816152893007, Validation Loss: 0.00014800330973230302\n",
      "Epoch [24/25], Train Loss: 0.00014571388601325452, Validation Loss: 0.00014906167853041553\n",
      "Epoch [24/25], Train Loss: 0.00015822092245798558, Validation Loss: 0.0001508725462675405\n",
      "Epoch [24/25], Train Loss: 0.0002019582170760259, Validation Loss: 0.00015168836107477547\n",
      "Epoch [24/25], Train Loss: 0.00017224819748662412, Validation Loss: 0.00015316638916071195\n",
      "Epoch [24/25], Train Loss: 0.00015024421736598015, Validation Loss: 0.00015185324446065351\n",
      "Epoch [24/25], Train Loss: 0.00016400702588725835, Validation Loss: 0.0001502196483973724\n",
      "Epoch [24/25], Train Loss: 0.00017362355720251799, Validation Loss: 0.00014710324685438537\n",
      "Epoch [24/25], Train Loss: 0.0001279916614294052, Validation Loss: 0.0001447818676145592\n",
      "Epoch [24/25], Train Loss: 0.00018036698747891933, Validation Loss: 0.0001440713063251072\n",
      "Epoch [24/25], Train Loss: 0.00021803064737468958, Validation Loss: 0.00014496524430190522\n",
      "Epoch [24/25], Train Loss: 0.00013332843082025647, Validation Loss: 0.00014670741147710943\n",
      "Epoch [24/25], Train Loss: 0.00013763261085841805, Validation Loss: 0.00014704167818611799\n",
      "Epoch [24/25], Train Loss: 0.0001372402475681156, Validation Loss: 0.0001464118480119699\n",
      "Epoch [24/25], Train Loss: 9.369445615448058e-05, Validation Loss: 0.00014480043149281603\n",
      "Epoch [24/25], Train Loss: 0.00016955680621322244, Validation Loss: 0.00014426452326006257\n",
      "Epoch [24/25], Train Loss: 0.00013692128413822502, Validation Loss: 0.0001444912585914911\n",
      "Epoch [24/25], Train Loss: 0.0001579043600941077, Validation Loss: 0.00014557787944795563\n",
      "Epoch [24/25], Train Loss: 0.00014839458162896335, Validation Loss: 0.00014598555402092945\n",
      "Epoch [24/25], Train Loss: 0.00014812362496741116, Validation Loss: 0.00014526099039358087\n",
      "Epoch [24/25], Train Loss: 0.0001547369611216709, Validation Loss: 0.00014441792736761272\n",
      "Epoch [24/25], Train Loss: 0.0001831578410929069, Validation Loss: 0.00014396286132978275\n",
      "Epoch [24/25], Train Loss: 0.00016334907559212297, Validation Loss: 0.00014411450538318605\n",
      "Epoch [24/25], Train Loss: 0.00012993882410228252, Validation Loss: 0.00014470793757936917\n",
      "Epoch [24/25], Train Loss: 0.00013629136083181947, Validation Loss: 0.00014491893040637175\n",
      "Epoch [24/25], Train Loss: 0.00014656645362265408, Validation Loss: 0.00014481825201073661\n",
      "Epoch [24/25], Train Loss: 0.00016565725672990084, Validation Loss: 0.00014434176846407355\n",
      "Epoch [24/25], Train Loss: 0.00011979920964222401, Validation Loss: 0.00014395573549942734\n",
      "Epoch [24/25], Train Loss: 0.00013756356202065945, Validation Loss: 0.00014390429059858435\n",
      "Epoch [24/25], Train Loss: 0.0001625285076443106, Validation Loss: 0.00014403388680269322\n",
      "Epoch [24/25], Train Loss: 0.00013379573647398502, Validation Loss: 0.0001441715469506259\n",
      "Epoch [24/25], Train Loss: 0.0001475362223573029, Validation Loss: 0.00014434674206616668\n",
      "Epoch [24/25], Train Loss: 0.00017002139065880328, Validation Loss: 0.0001443172470317222\n",
      "Epoch [24/25], Train Loss: 0.00014306750381365418, Validation Loss: 0.00014414332205584893\n",
      "Epoch [24/25], Train Loss: 0.00015967420767992735, Validation Loss: 0.00014400893827162993\n",
      "Epoch [24/25], Train Loss: 0.00016459872131235898, Validation Loss: 0.00014387067179389608\n",
      "Epoch [24/25], Train Loss: 0.00015536678256466985, Validation Loss: 0.00014381365666243557\n",
      "Epoch [24/25], Train Loss: 0.00017542511341162026, Validation Loss: 0.00014386924134062912\n",
      "Epoch [24/25], Train Loss: 0.00017405726248398423, Validation Loss: 0.00014393865033828963\n",
      "Epoch [24/25], Train Loss: 0.00014325843949336559, Validation Loss: 0.00014405787903039405\n",
      "Epoch [24/25], Train Loss: 0.00015772387268953025, Validation Loss: 0.00014416606839707432\n",
      "Epoch [24/25], Train Loss: 0.00013197495718486607, Validation Loss: 0.00014422031745198183\n",
      "Epoch [24/25], Train Loss: 0.0001144671841757372, Validation Loss: 0.00014418141751472527\n",
      "Epoch [24/25], Train Loss: 0.00015505636110901833, Validation Loss: 0.00014412723564115974\n",
      "Epoch [24/25], Train Loss: 0.0001527893473394215, Validation Loss: 0.00014412530460200895\n",
      "Epoch [24/25], Train Loss: 0.00018638133769854903, Validation Loss: 0.00014414345302308598\n",
      "Epoch [24/25], Train Loss: 0.00019044576038140804, Validation Loss: 0.0001439169859319615\n",
      "Epoch [24/25], Train Loss: 0.00014288336387835443, Validation Loss: 0.0001439786627694654\n",
      "Epoch [24/25], Train Loss: 0.00016853108536452055, Validation Loss: 0.00014388885853501657\n",
      "Epoch [24/25], Train Loss: 0.00012346076255198568, Validation Loss: 0.00014392796826238433\n",
      "Epoch [24/25], Train Loss: 0.0001469892740715295, Validation Loss: 0.00014393799040893402\n",
      "Epoch [24/25], Train Loss: 0.0001403822097927332, Validation Loss: 0.00014403184492645476\n",
      "Epoch [24/25], Train Loss: 0.00018032286607194692, Validation Loss: 0.00014407281948175903\n",
      "Epoch [24/25], Train Loss: 0.00018012257351074368, Validation Loss: 0.00014403711077951205\n",
      "Epoch [24/25], Train Loss: 0.00018616968009155244, Validation Loss: 0.00014413273383979687\n",
      "Epoch [24/25], Train Loss: 0.00015078295837156475, Validation Loss: 0.00014405726639476294\n",
      "Epoch [24/25], Train Loss: 0.0001592635817360133, Validation Loss: 0.00014396138140000404\n",
      "Epoch [24/25], Train Loss: 0.00015525399066973478, Validation Loss: 0.00014401210016027713\n",
      "Epoch [24/25], Train Loss: 0.00013738841516897082, Validation Loss: 0.00014390759073042622\n",
      "Epoch [24/25], Train Loss: 0.0001454772282158956, Validation Loss: 0.0001438924225415879\n",
      "Epoch [24/25], Train Loss: 0.00017939267854671925, Validation Loss: 0.00014386943294084631\n",
      "Epoch [24/25], Train Loss: 0.0001922993251355365, Validation Loss: 0.00014400883277024452\n",
      "Epoch [24/25], Train Loss: 0.0001446568057872355, Validation Loss: 0.00014394970833867167\n",
      "Epoch [24/25], Train Loss: 0.00014885810378473252, Validation Loss: 0.00014398940208290394\n",
      "Epoch [24/25], Train Loss: 0.00012735254131257534, Validation Loss: 0.00014393596963297267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], Train Loss: 9.995103027904406e-05, Validation Loss: 0.00014401095904759131\n",
      "Epoch [24/25], Train Loss: 0.00010767654748633504, Validation Loss: 0.00014407476289003777\n",
      "Epoch [24/25], Train Loss: 0.0001391993573633954, Validation Loss: 0.00014401317845719557\n",
      "Epoch [24/25], Train Loss: 0.00017271260730922222, Validation Loss: 0.00014411137405356082\n",
      "Epoch [24/25], Train Loss: 0.00014678557636216283, Validation Loss: 0.000143984022239844\n",
      "Epoch [24/25], Train Loss: 0.00013407698133960366, Validation Loss: 0.00014419789586099795\n",
      "Epoch [24/25], Train Loss: 0.00017280378961004317, Validation Loss: 0.0001441186698987925\n",
      "Epoch [24/25], Train Loss: 0.00019256907398812473, Validation Loss: 0.0001440536618853609\n",
      "Epoch [24/25], Train Loss: 0.00012095066631445661, Validation Loss: 0.00014410599008745823\n",
      "Epoch [24/25], Train Loss: 0.0001244220620719716, Validation Loss: 0.00014402870777606344\n",
      "Epoch [24/25], Train Loss: 0.0001373730628984049, Validation Loss: 0.00014399428594818648\n",
      "Epoch [24/25], Train Loss: 0.00019172740576323122, Validation Loss: 0.00014388449199032038\n",
      "Epoch [24/25], Train Loss: 0.0001252779911737889, Validation Loss: 0.00014405584782556009\n",
      "Epoch [24/25], Train Loss: 0.00014790744171477854, Validation Loss: 0.00014386635327052016\n",
      "Epoch [24/25], Train Loss: 0.00016199049423448741, Validation Loss: 0.00014400254755552548\n",
      "Epoch [24/25], Train Loss: 0.00013341572775971144, Validation Loss: 0.00014403103923541494\n",
      "Epoch [24/25], Train Loss: 0.00012253788008820266, Validation Loss: 0.00014400014697457664\n",
      "Epoch [24/25], Train Loss: 0.00018126016948372126, Validation Loss: 0.00014390749687057298\n",
      "Epoch [24/25], Train Loss: 0.00013982827658765018, Validation Loss: 0.00014411838613644554\n",
      "Epoch [24/25], Train Loss: 0.00022651157632935792, Validation Loss: 0.00014400560224506382\n",
      "Epoch [24/25], Train Loss: 0.00017114680667873472, Validation Loss: 0.0001440475459579223\n",
      "Epoch [24/25], Train Loss: 0.00011964995064772666, Validation Loss: 0.00014412705010424058\n",
      "Epoch [24/25], Train Loss: 0.00015080753655638546, Validation Loss: 0.0001442283302215704\n",
      "Epoch [24/25], Train Loss: 0.00015401942073367536, Validation Loss: 0.00014435592674999498\n",
      "Epoch [24/25], Train Loss: 0.00019370016525499523, Validation Loss: 0.00014457366730008896\n",
      "Epoch [24/25], Train Loss: 9.588291140971705e-05, Validation Loss: 0.00014490235213694783\n",
      "Epoch [24/25], Train Loss: 0.0002109531342284754, Validation Loss: 0.00014544081714120693\n",
      "Epoch [24/25], Train Loss: 0.00012860306014772505, Validation Loss: 0.00014608520117083876\n",
      "Epoch [24/25], Train Loss: 0.0002010121534112841, Validation Loss: 0.00014724415207941396\n",
      "Epoch [24/25], Train Loss: 0.00015015306416898966, Validation Loss: 0.00014838060257413115\n",
      "Epoch [24/25], Train Loss: 0.00017489954188931733, Validation Loss: 0.00015046824943662311\n",
      "Epoch [24/25], Train Loss: 0.00019382160098757595, Validation Loss: 0.00015194748193607665\n",
      "Epoch [24/25], Train Loss: 0.00018785276915878057, Validation Loss: 0.0001546303586413463\n",
      "Epoch [24/25], Train Loss: 0.00013325377949513495, Validation Loss: 0.00015390344366702873\n",
      "Epoch [24/25], Train Loss: 0.00022966772667132318, Validation Loss: 0.00015300972857706558\n",
      "Epoch [24/25], Train Loss: 0.00017632955859880894, Validation Loss: 0.0001489020932543402\n",
      "Epoch [24/25], Train Loss: 0.00011808790441136807, Validation Loss: 0.00014552328090455073\n",
      "Epoch [24/25], Train Loss: 0.00018326682038605213, Validation Loss: 0.00014388921432934392\n",
      "Epoch [24/25], Train Loss: 0.00017133014625869691, Validation Loss: 0.00014488978195004165\n",
      "Epoch [24/25], Train Loss: 0.0001305244950344786, Validation Loss: 0.00014692780847932834\n",
      "Epoch [24/25], Train Loss: 0.0001411366683896631, Validation Loss: 0.00014739437101525256\n",
      "Epoch [24/25], Train Loss: 0.00013953899906482548, Validation Loss: 0.00014649543348544588\n",
      "Epoch [24/25], Train Loss: 0.00014178280252963305, Validation Loss: 0.00014459446950543982\n",
      "Epoch [24/25], Train Loss: 0.00016770500224083662, Validation Loss: 0.0001440257741099534\n",
      "Epoch [24/25], Train Loss: 0.00017643955652602017, Validation Loss: 0.00014493187748788235\n",
      "Epoch [24/25], Train Loss: 0.00019150406296830624, Validation Loss: 0.0001460144400577216\n",
      "Epoch [24/25], Train Loss: 0.0001485885150032118, Validation Loss: 0.0001461328536000413\n",
      "Epoch [24/25], Train Loss: 0.00013318026321940124, Validation Loss: 0.0001453808613102107\n",
      "Epoch [24/25], Train Loss: 0.00013545950059778988, Validation Loss: 0.0001441850072296802\n",
      "Epoch [24/25], Train Loss: 0.00016520984354428947, Validation Loss: 0.00014414218264088656\n",
      "Epoch [24/25], Train Loss: 0.0001639701222302392, Validation Loss: 0.00014423669660269903\n",
      "Epoch [24/25], Train Loss: 0.00018545812054071575, Validation Loss: 0.00014482578820510146\n",
      "Epoch [24/25], Train Loss: 9.72851412370801e-05, Validation Loss: 0.00014485916690318846\n",
      "Epoch [24/25], Train Loss: 0.00010848133388208225, Validation Loss: 0.0001446372982172761\n",
      "Epoch [24/25], Train Loss: 0.00012984636123292148, Validation Loss: 0.00014421804929346156\n",
      "Epoch [24/25], Train Loss: 0.00020516759832389653, Validation Loss: 0.00014383561016681294\n",
      "Epoch [24/25], Train Loss: 0.0001798702433006838, Validation Loss: 0.00014388702062812325\n",
      "Epoch [24/25], Train Loss: 0.00012954138219356537, Validation Loss: 0.00014423405567261702\n",
      "Epoch [24/25], Train Loss: 0.0001276852999581024, Validation Loss: 0.0001443679902877193\n",
      "Epoch [24/25], Train Loss: 0.00017783833027351648, Validation Loss: 0.00014472517286776566\n",
      "Epoch [24/25], Train Loss: 0.00015537872968707234, Validation Loss: 0.00014489735282647114\n",
      "Epoch [24/25], Train Loss: 0.0001766236819094047, Validation Loss: 0.00014455966617485199\n",
      "Epoch [24/25], Train Loss: 0.00014587929763365537, Validation Loss: 0.00014440130617003887\n",
      "Epoch [24/25], Train Loss: 0.00015293881006073207, Validation Loss: 0.0001442001744483908\n",
      "Epoch [24/25], Train Loss: 0.00019182405958417803, Validation Loss: 0.00014399590751660676\n",
      "Epoch [24/25], Train Loss: 0.0001546183048048988, Validation Loss: 0.00014385508038685657\n",
      "Epoch [24/25], Train Loss: 0.00014379325148183852, Validation Loss: 0.00014394857037890082\n",
      "Epoch [24/25], Train Loss: 0.00014956518134567887, Validation Loss: 0.00014392751400009728\n",
      "Epoch [24/25], Train Loss: 0.0001481804356444627, Validation Loss: 0.00014406175396288745\n",
      "Epoch [24/25], Train Loss: 0.00016244480502791703, Validation Loss: 0.0001440990320891918\n",
      "Epoch [24/25], Train Loss: 0.00015681199147365987, Validation Loss: 0.00014414432807825507\n",
      "Epoch [24/25], Train Loss: 0.00018329785962123424, Validation Loss: 0.00014416578366459968\n",
      "Epoch [24/25], Train Loss: 0.00011386114056222141, Validation Loss: 0.00014418751816265286\n",
      "Epoch [24/25], Train Loss: 0.00015699876530561596, Validation Loss: 0.00014416634561105943\n",
      "Epoch [24/25], Train Loss: 0.00017617289267946035, Validation Loss: 0.00014411860295998242\n",
      "Epoch [24/25], Train Loss: 0.00015210738638415933, Validation Loss: 0.0001440305556267655\n",
      "Epoch [24/25], Train Loss: 0.0001334589469479397, Validation Loss: 0.00014398224132795197\n",
      "Epoch [24/25], Train Loss: 0.0001080717847798951, Validation Loss: 0.0001439541937240089\n",
      "Epoch [24/25], Train Loss: 0.00015014583186712116, Validation Loss: 0.00014397696286323481\n",
      "Epoch [24/25], Train Loss: 0.00019368583161849529, Validation Loss: 0.00014387884778746713\n",
      "Epoch [24/25], Train Loss: 0.00016007466183509678, Validation Loss: 0.00014395745068516893\n",
      "Epoch [24/25], Train Loss: 0.00015550472016911954, Validation Loss: 0.00014397208021061186\n",
      "Epoch [24/25], Train Loss: 0.00013515257160179317, Validation Loss: 0.00014388297689341318\n",
      "Epoch [24/25], Train Loss: 0.00015297048958018422, Validation Loss: 0.0001438646999304183\n",
      "Epoch [24/25], Train Loss: 0.00015871625510044396, Validation Loss: 0.00014390020369319244\n",
      "Epoch [24/25], Train Loss: 0.0001671627105679363, Validation Loss: 0.0001439227067749016\n",
      "Epoch [24/25], Train Loss: 0.00013374445552472025, Validation Loss: 0.00014400127474800684\n",
      "Epoch [24/25], Train Loss: 0.00015302949759643525, Validation Loss: 0.0001440707380728175\n",
      "Epoch [24/25], Train Loss: 0.00019959955534432083, Validation Loss: 0.0001439339823264163\n",
      "Epoch [24/25], Train Loss: 0.000129502615891397, Validation Loss: 0.0001439267750053356\n",
      "Epoch [24/25], Train Loss: 0.0001335274282610044, Validation Loss: 0.00014393984577812564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], Train Loss: 0.00015375623479485512, Validation Loss: 0.00014387526049783144\n",
      "Epoch [24/25], Train Loss: 0.00014921346155460924, Validation Loss: 0.00014383090480502386\n",
      "Epoch [24/25], Train Loss: 0.00016025030345190316, Validation Loss: 0.00014398429630091414\n",
      "Epoch [24/25], Train Loss: 0.0001615485525690019, Validation Loss: 0.00014387602520097667\n",
      "Epoch [24/25], Train Loss: 0.0001323245232924819, Validation Loss: 0.00014388965500984342\n",
      "Epoch [24/25], Train Loss: 0.00019465027435217053, Validation Loss: 0.00014405328086771382\n",
      "Epoch [24/25], Train Loss: 0.000121374694572296, Validation Loss: 0.00014389114439836704\n",
      "Epoch [24/25], Train Loss: 0.00013312163355294615, Validation Loss: 0.00014390334157117954\n",
      "Epoch [24/25], Train Loss: 0.0001411692937836051, Validation Loss: 0.00014402491433429532\n",
      "Epoch [24/25], Train Loss: 0.00018881321011576802, Validation Loss: 0.00014405868884447652\n",
      "Epoch [24/25], Train Loss: 0.00019652576884254813, Validation Loss: 0.00014393064581478635\n",
      "Epoch [24/25], Train Loss: 0.00014212276437319815, Validation Loss: 0.00014416932341797898\n",
      "Epoch [24/25], Train Loss: 0.0001288975472562015, Validation Loss: 0.00014414989370076608\n",
      "Epoch [24/25], Train Loss: 0.0001553897891426459, Validation Loss: 0.00014401309260089572\n",
      "Epoch [24/25], Train Loss: 0.00022758608974982053, Validation Loss: 0.00014416935106661792\n",
      "Epoch [24/25], Train Loss: 0.0001061481234501116, Validation Loss: 0.0001441076065627082\n",
      "Epoch [24/25], Train Loss: 0.00018054654356092215, Validation Loss: 0.00014408206455603553\n",
      "Epoch [24/25], Train Loss: 0.0001316009002039209, Validation Loss: 0.0001440597892117997\n",
      "Epoch [24/25], Train Loss: 0.00013515882892534137, Validation Loss: 0.0001442412062412283\n",
      "Epoch [24/25], Train Loss: 0.00018122431356459856, Validation Loss: 0.0001442848156633166\n",
      "Epoch [24/25], Train Loss: 0.00011535312660271302, Validation Loss: 0.00014446899901183013\n",
      "Epoch [24/25], Train Loss: 0.00015152480045799166, Validation Loss: 0.00014475429755596755\n",
      "Epoch [24/25], Train Loss: 0.00013470245175994933, Validation Loss: 0.00014517627811680238\n",
      "Epoch [24/25], Train Loss: 0.00018367584561929107, Validation Loss: 0.0001456311292713508\n",
      "Epoch [24/25], Train Loss: 0.0001247620675712824, Validation Loss: 0.00014629269814273962\n",
      "Epoch [24/25], Train Loss: 0.0001625347649678588, Validation Loss: 0.00014687529231499258\n",
      "Epoch [24/25], Train Loss: 0.00013741344446316361, Validation Loss: 0.0001480007709081595\n",
      "Epoch [24/25], Train Loss: 0.00018217320030089468, Validation Loss: 0.00014873029164543065\n",
      "Epoch [24/25], Train Loss: 0.00015831827477086335, Validation Loss: 0.000149827230052324\n",
      "Epoch [24/25], Train Loss: 0.00018242490477859974, Validation Loss: 0.00015008250872294108\n",
      "Epoch [24/25], Train Loss: 0.00016361090820282698, Validation Loss: 0.00015008808137887777\n",
      "Epoch [24/25], Train Loss: 0.00015734021144453436, Validation Loss: 0.00014867516729282214\n",
      "Epoch [24/25], Train Loss: 0.00021095779084134847, Validation Loss: 0.00014722267248240922\n",
      "Epoch [24/25], Train Loss: 0.0001661038986640051, Validation Loss: 0.00014509374280654205\n",
      "Epoch [24/25], Train Loss: 0.00014339765766635537, Validation Loss: 0.00014401359561209878\n",
      "Epoch [24/25], Train Loss: 0.000262535730144009, Validation Loss: 0.00014427839632844553\n",
      "Epoch [25/25], Train Loss: 0.0001670376368565485, Validation Loss: 0.00014464014384429902\n",
      "Epoch [25/25], Train Loss: 0.00015255052130669355, Validation Loss: 0.00014531091267902714\n",
      "Epoch [25/25], Train Loss: 0.0002087360480800271, Validation Loss: 0.00014587301411665975\n",
      "Epoch [25/25], Train Loss: 0.00016899030015338212, Validation Loss: 0.0001458125521215455\n",
      "Epoch [25/25], Train Loss: 0.00014538232062477618, Validation Loss: 0.00014488185309649755\n",
      "Epoch [25/25], Train Loss: 0.00015879303100518882, Validation Loss: 0.0001442305030650459\n",
      "Epoch [25/25], Train Loss: 0.00012011965009151027, Validation Loss: 0.00014397815029951744\n",
      "Epoch [25/25], Train Loss: 0.0001436793536413461, Validation Loss: 0.00014393210634201144\n",
      "Epoch [25/25], Train Loss: 0.00012209301348775625, Validation Loss: 0.00014426162524614484\n",
      "Epoch [25/25], Train Loss: 0.00015109090600162745, Validation Loss: 0.0001446997982081181\n",
      "Epoch [25/25], Train Loss: 0.0002128150808857754, Validation Loss: 0.00014484609152229193\n",
      "Epoch [25/25], Train Loss: 0.00011485272261779755, Validation Loss: 0.00014474959486202957\n",
      "Epoch [25/25], Train Loss: 0.00016004596545826644, Validation Loss: 0.0001446867718186695\n",
      "Epoch [25/25], Train Loss: 0.00014149789058137685, Validation Loss: 0.00014432331615050014\n",
      "Epoch [25/25], Train Loss: 0.00017334210861008614, Validation Loss: 0.00014412355982737306\n",
      "Epoch [25/25], Train Loss: 0.00011251922114752233, Validation Loss: 0.00014414589046888673\n",
      "Epoch [25/25], Train Loss: 0.00015290157170966268, Validation Loss: 0.0001439209406574567\n",
      "Epoch [25/25], Train Loss: 0.00014153179654385895, Validation Loss: 0.00014384531508161065\n",
      "Epoch [25/25], Train Loss: 0.0001789153175195679, Validation Loss: 0.00014416284539038316\n",
      "Epoch [25/25], Train Loss: 0.00011874367919517681, Validation Loss: 0.00014419235740206204\n",
      "Epoch [25/25], Train Loss: 0.00017771331476978958, Validation Loss: 0.0001443087681157825\n",
      "Epoch [25/25], Train Loss: 0.0001487012195866555, Validation Loss: 0.00014435164848691784\n",
      "Epoch [25/25], Train Loss: 8.593548409407958e-05, Validation Loss: 0.0001443662379945939\n",
      "Epoch [25/25], Train Loss: 0.00015271207666955888, Validation Loss: 0.00014423890461330303\n",
      "Epoch [25/25], Train Loss: 0.0001829817338148132, Validation Loss: 0.00014426770236847613\n",
      "Epoch [25/25], Train Loss: 0.00013643089914694428, Validation Loss: 0.00014408641873160378\n",
      "Epoch [25/25], Train Loss: 0.0001442845823476091, Validation Loss: 0.00014397051781998016\n",
      "Epoch [25/25], Train Loss: 0.00016205418796744198, Validation Loss: 0.00014383656161953695\n",
      "Epoch [25/25], Train Loss: 0.00013470968406181782, Validation Loss: 0.00014396279657376\n",
      "Epoch [25/25], Train Loss: 0.0001516597403679043, Validation Loss: 0.00014376398564005892\n",
      "Epoch [25/25], Train Loss: 0.00018690948490984738, Validation Loss: 0.00014386025989855019\n",
      "Epoch [25/25], Train Loss: 0.00014114996884018183, Validation Loss: 0.0001439427568887671\n",
      "Epoch [25/25], Train Loss: 0.00015457150584552437, Validation Loss: 0.0001438533441008379\n",
      "Epoch [25/25], Train Loss: 0.00016515991592314094, Validation Loss: 0.00014396826785135393\n",
      "Epoch [25/25], Train Loss: 0.00018479133723303676, Validation Loss: 0.00014402397646335886\n",
      "Epoch [25/25], Train Loss: 0.00013578914513345808, Validation Loss: 0.0001440912288671825\n",
      "Epoch [25/25], Train Loss: 0.0001214165095007047, Validation Loss: 0.00014417330409438971\n",
      "Epoch [25/25], Train Loss: 0.0001226213644258678, Validation Loss: 0.0001441536008011705\n",
      "Epoch [25/25], Train Loss: 0.00015078866272233427, Validation Loss: 0.00014445798248440648\n",
      "Epoch [25/25], Train Loss: 0.00014378306514117867, Validation Loss: 0.00014458278868308602\n",
      "Epoch [25/25], Train Loss: 0.000184824297321029, Validation Loss: 0.00014455105483648367\n",
      "Epoch [25/25], Train Loss: 0.0001399382745148614, Validation Loss: 0.0001444492149554814\n",
      "Epoch [25/25], Train Loss: 0.00012530272942967713, Validation Loss: 0.0001445404263601328\n",
      "Epoch [25/25], Train Loss: 0.0001693647645879537, Validation Loss: 0.00014444899182611455\n",
      "Epoch [25/25], Train Loss: 0.00011903560516657308, Validation Loss: 0.000144451399683021\n",
      "Epoch [25/25], Train Loss: 0.0001515041949460283, Validation Loss: 0.00014439428705372847\n",
      "Epoch [25/25], Train Loss: 0.00014786965039093047, Validation Loss: 0.00014474170117561395\n",
      "Epoch [25/25], Train Loss: 0.000162237643962726, Validation Loss: 0.00014448808530384365\n",
      "Epoch [25/25], Train Loss: 0.00018053370877169073, Validation Loss: 0.00014436794153880328\n",
      "Epoch [25/25], Train Loss: 0.00010106007539434358, Validation Loss: 0.00014445514607359656\n",
      "Epoch [25/25], Train Loss: 0.0002051073097391054, Validation Loss: 0.00014439505321206525\n",
      "Epoch [25/25], Train Loss: 0.00017654712428338826, Validation Loss: 0.00014431070085265673\n",
      "Epoch [25/25], Train Loss: 0.0001677696855040267, Validation Loss: 0.00014419823201023975\n",
      "Epoch [25/25], Train Loss: 0.00016552041051909328, Validation Loss: 0.0001442823481435577\n",
      "Epoch [25/25], Train Loss: 0.0001461992651456967, Validation Loss: 0.00014412269035043816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], Train Loss: 0.00014368275878950953, Validation Loss: 0.00014436305937124417\n",
      "Epoch [25/25], Train Loss: 0.0001298857678193599, Validation Loss: 0.00014427468073942388\n",
      "Epoch [25/25], Train Loss: 0.00015422220167238265, Validation Loss: 0.00014437952001268665\n",
      "Epoch [25/25], Train Loss: 0.00018196455494035035, Validation Loss: 0.00014444938036225116\n",
      "Epoch [25/25], Train Loss: 0.00019773803069256246, Validation Loss: 0.00014476802510519823\n",
      "Epoch [25/25], Train Loss: 0.00012076621351297945, Validation Loss: 0.00014488107505409667\n",
      "Epoch [25/25], Train Loss: 0.00012173169670859352, Validation Loss: 0.0001453250450140331\n",
      "Epoch [25/25], Train Loss: 0.00012704971595667303, Validation Loss: 0.00014575295523779156\n",
      "Epoch [25/25], Train Loss: 0.0001805307692848146, Validation Loss: 0.00014641503876191563\n",
      "Epoch [25/25], Train Loss: 0.00012278441863600165, Validation Loss: 0.00014706139724391203\n",
      "Epoch [25/25], Train Loss: 0.00018059455032926053, Validation Loss: 0.00014776653745987762\n",
      "Epoch [25/25], Train Loss: 0.00014285398356150836, Validation Loss: 0.00014852686532928298\n",
      "Epoch [25/25], Train Loss: 0.00014406839909497648, Validation Loss: 0.00014889879579034945\n",
      "Epoch [25/25], Train Loss: 0.00014883381663821638, Validation Loss: 0.00014934823605775212\n",
      "Epoch [25/25], Train Loss: 0.00023830958525650203, Validation Loss: 0.00014916434981084118\n",
      "Epoch [25/25], Train Loss: 0.00018436028040014207, Validation Loss: 0.00014855811969027855\n",
      "Epoch [25/25], Train Loss: 0.00013480964116752148, Validation Loss: 0.00014697120738370965\n",
      "Epoch [25/25], Train Loss: 0.0001273476518690586, Validation Loss: 0.00014567278315856432\n",
      "Epoch [25/25], Train Loss: 0.00014179405116010457, Validation Loss: 0.00014448557825138172\n",
      "Epoch [25/25], Train Loss: 0.00017091739573515952, Validation Loss: 0.0001441057313058991\n",
      "Epoch [25/25], Train Loss: 0.00011586121399886906, Validation Loss: 0.0001445084562874399\n",
      "Epoch [25/25], Train Loss: 0.00018647876277100295, Validation Loss: 0.00014537821892493714\n",
      "Epoch [25/25], Train Loss: 0.0001764282351359725, Validation Loss: 0.00014592321943685723\n",
      "Epoch [25/25], Train Loss: 0.00013937930634710938, Validation Loss: 0.0001460858618277901\n",
      "Epoch [25/25], Train Loss: 0.00012022987357340753, Validation Loss: 0.0001454324742856746\n",
      "Epoch [25/25], Train Loss: 0.00015832175267860293, Validation Loss: 0.00014485366482404062\n",
      "Epoch [25/25], Train Loss: 0.00018623494543135166, Validation Loss: 0.00014414895655742537\n",
      "Epoch [25/25], Train Loss: 0.00013507627591025084, Validation Loss: 0.00014411736750237955\n",
      "Epoch [25/25], Train Loss: 0.00015386054292321205, Validation Loss: 0.00014419662329601124\n",
      "Epoch [25/25], Train Loss: 0.00014711501717101783, Validation Loss: 0.000144795500333809\n",
      "Epoch [25/25], Train Loss: 0.000162270778673701, Validation Loss: 0.0001450639802593893\n",
      "Epoch [25/25], Train Loss: 0.00011764095688704401, Validation Loss: 0.00014507157175103202\n",
      "Epoch [25/25], Train Loss: 0.00019787380006164312, Validation Loss: 0.00014486429281532765\n",
      "Epoch [25/25], Train Loss: 0.0001940977672347799, Validation Loss: 0.00014457947181654163\n",
      "Epoch [25/25], Train Loss: 0.0001445037341909483, Validation Loss: 0.000144359729650508\n",
      "Epoch [25/25], Train Loss: 0.00020077369117643684, Validation Loss: 0.00014401759617612697\n",
      "Epoch [25/25], Train Loss: 0.0001476784673286602, Validation Loss: 0.00014417026468436234\n",
      "Epoch [25/25], Train Loss: 0.00016675163351465017, Validation Loss: 0.000144136374728987\n",
      "Epoch [25/25], Train Loss: 0.00010682423453545198, Validation Loss: 0.00014432860722687717\n",
      "Epoch [25/25], Train Loss: 0.00021416366507764906, Validation Loss: 0.00014438720754696987\n",
      "Epoch [25/25], Train Loss: 0.00018726637063082308, Validation Loss: 0.0001446060550127489\n",
      "Epoch [25/25], Train Loss: 0.00019726577738765627, Validation Loss: 0.0001444756528750683\n",
      "Epoch [25/25], Train Loss: 0.00017096803640015423, Validation Loss: 0.0001446138550818432\n",
      "Epoch [25/25], Train Loss: 0.000168253987794742, Validation Loss: 0.0001443578223794854\n",
      "Epoch [25/25], Train Loss: 0.00010554114851402119, Validation Loss: 0.00014430113151320256\n",
      "Epoch [25/25], Train Loss: 0.00014300033217296004, Validation Loss: 0.00014417557589088876\n",
      "Epoch [25/25], Train Loss: 0.0001519220240879804, Validation Loss: 0.00014412511178913217\n",
      "Epoch [25/25], Train Loss: 0.00019620341481640935, Validation Loss: 0.00014396454716916196\n",
      "Epoch [25/25], Train Loss: 0.00013182540715206414, Validation Loss: 0.00014384229192122196\n",
      "Epoch [25/25], Train Loss: 0.00016212995979003608, Validation Loss: 0.00014381795044755564\n",
      "Epoch [25/25], Train Loss: 0.00019245318253524601, Validation Loss: 0.00014382044998152802\n",
      "Epoch [25/25], Train Loss: 0.00012816430535167456, Validation Loss: 0.00014382500570112218\n",
      "Epoch [25/25], Train Loss: 0.00010354780533816665, Validation Loss: 0.00014385370717112284\n",
      "Epoch [25/25], Train Loss: 0.00016660531400702894, Validation Loss: 0.00014392908681960155\n",
      "Epoch [25/25], Train Loss: 0.00015028793131932616, Validation Loss: 0.00014385941637253078\n",
      "Epoch [25/25], Train Loss: 0.00018001513672061265, Validation Loss: 0.00014392368199575383\n",
      "Epoch [25/25], Train Loss: 0.0001827566884458065, Validation Loss: 0.000144058120107123\n",
      "Epoch [25/25], Train Loss: 0.00011937489762203768, Validation Loss: 0.00014399356490078693\n",
      "Epoch [25/25], Train Loss: 0.00017299507453572005, Validation Loss: 0.00014399510740380114\n",
      "Epoch [25/25], Train Loss: 0.00020191300427541137, Validation Loss: 0.0001441261976045401\n",
      "Epoch [25/25], Train Loss: 0.00018363278650213033, Validation Loss: 0.00014418672872125172\n",
      "Epoch [25/25], Train Loss: 0.00017037642828654498, Validation Loss: 0.0001440972468117252\n",
      "Epoch [25/25], Train Loss: 0.0001685601018834859, Validation Loss: 0.00014425013432628475\n",
      "Epoch [25/25], Train Loss: 0.0002472930063959211, Validation Loss: 0.0001449051155456497\n",
      "Epoch [25/25], Train Loss: 0.00019181113748345524, Validation Loss: 0.0001447071442574573\n",
      "Epoch [25/25], Train Loss: 0.00019508138939272612, Validation Loss: 0.00014471100512309932\n",
      "Epoch [25/25], Train Loss: 0.00018783721316140145, Validation Loss: 0.000144884288359511\n",
      "Epoch [25/25], Train Loss: 0.00014474065392278135, Validation Loss: 0.00014501455322412464\n",
      "Epoch [25/25], Train Loss: 0.00011814401659648865, Validation Loss: 0.0001451624179026112\n",
      "Epoch [25/25], Train Loss: 0.00014367660332936794, Validation Loss: 0.0001450316073411765\n",
      "Epoch [25/25], Train Loss: 0.00012557314767036587, Validation Loss: 0.0001450803201199354\n",
      "Epoch [25/25], Train Loss: 9.955062705557793e-05, Validation Loss: 0.00014513048614996175\n",
      "Epoch [25/25], Train Loss: 0.00017560269043315202, Validation Loss: 0.0001453580329931962\n",
      "Epoch [25/25], Train Loss: 0.0001599616080056876, Validation Loss: 0.0001453354712187623\n",
      "Epoch [25/25], Train Loss: 0.00016040887567214668, Validation Loss: 0.0001454164543247316\n",
      "Epoch [25/25], Train Loss: 0.00017249747179448605, Validation Loss: 0.00014547287015981663\n",
      "Epoch [25/25], Train Loss: 0.0001511856826255098, Validation Loss: 0.00014552307499495024\n",
      "Epoch [25/25], Train Loss: 0.00013373076217249036, Validation Loss: 0.00014548004643681148\n",
      "Epoch [25/25], Train Loss: 0.00018416825332678854, Validation Loss: 0.00014532540881191381\n",
      "Epoch [25/25], Train Loss: 0.00017142978322226554, Validation Loss: 0.00014508265060915923\n",
      "Epoch [25/25], Train Loss: 0.00013582108658738434, Validation Loss: 0.00014475929371352927\n",
      "Epoch [25/25], Train Loss: 0.0001379717286908999, Validation Loss: 0.00014443978652707302\n",
      "Epoch [25/25], Train Loss: 0.00017063076666090637, Validation Loss: 0.00014417879453200536\n",
      "Epoch [25/25], Train Loss: 0.00014017631474416703, Validation Loss: 0.0001440044849005062\n",
      "Epoch [25/25], Train Loss: 0.0001631721534067765, Validation Loss: 0.00014390321860749584\n",
      "Epoch [25/25], Train Loss: 0.00012893311213701963, Validation Loss: 0.00014390221525294086\n",
      "Epoch [25/25], Train Loss: 0.00012515074922703207, Validation Loss: 0.00014385608592419884\n",
      "Epoch [25/25], Train Loss: 0.00017452325846534222, Validation Loss: 0.0001439951922899733\n",
      "Epoch [25/25], Train Loss: 0.00015656615141779184, Validation Loss: 0.00014387912257613305\n",
      "Epoch [25/25], Train Loss: 0.00015797947708051652, Validation Loss: 0.0001441773403106102\n",
      "Epoch [25/25], Train Loss: 0.00016911908460315317, Validation Loss: 0.0001442676963051781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], Train Loss: 0.0001921856455737725, Validation Loss: 0.00014433050697941022\n",
      "Epoch [25/25], Train Loss: 0.00021441839635372162, Validation Loss: 0.00014454773311930089\n",
      "Epoch [25/25], Train Loss: 0.00018039638234768063, Validation Loss: 0.00014484422960473844\n",
      "Epoch [25/25], Train Loss: 0.0001287725317524746, Validation Loss: 0.00014479821038548835\n",
      "Epoch [25/25], Train Loss: 0.0001382494519930333, Validation Loss: 0.00014506320003420113\n",
      "Epoch [25/25], Train Loss: 0.0001657069951761514, Validation Loss: 0.0001453210468753241\n",
      "Epoch [25/25], Train Loss: 0.00018375601212028414, Validation Loss: 0.00014553135139673637\n",
      "Epoch [25/25], Train Loss: 0.00016600142407696694, Validation Loss: 0.00014581952079121643\n",
      "Epoch [25/25], Train Loss: 0.00016491900896653533, Validation Loss: 0.0001461153253330849\n",
      "Epoch [25/25], Train Loss: 0.00013302132720127702, Validation Loss: 0.0001465146890647399\n",
      "Epoch [25/25], Train Loss: 0.00017024925909936428, Validation Loss: 0.0001466060023327979\n",
      "Epoch [25/25], Train Loss: 0.0001925614196807146, Validation Loss: 0.00014703272900078445\n",
      "Epoch [25/25], Train Loss: 0.00015747558791190386, Validation Loss: 0.00014692652645559672\n",
      "Epoch [25/25], Train Loss: 0.00018480072321835905, Validation Loss: 0.00014694491716606232\n",
      "Epoch [25/25], Train Loss: 0.00015739674563519657, Validation Loss: 0.00014632179930534523\n",
      "Epoch [25/25], Train Loss: 0.00014793628361076117, Validation Loss: 0.00014581357051307958\n",
      "Epoch [25/25], Train Loss: 0.00011266143701504916, Validation Loss: 0.0001448218686467347\n",
      "Epoch [25/25], Train Loss: 0.00027267474797554314, Validation Loss: 0.0001444576924162296\n",
      "Epoch [25/25], Train Loss: 0.00017243290494661778, Validation Loss: 0.00014385792649894332\n",
      "Epoch [25/25], Train Loss: 0.0001688638876657933, Validation Loss: 0.0001440833070470641\n",
      "Epoch [25/25], Train Loss: 0.00012321563553996384, Validation Loss: 0.00014446671872671383\n",
      "Epoch [25/25], Train Loss: 0.0001856456365203485, Validation Loss: 0.00014504845142558528\n",
      "Epoch [25/25], Train Loss: 0.00016457993478979915, Validation Loss: 0.00014547509078208047\n",
      "Epoch [25/25], Train Loss: 0.0001389670796925202, Validation Loss: 0.00014559610669190686\n",
      "Epoch [25/25], Train Loss: 0.00016958975174929947, Validation Loss: 0.00014546972306561656\n",
      "Epoch [25/25], Train Loss: 0.00015408877516165376, Validation Loss: 0.0001450812417412332\n",
      "Epoch [25/25], Train Loss: 0.00018448769696988165, Validation Loss: 0.00014452562463702635\n",
      "Epoch [25/25], Train Loss: 0.00018206545792054385, Validation Loss: 0.00014413915729771058\n",
      "Epoch [25/25], Train Loss: 0.00013073744776193053, Validation Loss: 0.00014389101052074692\n",
      "Epoch [25/25], Train Loss: 0.0001470708812121302, Validation Loss: 0.0001437847929385801\n",
      "Epoch [25/25], Train Loss: 0.00016720028361305594, Validation Loss: 0.00014389084608410486\n",
      "Epoch [25/25], Train Loss: 0.00012039641296723858, Validation Loss: 0.00014407774700278726\n",
      "Epoch [25/25], Train Loss: 9.918730211211368e-05, Validation Loss: 0.00014432887959022386\n",
      "Epoch [25/25], Train Loss: 0.00016251001216005534, Validation Loss: 0.0001445077015281034\n",
      "Epoch [25/25], Train Loss: 0.00013534830941352993, Validation Loss: 0.0001446315322633988\n",
      "Epoch [25/25], Train Loss: 0.00018763674597721547, Validation Loss: 0.00014465459341105695\n",
      "Epoch [25/25], Train Loss: 0.0001629095640964806, Validation Loss: 0.00014464797156203227\n",
      "Epoch [25/25], Train Loss: 0.00017908052541315556, Validation Loss: 0.0001445930560294073\n",
      "Epoch [25/25], Train Loss: 0.00017631525406613946, Validation Loss: 0.0001444949249465329\n",
      "Epoch [25/25], Train Loss: 0.00013846777437720448, Validation Loss: 0.00014450566668529065\n",
      "Epoch [25/25], Train Loss: 0.00017230531375389546, Validation Loss: 0.0001442713451979216\n",
      "Epoch [25/25], Train Loss: 0.00018011614156421274, Validation Loss: 0.00014431183371925725\n",
      "Epoch [25/25], Train Loss: 0.00014477031072601676, Validation Loss: 0.00014415763168168876\n",
      "Epoch [25/25], Train Loss: 0.0001488107373006642, Validation Loss: 0.00014405938563868403\n",
      "Epoch [25/25], Train Loss: 0.00016587023856118321, Validation Loss: 0.00014400376361057473\n",
      "Epoch [25/25], Train Loss: 0.00011458821245469153, Validation Loss: 0.00014389292021708874\n",
      "Epoch [25/25], Train Loss: 0.00018720250227488577, Validation Loss: 0.0001439184918126557\n",
      "Epoch [25/25], Train Loss: 0.00014808890409767628, Validation Loss: 0.00014385395964685208\n",
      "Epoch [25/25], Train Loss: 0.00014507707965094596, Validation Loss: 0.00014390827685322922\n",
      "Epoch [25/25], Train Loss: 0.00013270066119730473, Validation Loss: 0.00014390365346722927\n",
      "Epoch [25/25], Train Loss: 0.0001376949658151716, Validation Loss: 0.00014382581939571538\n",
      "Epoch [25/25], Train Loss: 0.00014701581676490605, Validation Loss: 0.00014390018623089418\n",
      "Epoch [25/25], Train Loss: 8.294290455523878e-05, Validation Loss: 0.00014391722579603082\n",
      "Epoch [25/25], Train Loss: 0.00013179985398892313, Validation Loss: 0.00014386727489181794\n",
      "Epoch [25/25], Train Loss: 0.00018276655464433134, Validation Loss: 0.00014402698191891736\n",
      "Epoch [25/25], Train Loss: 0.00012078364670742303, Validation Loss: 0.00014393558376468718\n",
      "Epoch [25/25], Train Loss: 0.00015935883857309818, Validation Loss: 0.00014399054683356856\n",
      "Epoch [25/25], Train Loss: 0.00015826204617042094, Validation Loss: 0.00014417406879753495\n",
      "Epoch [25/25], Train Loss: 0.00013860792387276888, Validation Loss: 0.00014414884305248658\n",
      "Epoch [25/25], Train Loss: 0.0002002968976739794, Validation Loss: 0.0001444733973282079\n",
      "Epoch [25/25], Train Loss: 0.00012738382793031633, Validation Loss: 0.0001445355648077869\n",
      "Epoch [25/25], Train Loss: 7.773158722557127e-05, Validation Loss: 0.00014449050916785685\n",
      "Epoch [25/25], Train Loss: 0.00010498108895262703, Validation Loss: 0.00014483144113910384\n",
      "Epoch [25/25], Train Loss: 0.00013485962699633092, Validation Loss: 0.00014518769215404365\n",
      "Epoch [25/25], Train Loss: 0.00012934408732689917, Validation Loss: 0.00014537114208602967\n",
      "Epoch [25/25], Train Loss: 0.00017256040882784873, Validation Loss: 0.00014591987686192928\n",
      "Epoch [25/25], Train Loss: 0.00017517140076961368, Validation Loss: 0.00014650310379996274\n",
      "Epoch [25/25], Train Loss: 0.00015904138854239136, Validation Loss: 0.0001474701088833778\n",
      "Epoch [25/25], Train Loss: 0.0001441247877664864, Validation Loss: 0.0001482044627967601\n",
      "Epoch [25/25], Train Loss: 0.00014625561016146094, Validation Loss: 0.0001496193360556693\n",
      "Epoch [25/25], Train Loss: 0.00020456311176531017, Validation Loss: 0.0001501064522017259\n",
      "Epoch [25/25], Train Loss: 9.790857438929379e-05, Validation Loss: 0.00015085158180833485\n",
      "Epoch [25/25], Train Loss: 0.00011687252845149487, Validation Loss: 0.0001499901450491355\n",
      "Epoch [25/25], Train Loss: 0.00020024555851705372, Validation Loss: 0.000148831875170193\n",
      "Epoch [25/25], Train Loss: 0.00012726604472845793, Validation Loss: 0.00014641831779348043\n",
      "Epoch [25/25], Train Loss: 0.00015619536861777306, Validation Loss: 0.0001446666220241847\n",
      "Epoch [25/25], Train Loss: 0.0001566218415973708, Validation Loss: 0.00014392062633608778\n",
      "Epoch [25/25], Train Loss: 0.00011536457168404013, Validation Loss: 0.00014422037929762155\n",
      "Epoch [25/25], Train Loss: 0.00016586296260356903, Validation Loss: 0.0001449799856345635\n",
      "Epoch [25/25], Train Loss: 0.00013489100092556328, Validation Loss: 0.00014562654663071348\n",
      "Epoch [25/25], Train Loss: 0.0001978321379283443, Validation Loss: 0.00014559320916305297\n",
      "Epoch [25/25], Train Loss: 0.00013701285934075713, Validation Loss: 0.00014497078979426684\n",
      "Epoch [25/25], Train Loss: 0.00018876395188272, Validation Loss: 0.0001443234885906956\n",
      "Epoch [25/25], Train Loss: 0.00013563339598476887, Validation Loss: 0.0001438391792665546\n",
      "Epoch [25/25], Train Loss: 0.00014470618043560535, Validation Loss: 0.00014386223944408508\n",
      "Epoch [25/25], Train Loss: 0.00015583488857373595, Validation Loss: 0.00014417657415227342\n",
      "Epoch [25/25], Train Loss: 0.00015278464707080275, Validation Loss: 0.00014447753468023924\n",
      "Epoch [25/25], Train Loss: 0.00014210342487785965, Validation Loss: 0.0001445741892287818\n",
      "Epoch [25/25], Train Loss: 0.00011970159539487213, Validation Loss: 0.00014436507990467362\n",
      "Epoch [25/25], Train Loss: 0.00014105651644058526, Validation Loss: 0.00014409942135292417\n",
      "Epoch [25/25], Train Loss: 0.00013749347999691963, Validation Loss: 0.00014394602597652315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], Train Loss: 0.00012345562572591007, Validation Loss: 0.0001437976119632367\n",
      "Epoch [25/25], Train Loss: 0.00013620482059195638, Validation Loss: 0.00014388162113997775\n",
      "Epoch [25/25], Train Loss: 0.00011290369729977101, Validation Loss: 0.0001438856590539217\n",
      "Epoch [25/25], Train Loss: 0.00016217607480939478, Validation Loss: 0.00014407304988708347\n",
      "Epoch [25/25], Train Loss: 0.0001228248729603365, Validation Loss: 0.00014402697367283206\n",
      "Epoch [25/25], Train Loss: 0.00016279586998280138, Validation Loss: 0.00014406472619157284\n",
      "Epoch [25/25], Train Loss: 0.0001669173507252708, Validation Loss: 0.0001441043699742295\n",
      "Epoch [25/25], Train Loss: 0.00016653633792884648, Validation Loss: 0.00014399061328731477\n",
      "Epoch [25/25], Train Loss: 0.0001304814068134874, Validation Loss: 0.00014405474673064115\n",
      "Epoch [25/25], Train Loss: 0.00016316396067850292, Validation Loss: 0.00014394405952771193\n",
      "Epoch [25/25], Train Loss: 0.00011810533760581166, Validation Loss: 0.00014386667195746364\n",
      "Epoch [25/25], Train Loss: 0.00012233293091412634, Validation Loss: 0.00014386245117445167\n",
      "Epoch [25/25], Train Loss: 0.00013527680130209774, Validation Loss: 0.00014384878692605223\n",
      "Epoch [25/25], Train Loss: 0.00014348422701004893, Validation Loss: 0.00014383206968583787\n",
      "Epoch [25/25], Train Loss: 0.00015256687765941024, Validation Loss: 0.00014384490981077155\n",
      "Epoch [25/25], Train Loss: 0.00016713068180251867, Validation Loss: 0.0001437463860687179\n",
      "Epoch [25/25], Train Loss: 0.00017846991249825805, Validation Loss: 0.0001438107945432421\n",
      "Epoch [25/25], Train Loss: 6.689212750643492e-05, Validation Loss: 0.00014376490532110134\n",
      "Epoch [25/25], Train Loss: 0.00016754244279582053, Validation Loss: 0.0001437377014857096\n",
      "Epoch [25/25], Train Loss: 0.00011025145067833364, Validation Loss: 0.0001438066048043159\n",
      "Epoch [25/25], Train Loss: 0.0001071702572517097, Validation Loss: 0.0001437720672887129\n",
      "Epoch [25/25], Train Loss: 0.00014483637642115355, Validation Loss: 0.00014375599081783245\n",
      "Epoch [25/25], Train Loss: 0.00017680473683867604, Validation Loss: 0.00014389046724924508\n",
      "Epoch [25/25], Train Loss: 0.00019446857913862914, Validation Loss: 0.00014376458105592367\n",
      "Epoch [25/25], Train Loss: 0.00020800073980353773, Validation Loss: 0.00014389227120166956\n",
      "Epoch [25/25], Train Loss: 0.00015673137386329472, Validation Loss: 0.00014411792944883927\n",
      "Epoch [25/25], Train Loss: 0.0001689290365902707, Validation Loss: 0.00014390181167982518\n",
      "Epoch [25/25], Train Loss: 0.00016861295443959534, Validation Loss: 0.00014398361624140915\n",
      "Epoch [25/25], Train Loss: 0.00015042003360576928, Validation Loss: 0.0001441707807922891\n",
      "Epoch [25/25], Train Loss: 0.00018039641145151109, Validation Loss: 0.00014412355594686232\n",
      "Epoch [25/25], Train Loss: 5.5660442740190774e-05, Validation Loss: 0.0001443553444308539\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model_preictal = DDPM(input_channels, num_filters)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for reconstruction\n",
    "optimizer = optim.Adam(model_preictal.parameters(), lr=learning_rate)\n",
    "\n",
    "train_losses_preictal = []\n",
    "val_losses_preictal = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model_preictal.train()  # Set model to training mode\n",
    "    for i, (batch_X, _) in enumerate(train_dataloader):\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed = model_preictal(batch_X)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(reconstructed, batch_X)\n",
    "        \n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append losses to the lists after each epoch\n",
    "        train_losses_preictal.append(loss.item())\n",
    "        \n",
    "        \n",
    "        # Print progress\n",
    "       # print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(dataloader)}], Loss: {loss.item()}\")\n",
    "        \n",
    "        # Validation\n",
    "        model_preictal.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X_val, _ in val_dataloader:\n",
    "                reconstructed_val = model_preictal(batch_X_val)\n",
    "                val_loss += criterion(reconstructed_val, batch_X_val).item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        val_losses_preictal.append(val_loss)\n",
    "\n",
    "        # Print training and validation loss\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()}, Validation Loss: {val_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# train_losses_preictal and val_losses_preictal contain the loss values across epochs\n",
    "# Plotting training and validation loss\n",
    "epochs = range(1, len(train_losses_preictal) + 1)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_losses_preictal, label='Training Loss')\n",
    "plt.plot(epochs, val_losses_preictal, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss for Preictal data generation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data samples\n",
    "num_samples_to_generate = 2714\n",
    "with torch.no_grad():\n",
    "    generated_samples_preictal = []\n",
    "    for _ in range(num_samples_to_generate):\n",
    "        noise = torch.randn(1, X_pre.shape[1], 32)  # Generate noise with 32 channels\n",
    "        generated = model_preictal.decoder(noise.permute(0, 2, 1)).squeeze().numpy()  # Adjust input shape for the decoder\n",
    "        generated_samples_preictal.append(generated)\n",
    "\n",
    "# Convert generated samples to a NumPy array\n",
    "generated_samples_preictal = np.array(generated_samples_preictal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2714, 16, 4000)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_samples_preictal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('generated_samples_preictal', generated_samples_preictal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('X_train_samples_preictal', X_pre_train)\n",
    "np.save('y_train_samples_preictal', y_pre_train)\n",
    "np.save('X_val_samples_preictal', X_pre_val)\n",
    "np.save('y_val_samples_preictal', y_pre_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
